Directory Structure

accdatatools
 |.git
 |.gitignore
 |Clustering
 | |metrics_figs.py
 | |string_clustering.py
 | |tslearn_figs.py
 | |tslearn_spks.py
 | |tslearn_spks_vs_events.py
 | |__init__.py
 |DataCleaning
 | |both_sides_high_contrast.pkl
 | |both_sides_high_contrast_uncleaned.pkl
 | |determine_dprime.py
 | |group_recordings_into_datasets.py
 | |left_only_high_contrast.pkl
 | |left_only_high_contrast_uncleaned.pkl
 | |low_contrast.pkl
 | |low_contrast_uncleaned.pkl
 | |__init__.py
 | |__pycache__
 | | |determine_dprime.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |DataVisualisation
 | |dprime_of_all_recordings_figure.py
 | |eyecam_frametime_figure.py
 | |fitted_vs_realvals.py
 | |kernel_heat_map.py
 | |mean_videos.py
 | |pupil_figure_std_error.py
 | |pupil_mixed_model_output_visualisation.py
 | |pupil_size_vs_licking.py
 | |synch_sanity_check_figure.py
 | |whole_experiment_figure.py
 | |__init__.py
 | |__pycache__
 | | |mean_videos.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |DeepLabCutPipeline
 | |cmdline_utils.py
 | |create_testing_dataset.py
 | |instantiate_model.py
 | |validate_model_with_testing_dataset.py
 | |__init__.py
 |GLOBALS.py
 |high_contrast_licking_pca.gif
 |Observations
 | |recordings.py
 | |roi_trials.py
 | |trials.py
 | |__init__.py
 | |__pycache__
 | | |recordings.cpython-37.pyc
 | | |trials.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |ProcessFluorescence
 | |data_cleaning.py
 | |df_on_f_calculations.py
 | |NovelRegressionDevelopment
 | | |df_on_f_novel_regression_scratchpad.py
 | | |df_on_f_novel_regression_scratchpad_copy.py
 | | |df_on_f_novel_regression_test.py
 | | |df_on_f_novel_regression_validation_with_created_data.py
 | | |negative_value_detection_script.py
 | | |untitled0.py
 | | |untitled2.py
 | | |vectorised_underline_regression.py
 | |__pycache__
 | | |data_cleaning.cpython-37.pyc
 | | |df_on_f_calculations.cpython-37.pyc
 | | |df_on_f_novel_regression_scratchpad.cpython-37.pyc
 | | |df_on_f_novel_regression_test.cpython-37.pyc
 | | |vectorised_underline_regression.cpython-37.pyc
 |ProcessLicking
 | |kernel.py
 | |neuron_response_to_lick_validation.py
 | |__init__.py
 | |__pycache__
 | | |kernel.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |ProcessPupil
 | |pupil_frame_synchronization.py
 | |pupil_trends_figure.py
 | |pupil_trends_subtyped.py
 | |size.py
 | |size_pipeline_quality_control.py
 | |__init__.py
 | |__pycache__
 | | |size.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |README.md
 |ResultsVisualisation
 | |bilat_highcon_licking_pca.gif
 | |lowcon_licking_pca.gif
 | |pie_chart.py
 | |unilat_highcon_licking.gif
 |RScripts
 | |.Rhistory
 | |compare_modelling_approaches.R
 | |example_r_script.R
 | |full_kernel_model.R
 | |get_model_pval.R
 | |is_there_elevated_activity_during_trials.R
 | |lick_kernel_subtraction_test.R
 | |pretrial_pupil_size_mixed_linear_model.R
 | |pupils_vs_peritrial.R
 | |pupil_size_mixed_linear_model.R
 | |r_interface.py
 | |subtract_licking_then_average.R
 | |subtract_licking_then_average_figuregen.R
 | |subtract_licking_then_average_validation.R
 | |__init__.py
 |Suite2p
 | |automate_s2p.py
 | |get_tiff_from_binary.py
 | |overwrite_iscells.py
 | |to_interleaved_tiffs.py
 | |to_tiffs.py
 | |untitled0.py
 | |__init__.py
 |Summaries
 | |classwise_summary.txt
 | |produce_trial_summary_document.py
 | |__init__.py
 | |__pycache__
 | | |produce_trial_summary_document.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |TheoreticalVisualisation
 | |figures_for_thesis_intro.py
 | |kernel_figures.py
 | |time_series_merging_algorithm_figure.py
 | |__init__.py
 |Timing
 | |synchronisation.py
 | |__init__.py
 | |__pycache__
 | | |synchronisation.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |ToCSV
 | |without_collapsing.py
 | |with_collapsing.py
 | |__init__.py
 | |__pycache__
 | | |without_collapsing.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |Utils
 | |convienience.py
 | |copy_entire_dataset_excluding_binaries.py
 | |deeploadmat.py
 | |get_files_with_dprime_and_condn.py
 | |map_across_dataset.py
 | |path.py
 | |quine.py
 | |quine.txt
 | |signal_processing.py
 | |__init__.py
 | |__pycache__
 | | |convienience.cpython-37.pyc
 | | |deeploadmat.cpython-37.pyc
 | | |map_across_dataset.cpython-37.pyc
 | | |path.cpython-37.pyc
 | | |signal_processing.cpython-37.pyc
 | | |__init__.cpython-37.pyc
 |__init__.py
 |__pycache__
 | |GLOBALS.cpython-37.pyc
 | |__init__.cpython-37.pyc
.gitignore
0  | # Byte-compiled / optimized / DLL files
1  | __pycache__/
2  | *.py[cod]
3  | *$py.class
4  | 
5  | # C extensions
6  | *.so
7  | 
8  | # Distribution / packaging
9  | .Python
10 | build/
11 | develop-eggs/
12 | dist/
13 | downloads/
14 | eggs/
15 | .eggs/
16 | lib/
17 | lib64/
18 | parts/
19 | sdist/
20 | var/
21 | wheels/
22 | pip-wheel-metadata/
23 | share/python-wheels/
24 | *.egg-info/
25 | .installed.cfg
26 | *.egg
27 | MANIFEST
28 | 
29 | # PyInstaller
30 | #  Usually these files are written by a python script from a template
31 | #  before PyInstaller builds the exe, so as to inject date/other infos into it.
32 | *.manifest
33 | *.spec
34 | 
35 | # Installer logs
36 | pip-log.txt
37 | pip-delete-this-directory.txt
38 | 
39 | # Unit test / coverage reports
40 | htmlcov/
41 | .tox/
42 | .nox/
43 | .coverage
44 | .coverage.*
45 | .cache
46 | nosetests.xml
47 | coverage.xml
48 | *.cover
49 | *.py,cover
50 | .hypothesis/
51 | .pytest_cache/
52 | cover/
53 | 
54 | # Translations
55 | *.mo
56 | *.pot
57 | 
58 | # Django stuff:
59 | *.log
60 | local_settings.py
61 | db.sqlite3
62 | db.sqlite3-journal
63 | 
64 | # Flask stuff:
65 | instance/
66 | .webassets-cache
67 | 
68 | # Scrapy stuff:
69 | .scrapy
70 | 
71 | # Sphinx documentation
72 | docs/_build/
73 | 
74 | # PyBuilder
75 | target/
76 | 
77 | # Jupyter Notebook
78 | .ipynb_checkpoints
79 | 
80 | # IPython
81 | profile_default/
82 | ipython_config.py
83 | 
84 | # pyenv
85 | #   For a library or package, you might want to ignore these files since the code is
86 | #   intended to run in multiple environments; otherwise, check them in:
87 | # .python-version
88 | 
89 | # pipenv
90 | #   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
91 | #   However, in case of collaboration, if having platform-specific dependencies or dependencies
92 | #   having no cross-platform support, pipenv may install dependencies that don't work, or not
93 | #   install all needed dependencies.
94 | #Pipfile.lock
95 | 
96 | # PEP 582; used by e.g. github.com/David-OConnor/pyflow
97 | __pypackages__/
98 | 
99 | # Celery stuff
100| celerybeat-schedule
101| celerybeat.pid
102| 
103| # SageMath parsed files
104| *.sage.py
105| 
106| # Environments
107| .env
108| .venv
109| env/
110| venv/
111| ENV/
112| env.bak/
113| venv.bak/
114| 
115| # Spyder project settings
116| .spyderproject
117| .spyproject
118| 
119| # Rope project settings
120| .ropeproject
121| 
122| # mkdocs documentation
123| /site
124| 
125| # mypy
126| .mypy_cache/
127| .dmypy.json
128| dmypy.json
129| 
130| # Pyre type checker
131| .pyre/
132| 
133| # pytype static type analyzer
134| .pytype/
135| 
136| #CUSTOM ADDITION: ignore all .csv datasets, they can be generated from code
137| #and are too big to reasonably have in commit history
138| *.csv


GLOBALS.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Mar 26 12:19:15 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | TESTLEFT  = 1
7  | TESTRIGHT = 2
8  | GOLEFT    = 3
9  | GORIGHT   = 4
10 | NOGOLEFT  = 5
11 | NOGORIGHT = 6
12 | GO        = {GOLEFT, GORIGHT}
13 | NOGO      = {NOGOLEFT, NOGORIGHT}
14 | LEFT      = {TESTLEFT, GOLEFT,NOGOLEFT}
15 | RIGHT     = {TESTRIGHT, GORIGHT,NOGORIGHT}
16 | 
17 | TRIALTYPE = {
18 |     1: 'TESTLEFT',
19 |     2: 'TESTRIGHT',
20 |     3: 'GOLEFT',
21 |     4: 'GORIGHT',
22 |     5: 'NOGOLEFT',
23 |     6: 'NOGORIGHT',
24 |     7: 'STIMCODE7',
25 |     8: 'STIMCODE8'
26 |     }


__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\Clustering\metrics_figs.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Mar  9 11:52:41 2020
3  | 
4  | @author: uic
5  | """
6  | 
7  | import matplotlib.pyplot as plt
8  | from string import ascii_lowercase
9  | 
10 | 
11 | def edit_map(str1,str2):
12 |     res = []
13 |     for a,b in zip(str1,str2):
14 |         res.append(a!=b)
15 |     return res
16 | 
17 | edit_dist = lambda str1,str2:sum(edit_map(str1,str2))
18 | 
19 | 
20 | 'absolutely'
21 | 'conclusion'
22 | 
23 | a='retired'
24 | b='refined'
25 | 
26 | 
27 | 
28 | def format_table(str1,str2):
29 |     fig, axes = plt.subplots(2,1)
30 |     
31 |     data = [[],[],[]]
32 |     n = min(len(str1),len(str2))
33 |     data[0] = list(str1)[0:n-1]
34 |     data[1] = list(str2)[0:n-1]
35 |     data[2] = list(map(int,edit_map(data[0],data[1])))
36 |     for row in data:
37 |         print(row)
38 |     axes[0].axis('tight')
39 |     axes[0].axis('off')
40 |     axes[0].table(
41 |         cellText = data,
42 | #        colWidths = [2/len(data[0])] + [1/len(data[0])]*len(data[0]-1),
43 |         rowLabels = ['First String',
44 |                  'Second String',
45 |                  'Edit Required?'],
46 |         loc = 'center'
47 |         )
48 |     return fig
49 | 
50 | fig = format_table(a,b)
51 | fig.show()


\Clustering\string_clustering.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Mar  9 11:52:41 2020
3  | 
4  | @author: uic
5  | """
6  | import numpy as np
7  | from sklearn.cluster import SpectralClustering
8  | import matplotlib.pyplot as plt
9  | from string import ascii_lowercase
10 | from random import sample
11 | from scipy.linalg import fractional_matrix_power
12 | 
13 | 
14 | 
15 | def edit_map(str1,str2):
16 |     res = []
17 |     for a,b in zip(str1,str2):
18 |         res.append(a!=b)
19 |     return res
20 | 
21 | edit_dist = lambda str1,str2:sum(edit_map(str1,str2))
22 | d = edit_dist
23 | 
24 | def get_n_nearest_neighbours(point,dataset,n):
25 |     opt = [(np.inf,None)]*n
26 |     for idx,val in enumerate(dataset):
27 |         if idx != point:
28 |             d = edit_dist(dataset[point],val)
29 |             if (d,idx)<max(opt):
30 |                 opt[-1] = (d,idx)
31 |                 opt.sort()
32 |     return list(x[1] for x in opt)
33 | 
34 | 
35 | 
36 | def alter(strng,n):
37 |     strng = list(strng)
38 |     idxs = sample(range(len(strng)),n)
39 |     repls = sample(ascii_lowercase,n)
40 |     for idx,repl in zip(idxs,repls):
41 |         strng[idx] = repl
42 |     return ''.join(strng)
43 | 
44 |     
45 | def to_knn_graph_laplacian(dataset,k):
46 |     laplacian = np.zeros((len(dataset),len(dataset)))
47 |     degree = np.copy(laplacian)                     
48 |     for idx,value in enumerate(dataset):
49 |         laplacian[idx][idx] = k
50 |         degree[idx][idx] = k
51 |         neighbors = get_n_nearest_neighbours(idx,dataset,n=k)
52 |         for neighbor in neighbors:
53 |             laplacian[idx][neighbor] = -1
54 |             laplacian[neighbor][idx] = -1
55 |     d = fractional_matrix_power(degree,-0.5)
56 |     return d@laplacian@d
57 | 
58 | def to_knn_graph_affinity(dataset,k):
59 |     affinity = np.zeros((len(dataset),len(dataset)))                   
60 |     for idx,value in enumerate(dataset):
61 |         affinity[idx][idx] = k
62 |         neighbors = get_n_nearest_neighbours(idx,dataset,n=k)
63 |         for neighbor in neighbors:
64 |             affinity[idx][neighbor] = 1
65 |             affinity[neighbor][idx] = 1
66 |     return affinity
67 | 
68 | def pairwise_edit_distances(dataset):
69 |     matr = np.zeros((len(dataset),len(dataset)))
70 |     for idx0, elem0 in enumerate(dataset):
71 |         for idx1, elem1 in enumerate(dataset):
72 |             matr[idx0][idx1] = edit_dist(elem0,elem1)
73 |     return matr
74 | 
75 | def mean_edit_distance(dataset):
76 |     distances = pairwise_edit_distances(dataset)
77 |     return (distances.sum().sum() / (len(dataset)**2))
78 | 
79 | 
80 | def arrays_from_labels(labels):
81 |     result = []
82 |     for i in range(max(labels)+1):
83 |         result.append([])
84 |     for idx,label in enumerate(labels):
85 |         result[label].append(idx)
86 |     return result
87 | 
88 | def data_from_labels(labels,dataset):
89 |     arrays = arrays_from_labels(labels)
90 |     clusters = []
91 |     for i in range(max(labels)+1):
92 |         clusters.append([])
93 |     for idx,ls in enumerate(clusters):
94 |         ls += list(dataset[x] for x in arrays[idx])
95 |     return clusters
96 | 
97 | 
98 | a='absolutely'
99 | b='background'
100| c='conclusion'
101| def gen_data(*strs,n=6):
102|     dataset = []
103|     for i in range(10):
104|         for strng in strs:
105|             dataset.append(alter(strng,n))
106|     return dataset
107| 
108| dataset = gen_data(a,b,c)
109| 
110| def eval_n_clusters(dataset,n):
111|     A = to_knn_graph_affinity(dataset, 10)
112|     clustering = SpectralClustering(n_clusters=n,
113|                    affinity = 'precomputed',
114|                    assign_labels = 'discretize').fit(A)
115|     ls = []
116|     for cluster in data_from_labels(clustering.labels_,dataset):
117|         ls.append(mean_edit_distance(cluster))
118|     return sum(ls)/len(ls)
119| 
120| 
121| def eval_up_to(n_clusters,method = 'mean_dist'):
122|     total_mean_edit_distance = mean_edit_distance(dataset)
123|     ls = []
124|     if method == 'mean_dist':
125|         ls.append(total_mean_edit_distance)
126|     elif method == 'variance':
127|         ls.append(1)
128|     for i in range(2,n_clusters):
129|         if method == 'mean_dist':
130|             ls.append(eval_n_clusters(dataset,i))
131|         elif method == 'variance':
132|             val = eval_n_clusters(dataset,i)
133|             ls.append(val/total_mean_edit_distance)
134|     ls[0] = ls[0] if method=='mean_dist' else 1
135|     return ls
136| 
137| plt.plot(eval_up_to(10,method = 'variance'))
138| plt.plot(eval_up_to(10,method = 'variance'))
139| plt.plot(eval_up_to(10,method = 'variance'))
140| plt.ylim((0,1.1))
141| plt.xlabel("Number of clusters")
142| plt.ylabel("Mean intracluster edit distance")
143| plt.title("""Regardless of underlying data, mean intracluster distance decreases
144|           with increasing number of clusters""")
145| 
146| 
147| 
148| 
149| 
150| 
151| 
152| 
153| 
154| 
155| 
156| 
157| 
158| 
159| 
160| 
161| 


\Clustering\tslearn_figs.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Mar 10 14:06:33 2020
3  | 
4  | @author: uic
5  | """
6  | 
7  | from tslearn.clustering import TimeSeriesKMeans
8  | import numpy as np
9  | import random
10 | import matplotlib.pyplot as plt
11 | 
12 | #First create a dummy dataset
13 | #It will contain 30 sine waves and
14 | #30 sawtooth waves
15 | data = np.zeros((600,100))
16 | for i in range(300):
17 |     #sine waves
18 |     seed = 2*np.pi*random.random()
19 |     data[i] = np.sin(np.linspace(seed, 4*np.pi+seed,num=100))
20 | for i in range(300,600):
21 |     #sawtooth waves
22 |     seed = 2*np.pi*random.random()
23 |     data[i] = 0.4*(np.mod(np.linspace(seed, 4*np.pi+seed,num=100),2*np.pi)/np.pi-1)
24 | 
25 | model = TimeSeriesKMeans(
26 |     n_clusters = 2,
27 |     metric = 'dtw',
28 |     verbose = 1
29 |     )
30 | 
31 | 
32 | fig,axes = plt.subplots(1,3)
33 | for axis in axes:
34 |     axis.set_xlabel('Time')
35 |     axis.set_ylabel('Signal')
36 |     axis.set_ylim((-1.1,1.1))
37 | 
38 | 
39 | for datum in data[0:5]:
40 |     axes[0].plot(datum,
41 |              color = 'green')
42 |     
43 | for datum in data[300:305]:
44 |     axes[1].plot(datum,
45 |              color = 'red')
46 | axes[0].set_title('Examples of training data')
47 | axes[1].set_title('Examples of training data')
48 | model.fit(data)
49 | sines_cluster_1 = 0
50 | tris_cluster_1 = 0
51 | sines_cluster_2 = 0
52 | tris_cluster_2 = 0
53 | 
54 | #this comparator is broken!!
55 | for label in model.labels_[:300]:
56 |     if label:
57 |         sines_cluster_1+=1
58 |     else:
59 |         tris_cluster_1+=1
60 | for label in model.labels_[300:]:
61 |     if label:
62 |         sines_cluster_2+=1
63 |     else:
64 |         tris_cluster_2+=1
65 | sines_cluster_1 /= 3
66 | sines_cluster_2 /= 3
67 | tris_cluster_1  /= 3
68 | tris_cluster_2  /= 3
69 | a = f'''Cluster 1 (n={sum(model.labels_)}) contained {sines_cluster_1:.1f}% sine waves and
70 |         {tris_cluster_1:.1f}% sawtooth waves'''
71 | b = f'''Cluster 2 (n={sum(np.logical_not(model.labels_))}) contained {sines_cluster_2:.1f}% sine waves and
72 |         {tris_cluster_2:.1f}%sawtooth waves'''
73 | #broken code ends here
74 | 
75 | axes[2].set_title('Barycenters of clusters')
76 | axes[2].plot(model.cluster_centers_[0])
77 | axes[2].plot(model.cluster_centers_[1])
78 | axes[0].annotate(a, (0,0), (0, -45), xycoords='axes fraction', textcoords='offset points', va='top')
79 | axes[1].annotate(b, (0,0), (0, -45), xycoords='axes fraction', textcoords='offset points', va='top')
80 | plt.tight_layout()
81 | fig.show()


\Clustering\tslearn_spks.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Mar 10 14:06:33 2020
3  | 
4  | @author: uic
5  | """
6  | 
7  | from tslearn.clustering import TimeSeriesKMeans, silhouette_score
8  | import numpy as np
9  | from accdatatools.Utils.deeploadmat import loadmat
10 | import matplotlib.pyplot as plt
11 | 
12 | 
13 | 
14 | def get_fitted_model(dataset, clusters = 3, timepoints = 0, verbose = False,
15 |                      return_dataset = True):
16 |     if timepoints!=0:
17 |         dataset = dataset[:,:timepoints]
18 |     if verbose: print(f'Segmenting data into {clusters} clusters...')
19 |     model = TimeSeriesKMeans(
20 |         n_clusters = clusters,
21 |         n_init = 10,
22 |         metric = 'dtw',         #Dynamic time warping
23 |         verbose = verbose,
24 |         n_jobs = -1             #Use all cores
25 |         )
26 |     model.fit(dataset)
27 |     return model
28 | 
29 | def get_frame_times(timeline_path):
30 |     timeline = loadmat(timeline_path)
31 |     timeline = timeline['Timeline']
32 |     frame_counter = timeline['rawDAQData'][:,2]
33 |     timestamps = timeline['rawDAQTimestamps']
34 |     frame_times = np.zeros(int(frame_counter[-1]))
35 |     current_frame = 0
36 |     for timestamp,no_frames in zip(timestamps,frame_counter):
37 |         if no_frames == (current_frame + 1):
38 |             frame_times[current_frame] = timestamp
39 |             current_frame+=1
40 |         elif no_frames != current_frame:
41 |             raise IOError('Need a better error message')
42 |     return frame_times
43 | 
44 | 
45 | def get_inertia_stats(models):
46 |     inertias = []
47 |     for model in models:
48 |         inertias.append(model.inertia_)
49 |     inertias = list(enumerate(inertias,2))
50 |     xs = list(x[0] for x in inertias)
51 |     ys = list(x[1] for x in inertias)
52 |     return xs, ys
53 | 
54 | 
55 | if __name__=='__main__':
56 |     TIMELINE = 'C:/Users/Vivian Imbriotis/Desktop/2018-04-26_01_CFEB106/1-3-2018-04-26_01_CFEB106_1/metadata matlab/2018-04-26_01_CFEB106_Timeline.mat'
57 |     SPKS = 'C:/Users/Vivian Imbriotis/Desktop/2018-04-26_01_CFEB106/1-3-2018-04-26_01_CFEB106_1/npy/spks.npy'
58 |     TIMEPOINTS = 200
59 |     MAX_K = 6
60 |     stamps = get_frame_times(TIMELINE)
61 | 
62 |     scores = []
63 |     models = []
64 |     data = np.load(SPKS)
65 |     for i in range(2,MAX_K+1):
66 |         model = get_fitted_model(data, 
67 |                             clusters = i, 
68 |                             timepoints = TIMEPOINTS,
69 |                             verbose = True)
70 |         models.append(model)
71 |     for model in models:
72 |         scores.append(silhouette_score(
73 |             data[:,0:TIMEPOINTS],
74 |             model.labels_,
75 |             metric = 'dtw',
76 |             n_jobs = -1,
77 |             verbose = True))
78 |     plt.plot(list(range(2,MAX_K+1)),scores)
79 |     plt.title("Performance of KMeans on axonal data")
80 |     plt.xlabel('Number of clusters')
81 |     plt.ylabel('Silhouette Score')
82 |     plt.show()


\Clustering\tslearn_spks_vs_events.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Mar 10 14:06:33 2020
3  | 
4  | @author: uic
5  | """
6  | 
7  | from tslearn.clustering import TimeSeriesKMeans, silhouette_score
8  | import numpy as np
9  | from accdatatools.Utils.deeploadmat import loadmat
10 | import matplotlib.pyplot as plt
11 | import accdatatools.GLOBALS as g
12 | import pickle
13 | 
14 | 
15 | class Trial:
16 |     def __init__(self,struct):
17 |         self.stim_id        = struct.stimID
18 |         self.correct        = struct.correct
19 |         self.start_trial    = struct.timing.StartTrial
20 |         self.start_stimulus = struct.timing.StimulusStart
21 |         self.start_response = struct.timing.ResponseStart
22 |         self.end_response   = struct.timing.ResponseEnd
23 |         self.end_trial      = struct.timing.EndClearDelay
24 |     def __repr__(self):
25 |         trialtype = g.TRIALTYPE[self.stim_id]
26 |         response = 'correct' if self.correct else 'incorrect'
27 |         return f'{trialtype} trial with {response} response'
28 | 
29 | 
30 | 
31 | 
32 | def get_fitted_model(dataset, clusters = 3, start = 0, end = 0, verbose = False,
33 |                      return_dataset = True):
34 |     if end!=0 or start!=0:
35 |         dataset = dataset[:,start:end]
36 |     if verbose: print(f'Segmenting data into {clusters} clusters...')
37 |     metric_params = {
38 |         'global_constraint':'sakoe_chiba'
39 |         }
40 |     model = TimeSeriesKMeans(
41 |         n_clusters = clusters,
42 |         n_init = 1,
43 |         metric = 'dtw',         #Dynamic time warping
44 |         verbose = verbose,
45 |         n_jobs = -1,             #Use all cores
46 |         metric_params = metric_params)
47 |     model.fit(dataset)
48 |     return model
49 | 
50 | def get_frame_times(timeline_path):
51 |     timeline = loadmat(timeline_path)
52 |     timeline = timeline['Timeline']
53 |     frame_counter = timeline['rawDAQData'][:,2]
54 |     timestamps = timeline['rawDAQTimestamps']
55 |     frame_times = np.zeros(int(frame_counter[-1]))
56 |     current_frame = 0
57 |     for timestamp,no_frames in zip(timestamps,frame_counter):
58 |         if no_frames == (current_frame + 1):
59 |             frame_times[current_frame] = timestamp
60 |             current_frame+=1
61 |         elif no_frames != current_frame:
62 |             raise IOError('Need a better error message')
63 |     return frame_times
64 | 
65 | 
66 | def _get_trial_structs(psychstim_path):
67 |     matfile=loadmat(psychstim_path)
68 |     expData = matfile["expData"]
69 |     trialData = expData["trialData"]
70 |     trials = []
71 |     for trial in trialData:
72 |         trials.append(trial)
73 |     return trials
74 | 
75 | def get_trials(psychstim_path):
76 |     structs = _get_trial_structs(psychstim_path)
77 |     trials = []
78 |     for struct in structs:
79 |         trials.append(Trial(struct))
80 |     return trials
81 | 
82 | 
83 | def get_inertia_stats(models):
84 |     inertias = []
85 |     for model in models:
86 |         inertias.append(model.inertia_)
87 |     inertias = list(enumerate(inertias,2))
88 |     xs = list(x[0] for x in inertias)
89 |     ys = list(x[1] for x in inertias)
90 |     return xs, ys
91 | 
92 | 
93 | if __name__=='__main__':
94 |     TIMELINE   = 'Data/2018-04-26_01_CFEB106_Timeline.mat'
95 |     SPKS       = 'Data/spks.npy'
96 |     PSYCHSTIM  = 'Data/2018-04-26_01_CFEB106_psychstim.mat'
97 |     CLUSTERS   = 2
98 |     FIRST_TRIAL= 0
99 |     LAST_TRIAL = 10
100|     
101| 
102|     frametimes = get_frame_times(TIMELINE)
103|     trials = get_trials(PSYCHSTIM)[FIRST_TRIAL:LAST_TRIAL+1]
104|     
105|     start_time = trials[0].start_trial
106|     end_time   = trials[-1].end_trial
107|     
108| 
109|     start_idx = frametimes.searchsorted(start_time)
110|     end_idx = frametimes.searchsorted(end_time)
111|     data = np.load(SPKS)[:,start_idx:end_idx]
112|     print(data.shape)
113|     model = get_fitted_model(data, 
114|                             clusters = CLUSTERS, 
115|                             start = start_idx,
116|                             end = end_idx,
117|                             verbose = True)
118|     fig,ax = plt.subplots()
119|     for barycenter in model.cluster_centers_:
120|         ax.plot(barycenter)
121|     ylim = ax.get_ylim()
122|     for trial in trials:
123|         max_val = trial.end_trial
124|         response = plt.Rectangle(
125|             xy = (trial.start_response,ylim[0]),
126|             width = (trial.end_response - trial.start_response),
127|             height = ylim[1],
128|             angle = 0,
129|             color = 'green' if trial.correct else 'red',
130|             alpha = 0.5)
131|         ax.add_patch(response)
132|     ax.set_xlim((0,max_val+3))
133|     ax.set_title('ACC Bouton Cluster Barycenters with associated trials')
134|     ax.set_xlabel('Frame')
135|     ax.set_ylabel('Deconvolved firing rate')
136|     plt.savefig('Barycentres.png')
137|     with open('model.pkl','wb') as file:
138|         pickle.dump(model,file)


\Clustering\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\DataCleaning\determine_dprime.py
0  | ##Stimulus feature encodings
1  | GOLEFT    = 3
2  | GORIGHT   = 4
3  | NOGOLEFT  = 5
4  | NOGORIGHT = 6
5  | GO        = {GOLEFT, GORIGHT}
6  | NOGO      = {NOGOLEFT, NOGORIGHT}
7  | 
8  | 
9  | __debug = False
10 | from accdatatools.Utils.deeploadmat import loadmat
11 | import accdatatools.Utils.path as p
12 | from scipy.stats import norm
13 | from sys import float_info
14 | import numpy as np
15 | import os
16 | 
17 | 
18 | 
19 | ##  Statistics helper functions
20 | 
21 | def restrict_to_open_interval(x, a, b):
22 |     '''Rounds x to the nearest representable element of the open interval (a,b)
23 |     ::rtype:: float
24 |     '''
25 |     if x >= b:
26 |         x = b -  float_info.min
27 |     elif x <= a:
28 |         x = a + float_info.min
29 |     return x
30 | 
31 | def d_prime(hit_rate, false_alarm_rate):
32 |     hit_rate = restrict_to_open_interval(hit_rate, 0, 1)
33 |     false_alarm_rate = restrict_to_open_interval(false_alarm_rate, 0, 1)
34 |     return norm.ppf(hit_rate)  - norm.ppf(false_alarm_rate)
35 | 
36 | #--------------------------------------------------
37 | 
38 | # def extract_stim_corr_resptype(matfile_path):
39 | #     '''Returns a list of 3-tuples of (stimIDs, correct, responseType) from
40 | #     a 'psychstim' matlab file. 
41 | #     StimID is a numeric code for the stimulus type,
42 | #     correct is true for a successful behaviour response else false,
43 | #     responseType is true if Go behavior else false.
44 | #     ::rtype:: [(int,int,int)]
45 | #     '''
46 | #     file = loadmat(matfile_path)
47 | #     trials = file['expData']['trialData']
48 | #     try:
49 | #         stimIDs   = [trial.stimID for trial in trials]
50 | #         corrects   = [trial.correct for trial in trials]
51 | #         responses = [trial.responseType for trial in trials]
52 | #         return zip(stimIDs,corrects,responses)
53 | #     except AttributeError:
54 | #         raise AttributeError(
55 | #                 "That matlab file does not have behavioral structure")
56 | 
57 | def calc_d_prime(matfile_path):
58 |     '''
59 |         Calculates the Dprime statistic for a trial from that trial's psychstim
60 |         matlab file.
61 | 
62 |     Parameters
63 |     ----------
64 |     matfile_path : str
65 |         Path to a psychstim.mat matlab file.
66 | 
67 |     Raises
68 |     ------
69 |     AttributeError
70 |         Raised when the trial referenced by matfile_path is either not a 
71 |         formatted psychstim file or is not a behavioural experiment and 
72 |         d-prime cannot be calculated from it.
73 |         Specifically checks whether the file has the stimID, correct, 
74 |         and responseType fields.
75 | 
76 |     Returns
77 |     -------
78 |     Int
79 |         The dprime statistic for the experiment.
80 | 
81 |     '''
82 |     hits         = 0
83 |     false_alarms = 0
84 |     n_go   = 0
85 |     n_nogo = 0
86 |     file = loadmat(matfile_path)
87 |     trials = file['expData']['trialData']
88 |     try:
89 |         for trial in trials:
90 |             if trial.stimAttributes.go:
91 |                 hits += trial.correct
92 |                 n_go+=1
93 |             else:
94 |                 false_alarms += not trial.correct
95 |                 n_nogo+=1
96 |     except AttributeError:
97 |         raise AttributeError(
98 |             'That file does not refer to an experimental trial'
99 |             )
100|     try:
101|         hit_rate, false_alarm_rate = (hits/n_go, 
102|                                   false_alarms/n_nogo)
103|         return d_prime(hit_rate,false_alarm_rate)
104|     except ZeroDivisionError:
105|         return np.nan
106| 
107| def get_dprimes_from_dirtree(path):
108|     '''
109|     Determines the Dprime statistic for each trial for each mouse in the
110|     directory rooted at path.
111| 
112| 
113|     Parameters
114|     ----------
115|     path : str
116|         The root path from which to fetch trials.
117| 
118|     Returns
119|     -------
120|     results : list
121|         A list of 3-tuples of 
122|         (str mouse_ID, str experiment_path, float dprime)
123| 
124|     '''
125|     performances = {}
126| 
127|     for root, dirs, files in os.walk(path):
128|         for file in files:
129|             if 'psychstim' in file:
130|                 try:
131|                     performances[root] = calc_d_prime(os.path.join(root,file))
132|                 except AttributeError:
133|                     if(__debug==True):
134|                         print(f"Non-behavioural exp located at {root}")
135|                     else:
136|                         pass
137|     results = []
138|     
139|     for root, dprime in performances.items():
140|         if np.isfinite(dprime):
141|             results.append((
142|                 p.mouse_id(root),
143|                 p.exp_id(root),
144|                 dprime))
145| 
146|     return results
147| 
148| 
149| if __name__=='__main__':
150|     # import pickle
151|     # #For the purpose of graphing the dprime statistics, dump it as a pickle
152|     # #file.
153|     # results = get_dprimes_from_dirtree('D:\\Local_Repository')
154|     # # with open('C:\\Users\\uic\\Desktop\\resultsdump.p', 'wb') as file:
155|     #     results = get_dprimes_from_dirtree('D:\\Local_Repository')
156|     #     pickle.dump(results, file)
157|     
158|     print(sum(len([get_dprimes_from_dirtree('D:\\Local_Repository').items()][1])))
159| 
160| 
161| 


\DataCleaning\group_recordings_into_datasets.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Aug 11 21:40:19 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | #yikes, both of these files should probably get this function
8  | #from an abstract file
9  | import os
10 | import time
11 | import pickle as pkl
12 | from copy import copy
13 | import random
14 | 
15 | seed = 987654321
16 | random.seed(seed)
17 | 
18 | import pandas as pd
19 | import numpy as np
20 | import matplotlib.pyplot as plt
21 | 
22 | from accdatatools.Summaries.produce_trial_summary_document import (
23 |     get_classes_of_recording,
24 |     get_count_and_stats)
25 | from accdatatools.Utils.path import get_exp_path,get_psychstim_path,get_exp_id
26 | from accdatatools.DataCleaning.determine_dprime import calc_d_prime
27 | from accdatatools.DataVisualisation.mean_videos import AnimalMeanVideoFigure
28 | 
29 | class RecordingClassSummary:
30 |     def __init__(self,df,signature,dic,name,drive = "H://"):
31 |         ls_of_recordings = dic[signature]
32 |         self.recordings = ls_of_recordings
33 |         self.df = df
34 |         recording_paths = list(map(lambda p:get_exp_path(p,drive),
35 |                                         self.recordings))
36 |         psychstims = [get_psychstim_path(r) for r in recording_paths]
37 |         self.dprimes = [calc_d_prime(r) for r in psychstims]
38 |         self.df_repr = pd.DataFrame((self.recordings,self.dprimes)).transpose()
39 |         self.df_repr.columns = ["recording_id","dprime"]
40 |         self.df_repr[self.df_repr==np.inf] = np.nan
41 |         self.df_repr = self.df_repr[self.df_repr.dprime>1]
42 |         self.recordings = self.df_repr.recording_id.values
43 |         subset = self.df[self.df.recording_id.isin(self.recordings)]
44 |         self.pooled_stats = get_count_and_stats(subset,subsetting=False)
45 |         self.name = name
46 |         self.signature = signature
47 |         self.drive = drive
48 |         self.recording_paths = list(map(lambda p:get_exp_path(p,drive),
49 |                                         self.recordings))
50 |     def set_recordings(self,ls_of_exp_paths):
51 |         ls_of_exp_ids = [get_exp_id(i) for i in ls_of_exp_paths]
52 |         self.recordings = ls_of_exp_ids
53 |         self.df_repr[self.df_repr.recording_id.isin(self.recordings)]
54 |         subset = self.df[self.df.recording_id.isin(self.recordings)]
55 |         self.pooled_stats = get_count_and_stats(subset,subsetting=False)
56 |         self.recording_paths = ls_of_exp_paths
57 |     def __repr__(self):
58 |         intro = f"{self.name} recordings with pooled stats {self.pooled_stats}"
59 |         varying = "This class varied across parameters like so:"
60 |         params = map(lambda a:a.__repr__(),self.signature)
61 |         sep = "-------------\n"
62 |         num = f"There are {len(self.df_repr)} recordings in this class with dprime>1"
63 |         lines_to_print = [intro,varying,*params,sep,num,sep,
64 |                           self.df_repr.__repr__()]
65 |         return "\n".join(lines_to_print)
66 | 
67 | 
68 | recording_classes, df, planes = get_classes_of_recording(root="H://")
69 | 
70 | 
71 | both_sides_high_contrast = (('affirmative', (False, 1)),
72 |                             ('contrast', (1.0,)),
73 |                             ('correct', (0.0, 1.0)),
74 |                             ('go', (0.0, 1.0)),
75 |                             ('side', ('left', 'right')),
76 |                             ('task', ('bGoNoGoLickAdapt',)),
77 |                             ('test', (False, True)))
78 |  
79 | 
80 | low_contrast =  (('affirmative', (False, 1)),
81 |                   ('contrast', (0.1, 0.5)),
82 |                   ('correct', (0.0, 1.0)),
83 |                   ('go', (0.0, 1.0)),
84 |                   ('side', ('left', 'right')),
85 |                   ('task', ('bGoNoGoLickFull',)),
86 |                   ('test', (False, True)))
87 |                  
88 | 
89 | left_only_high_contrast = (('affirmative', (False, 1)),
90 |                             ('contrast', (1.0,)),
91 |                             ('correct', (0.0, 1.0)),
92 |                             ('go', (0.0, 1.0)),
93 |                             ('side', ('left',)),
94 |                             ('task', ('bGoNoGoLickAdaptOne',)),
95 |                             ('test', (False, True)))
96 |  
97 | both_sides_high_contrast_summary = RecordingClassSummary(
98 |                                             df,
99 |                                             both_sides_high_contrast,
100|                                             recording_classes,
101|                                             name = "Bilateral High Contrast",
102|                                             drive = "H://"
103|                                             )
104| 
105| left_only_high_contrast_summary = RecordingClassSummary(
106|                                             df,
107|                                             left_only_high_contrast,
108|                                             recording_classes,
109|                                             name = "Left High Contrast",
110|                                             drive = "H://"
111|                                             )
112| 
113| low_contrast_summary = RecordingClassSummary(df,
114|                                             low_contrast,
115|                                             recording_classes,
116|                                             name = "Low Contrast",
117|                                             drive = "H://"
118|                                             )
119| 
120| 
121| # Now we need to pare down the recording classes to remove excess duplicate
122| # recordings of the same cortical region of the same animal, so one neuron
123| # is not assigned multiple ROIs across recordings!
124|     
125| # we need a function :: [exp_ids] -> [[exp_ids]]
126| # that maps a list of exp_ids of one animal to a list of lists of experiment ids
127| # that imaged a single cortical area. The best way to do this without training
128| # data is probably just visual inspection, because we don't need to do it for very
129| # many experiments.
130| 
131| 
132| def subtype_experiments_by_mouse(ls_of_exp_paths):
133|     ls_of_exp_paths = copy(ls_of_exp_paths)
134|     exps_by_mouse = []
135|     mouse_paths = set([os.path.split(s)[0] for s in ls_of_exp_paths])
136|     exps_by_mouse = [list(filter(lambda s:mouse_path in s,ls_of_exp_paths))
137|                      for mouse_path in mouse_paths]
138|     return sorted(exps_by_mouse)
139| 
140| def manually_group_recordings_by_cortical_area(ls_of_exp_paths):
141|     plt.ion()
142|     subgroups = []
143|     fig = AnimalMeanVideoFigure(ls_of_exp_paths=ls_of_exp_paths)
144|     sucessfully_retreived_paths = fig.ls_of_exp_paths
145|     remaining_exp_paths = copy(sucessfully_retreived_paths)
146|     print("""
147|           Identify a group of these mean images that show the same
148|           area of cortex. Provide a space-seperated 0-based list of 
149|           intergers to indicate each member of the group, then press enter.
150|           If all remaining mean images show different regions of cortex, 
151|           press enter without entering a list.""")
152|     while remaining_exp_paths:
153|         fig = AnimalMeanVideoFigure(ls_of_exp_paths = remaining_exp_paths)
154|         try:
155|             print(remaining_exp_paths)
156|             fig.canvas.manager.window.setGeometry(0,50,900,1000)
157|             ls = []
158|             line = ""
159|             fig.show()
160|             for i in range(15):
161|                 time.sleep(0.05)
162|                 fig.canvas.draw_idle()
163|                 plt.pause(.1)
164|             line = input("-> ")
165|             fig.show()
166|             ls.extend([int(i) for i in line.split(" ") if i!=""])
167|             if ls:
168|                 subgroups.append([remaining_exp_paths[i] for i in ls])
169|                 remaining_exp_paths = [e for i,e in enumerate(remaining_exp_paths)
170|                                        if i not in ls]
171|             else:
172|                 subgroups.extend([[path] for path in remaining_exp_paths])
173|                 remaining_exp_paths = []
174|         except Exception as e:
175|             raise e #debugging
176|             print("Something went wrong. Enter a space-seperated list of ints.")
177|         finally:
178|             plt.close(fig.fig)
179|             plt.close("all")
180|     return subgroups
181|            
182| def manually_group_recording_class_by_cortical_area(summary_obj):
183|     res = []
184|     for single_mouses_exps in subtype_experiments_by_mouse(
185|             summary_obj.recording_paths):
186|         res.extend(
187|             manually_group_recordings_by_cortical_area(
188|                 single_mouses_exps)
189|             )
190|     #drop multiple recordings of the same cortical area
191|     unique_cortical_areas = [random.choice(i) for i in res]
192|     return_obj = copy(summary_obj)
193|     return_obj.set_recordings(unique_cortical_areas)
194|     return return_obj
195| 
196| 
197| 
198| 
199| 
200| if __name__=="__main__":
201| 
202|     both_sides_high_contrast = (('affirmative', (False, 1)),
203|                             ('contrast', (1.0,)),
204|                             ('correct', (0.0, 1.0)),
205|                             ('go', (0.0, 1.0)),
206|                             ('side', ('left', 'right')),
207|                             ('task', ('bGoNoGoLickAdapt',)),
208|                             ('test', (False, True)))
209|  
210| 
211|     low_contrast =  (('affirmative', (False, 1)),
212|                       ('contrast', (0.1, 0.5)),
213|                       ('correct', (0.0, 1.0)),
214|                       ('go', (0.0, 1.0)),
215|                       ('side', ('left', 'right')),
216|                       ('task', ('bGoNoGoLickFull',)),
217|                       ('test', (False, True)))
218|                      
219|     
220|     left_only_high_contrast = (('affirmative', (False, 1)),
221|                                 ('contrast', (1.0,)),
222|                                 ('correct', (0.0, 1.0)),
223|                                 ('go', (0.0, 1.0)),
224|                                 ('side', ('left',)),
225|                                 ('task', ('bGoNoGoLickAdaptOne',)),
226|                                 ('test', (False, True)))
227|      
228|     both_sides_high_contrast_summary = RecordingClassSummary(
229|                                                 df,
230|                                                 both_sides_high_contrast,
231|                                                 recording_classes,
232|                                                 name = "Bilateral High Contrast",
233|                                                 drive = "H://"
234|                                                 )
235|     
236|     left_only_high_contrast_summary = RecordingClassSummary(
237|                                                 df,
238|                                                 left_only_high_contrast,
239|                                                 recording_classes,
240|                                                 name = "Left High Contrast",
241|                                                 drive = "H://"
242|                                                 )
243|     
244|     low_contrast_summary = RecordingClassSummary(df,
245|                                                 low_contrast,
246|                                                 recording_classes,
247|                                                 name = "Low Contrast",
248|                                                 drive = "H://"
249|                                                 )
250| 
251|     lc_grouped = manually_group_recording_class_by_cortical_area(
252|         low_contrast_summary)
253|     print("YOU'RE 1 THIRD DONE UWU")
254|     lohc_grouped = manually_group_recording_class_by_cortical_area(
255|         left_only_high_contrast_summary)
256|     print("YOU'RE 2 THRIDS DONE UWU")
257|     bshc_grouped = manually_group_recording_class_by_cortical_area(
258|         both_sides_high_contrast_summary)
259| 
260|     with open("both_sides_high_contrast_uncleaned.pkl",'wb') as file:
261|         pkl.dump(both_sides_high_contrast_summary.recording_paths, file)
262|     with open("left_only_high_contrast_uncleaned.pkl",'wb') as file:
263|         pkl.dump(left_only_high_contrast_summary.recording_paths, file)
264|     with open("low_contrast_uncleaned.pkl",'wb') as file:
265|         pkl.dump(low_contrast_summary.recording_paths, file)
266|     with open("both_sides_high_contrast.pkl",'wb') as file:
267|         pkl.dump(bshc_grouped.recording_paths, file)
268|     with open("left_only_high_contrast.pkl",'wb') as file:
269|         pkl.dump(lohc_grouped.recording_paths, file)
270|     with open("low_contrast.pkl",'wb') as file:
271|         pkl.dump(lc_grouped.recording_paths, file)
272|     
273|     for i in (bshc_grouped,
274|               lohc_grouped,
275|               lc_grouped):
276|         print("\n\n\n")
277|         print(i)


\DataCleaning\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\DataVisualisation\dprime_of_all_recordings_figure.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | import matplotlib.pyplot as plt
3  | import numpy as np
4  | import pickle
5  | 
6  | results = pickle.load(open('C:\\Users\\uic\\Desktop\\resultsdump.p','rb'))
7  | 
8  | 
9  | cols = int(len(results)**0.5)
10 | rows = len(results)//cols + 1
11 | 
12 | figure, axes = plt.subplots(rows,cols, figsize = (12,12))
13 | axes = axes.flatten()
14 | 
15 | for idx, (mouse, (experiment,dprimes)) in enumerate(results.items()):
16 |     dprimes.sort()
17 |     xords = np.arange(len(dprimes))
18 |     axes[idx].bar(xords, dprimes, 0.8, align='edge')
19 |     axes[idx].set_title(mouse[mouse.rfind('\\')+1:])
20 |     axes[idx].set_xlabel(f'Trials (n={len(dprimes)})')
21 |     axes[idx].set_ylabel('Dprime')
22 |     axes[idx].set(xlim=(0,len(dprimes)), ylim=(-6,6))
23 |     axes[idx].set_xbound(lower=0, upper=len(dprimes))
24 | 
25 | for idx, axis in enumerate(axes):
26 |     if idx>=len(results):
27 |         axis.remove()
28 | 
29 | plt.tight_layout()
30 | plt.savefig('figure.png')


\DataVisualisation\eyecam_frametime_figure.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jul 13 17:52:47 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | from pupil_size_extraction import get_pupil_size_over_time
8  | from deeploadmat import loadmat
9  | import numpy as np
10 | from trial_extraction import rising_edges, falling_edges, rising_or_falling_edges
11 | import matplotlib.pyplot as plt
12 | 
13 | def as_time(n):
14 |     return f"{int(n)//60}:{n%60:.2f}"
15 | 
16 | def intersperse_events(input_array, n):
17 |     '''
18 |     Replaces each single True value in the input array with N evenly 
19 |     distributed True values.
20 |     
21 |     >>> intersperse_events(np.array([1,0,0,0,1,0,1]),2).astype('int')
22 |     Out: array([1, 0, 1, 0, 1, 1, 0])
23 | 
24 |     Parameters
25 |     ----------
26 |     input_array : Array of Bool or Array of Bool-like
27 |     n : int
28 | 
29 |     Returns
30 |     -------
31 |     output_array : Array of Bool
32 |     '''
33 |     #get an array of true indexes
34 |     events, = np.where(input_array)
35 |     output_array = np.zeros(input_array.shape,dtype=bool)
36 |     #in terms of indexes, where should all the new events go?
37 |     #Well, just construct a linearly spaced vector between
38 |     #each event, including the starting but not the stopping
39 |     #event. This will get (input_array-1)*N events!
40 |     for (this_event,next_event) in zip(events[:-1],events[1:]):
41 |         interspersed_events = np.linspace(this_event,next_event,n,endpoint=False)
42 |         interspersed_events = np.round(interspersed_events).astype('int')
43 |         output_array[interspersed_events] = True
44 |     return output_array
45 | 
46 | timeline_path = ("H:\\Local_Repository\\CFEB014\\2016-05-28_02_CFEB014\\"+
47 | "2016-05-28_02_CFEB014_Timeline.mat")
48 | 
49 | h5_path = ("C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/videos/"+
50 |            "2016-05-28_02_CFEB014_eyeDLC_resnet50_micepupilsJul9shuffle1_1030000.h5")
51 | 
52 | def timing_plot():
53 |     obj = loadmat(timeline_path)
54 |     timeline = obj["Timeline"]
55 |     columns = np.array([i.name for i in timeline["hw"]["inputs"]])
56 |     eye_camera_strobe = timeline["rawDAQData"][:,np.where(columns=="eyeCameraStrobe")]
57 |     eye_camera_strobe = eye_camera_strobe.reshape(-1) #Remove excess axes
58 |     edges = rising_or_falling_edges(eye_camera_strobe, 0.5)
59 |     eye_frames = intersperse_events(edges,n=20)
60 |     timestamps = timeline['rawDAQTimestamps']
61 |     eye_frame_times = timestamps[eye_frames]
62 | 
63 |     plt.plot(timestamps[7500:15000:10], eye_camera_strobe[7500:15000:10], label = 'eyeCameraStrobe',
64 |              linewidth = 3); 
65 |     plt.plot(timestamps[7500:15000],eye_frames[7500:15000],label="Predicted eyecamera frames")
66 |     plt.legend(); 
67 |     plt.xlabel("Time (s) {from Timeline.rawDAQTimestamps}")
68 |     plt.ylabel("Signal")
69 |     plt.show()
70 | if __name__=="__main__":
71 | 	timing_plot()
72 | 


\DataVisualisation\fitted_vs_realvals.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jun 22 18:26:34 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | import matplotlib.pyplot as plt
8  | import pandas as pd
9  | import seaborn
10 | seaborn.set()
11 | 
12 | def main():
13 |     data = pd.read_csv("C:\\Users\\Vivian Imbriotis\\Documents\\vivFittedVsReal.csv")
14 |     fig,ax = plt.subplots()
15 |     real_data = ax.plot(data["roi.dF_on_F"], color = 'black')[0]
16 |     print(real_data)
17 |     real_data.set_label("Real data")
18 |     fitted_vals = ax.plot(data["model.fitted.values"], color='red')[0]
19 |     fitted_vals.set_label("Fitted linear model based on licking behavior")
20 |     ax.legend()
21 |     ax.set_title("ROI 2016-05-31_02_CFEB013 [169]")
22 |     fig.show()
23 |     return data
24 | 
25 | if __name__=="__main__":
26 |     data=main()


\DataVisualisation\kernel_heat_map.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Wed Jul  1 19:03:37 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import pandas as pd
7  | import seaborn
8  | import matplotlib.pyplot as plt
9  | import numpy as np
10 | from rastermap.mapping import Rastermap
11 | 
12 | class TrialKernelFigure:
13 |     def __init__(self,csv):
14 |         '''
15 |         An encapsulation of a heatmap figure for all
16 |         neurons' responses over the course of a trial.
17 |         
18 | 
19 |         Parameters
20 |         ----------
21 |         exp_path : string
22 |             The root path of an experiment.
23 | 
24 |         '''
25 |         seaborn.set_style("dark")
26 |         print(f"Loading data from {csv}")
27 |         self.df = pd.read_csv(csv)
28 |         self.licks = self.df.iloc[:,2:23].dropna().to_numpy()
29 |         self.trials = self.df.iloc[:,23:49].dropna().to_numpy()
30 |         self.r = Rastermap()
31 | 
32 |     def showtrials(self):
33 |         self.r.fit(self.licks)
34 |         low = np.percentile(self.trials ,5)
35 |         high = np.percentile(self.trials, 99)
36 |         plt.imshow(np.clip(self.licks,low,high)[self.r.isort])
37 |         plt.show()
38 |         
39 |     def showlicks(self):
40 |         self.r.fit(self.trials)
41 |         low = np.percentile(self.licks ,5)
42 |         high = np.percentile(self.licks, 99)
43 |         plt.imshow(np.clip(self.trials,low,high)[self.r.isort])
44 |         
45 |     def showgo(self):
46 |         fig, ax = plt.subplots(nrows = 3, ncols = 1)
47 |         ax
48 | 
49 | 
50 | if __name__=="__main__":
51 |     fig = TrialKernelFigure("coefficientArray.csv")


\DataVisualisation\mean_videos.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Aug 14 19:34:34 2020
3  | 
4  | @author: viviani
5  | """
6  | import os
7  | from math import ceil, sqrt
8  | from itertools import zip_longest
9  | 
10 | import numpy as np
11 | import matplotlib.pyplot as plt
12 | from matplotlib import gridspec
13 | import seaborn
14 | 
15 | from accdatatools.Utils.path import get_mouse_path, get_exp_path
16 | from accdatatools.Utils.map_across_dataset import no_of_planes
17 | 
18 | 
19 | plt.ioff()
20 | class AnimalMeanVideoFigure:
21 |     def __init__(self, mouse_id = None, drive = "H://",
22 |                   ls_of_exp_ids = None, ls_of_exp_paths=None):
23 |         if sum((not mouse_id,not ls_of_exp_ids, not ls_of_exp_paths))!=2:
24 |             raise ValueError("Supply exactly one of a mouse_id, a list of "
25 |                              +"exp_ids or a list of exp_paths!")
26 |         seaborn.set_style("dark")
27 |         if mouse_id:
28 |             experiment_paths = []
29 |             animal_path = get_mouse_path(mouse_id, drive)
30 |             
31 |             for experiment in os.listdir(animal_path):
32 |                 try:
33 |                     experiment_paths.append(
34 |                         get_exp_path(experiment,drive)
35 |                         )
36 |                 except FileNotFoundError: pass
37 |         elif ls_of_exp_ids:
38 |             experiment_paths = [get_exp_path(id,drive) for id in ls_of_exp_ids]
39 |         elif ls_of_exp_paths:
40 |             experiment_paths = ls_of_exp_paths
41 |         mean_images = {}
42 |         self.ls_of_exp_paths = []
43 |         for experiment_path in experiment_paths:
44 |             try:
45 |                 ops_path = os.path.join(experiment_path,
46 |                                    "suite2p",
47 |                                    "plane0",
48 |                                    "ops.npy")
49 |                 ops = np.load(ops_path,allow_pickle=True).item()
50 |                 mean_image = ops["meanImgE"]
51 |                 mean_images[experiment_path] = mean_image
52 |                 self.ls_of_exp_paths.append(experiment_path)
53 |             except FileNotFoundError:
54 |                 pass
55 |         n_exps = len(mean_images)
56 |         
57 |         for width in range(ceil(sqrt(n_exps)),1,-1):
58 |            if n_exps % width == 0: height = n_exps // width; break
59 |         else:
60 |             width = ceil(sqrt(n_exps))
61 |             height= width
62 |         
63 |         self.fig = plt.figure(figsize=(2*width+1, 2*height+1)) 
64 |         self.ax = np.empty((width,height),dtype=object)
65 |         gs = gridspec.GridSpec(width, height,
66 |                  wspace=0.0, hspace=0.0, 
67 |                  top=1.-0.5/(height+1), bottom=0.5/(height+1), 
68 |                  left=0.5/(width+1), right=1-0.5/(width+1)) 
69 |         
70 |         iterator = iter(mean_images.items())
71 |         im_idx = 0
72 |         for i in range(width):
73 |             for j in range(height):
74 |                 try:
75 |                     experiment, im = next(iterator)
76 |                 except StopIteration:
77 |                     break
78 |                 
79 |                 self.ax[i][j]= plt.subplot(gs[i,j])
80 |                 self.ax[i][j].imshow(im)
81 |                 self.ax[i][j].text(0.5, 0.5,str(im_idx),
82 |                                      horizontalalignment='center',
83 |                                      verticalalignment='center',
84 |                                      transform = self.ax[i][j].transAxes,
85 |                                      size = 16,
86 |                                      color = 'k')
87 |                 self.ax[i][j].set_xticklabels([])
88 |                 self.ax[i][j].set_yticklabels([])
89 |                 im_idx+=1
90 |         self.canvas = self.fig.canvas
91 |         self.show = self.fig.show
92 |         self.draw = self.fig.draw
93 | 
94 | 
95 | if __name__=="__main__":
96 |     plt.ioff()
97 |     testing_ls = [
98 |                 'D://Local_Repository\\CFEB037\\2017-01-27_01_CFEB037',
99 |                 'D://Local_Repository\\CFEB037\\2017-01-28_01_CFEB037',
100|                 'D://Local_Repository\\CFEB037\\2017-01-30_01_CFEB037',
101|                 'D://Local_Repository\\CFEB037\\2017-02-08_02_CFEB037']
102|     fig = AnimalMeanVideoFigure(ls_of_exp_paths = testing_ls)
103|     fig.show()


\DataVisualisation\pupil_figure_std_error.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Aug 11 21:39:51 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 


\DataVisualisation\pupil_mixed_model_output_visualisation.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Aug 14 18:43:42 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | 
8  | import pandas as pd
9  | import seaborn as sb
10 | import matplotlib.pyplot as plt
11 | import numpy as np
12 | 
13 | 
14 | 
15 | class PupilModelPredictionFigure:
16 |     def __init__(self, 
17 |                  path = "C:/Users/viviani/Desktop/pupilfactorpredictions.csv"):
18 |         sb.set_style("dark")
19 |         
20 |         df = pd.read_csv(path)
21 |         self.fig,ax = plt.subplots(ncols=2,nrows=2)
22 |         #hits
23 |         ax[0][0].set_title("Hits")
24 |         self.plot_with_confidence_interval(ax[0][0], 
25 |                                       df[df.trial_type=="hit"], 
26 |                                       color="green",
27 |                                       mean_color = "k")
28 |         ax[0][0].legend()
29 |         ax[0][0].set_ylabel("Pupil Diameter (pixels)")
30 |         
31 |         #misses
32 |         ax[0][1].set_title("Misses")
33 |         self.plot_with_confidence_interval(ax[0][1], 
34 |                                       df[df.trial_type=="miss"], 
35 |                                       color="darksalmon",
36 |                                       mean_color = "k")
37 |         
38 |         #false alarms
39 |         ax[1][0].set_title("False Alarms")
40 |         self.plot_with_confidence_interval(ax[1][0], 
41 |                                       df[df.trial_type=="fa"], 
42 |                                       color="palegreen",
43 |                                       mean_color = "k")
44 |         ax[1][0].set_ylabel("Pupil Diameter (pixels)")
45 |         ax[1][0].set_xlabel("Time since trial onset (s)")
46 |         #correct rejections
47 |         ax[1][1].set_title("Correct Rejections")
48 |         self.plot_with_confidence_interval(ax[1][1], 
49 |                                       df[df.trial_type=="cr"], 
50 |                                       color="red",
51 |                                       mean_color = "k")
52 |         ax[1][1].set_xlabel("Time since trial onset (s)")
53 |     
54 |     def show(self):
55 |         self.fig.show()
56 |     
57 |     @staticmethod
58 |     def plot_with_confidence_interval(axis,dataframe, color, mean_color=None):
59 |         if mean_color==None:
60 |             mean_color=color
61 |         x = np.arange(0,25)/5
62 |         axis.plot(x,dataframe.fit[:25],
63 |                   color = mean_color,
64 |                   label = "prediction")
65 |         axis.fill_between(x,dataframe.upr[:25],dataframe.lwr[:25],
66 |                           color = color,
67 |                           alpha = 0.3,
68 |                           label = "80% confidence interval")
69 |         axis.set_ylim((12,18))
70 | 
71 | 
72 | class PupilModelPredictionFigureWithSingleAxis(PupilModelPredictionFigure):
73 |     def __init__(self, path):
74 |         df = pd.read_csv(path)
75 |         self.fig,ax = plt.subplots()
76 |         self.plot_with_confidence_interval(ax, 
77 |                                       df[df.trial_type=="hit"], 
78 |                                       color="green")
79 |         self.plot_with_confidence_interval(ax, 
80 |                                       df[df.trial_type=="miss"], 
81 |                                       color="darksalmon")
82 |         self.plot_with_confidence_interval(ax, 
83 |                                       df[df.trial_type=="fa"], 
84 |                                       color="palegreen")
85 |         self.plot_with_confidence_interval(ax, 
86 |                                       df[df.trial_type=="cr"], 
87 |                                       color="red")
88 |         ax.set_ylabel("Pupil Diameter (pixels)")
89 |         ax.set_xlabel("Time since trial onset (s)")
90 | 
91 | 


\DataVisualisation\pupil_size_vs_licking.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Aug 11 11:36:41 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | 
8  | 
9  | import numpy as np
10 | import seaborn
11 | import matplotlib.pyplot as plt
12 | from matplotlib.patches import Rectangle
13 | 
14 | 
15 | from accdatatools.Timing.synchronisation import (get_lick_times,
16 |                                                  get_eyecam_frame_times)
17 | from accdatatools.ProcessPupil.size import get_pupil_size_at_each_eyecam_frame
18 | from accdatatool.Observations.trials import (SparseTrial,
19 |                                              _get_trial_structs)
20 | from accdatatools.Utils.path import (get_timeline_path, 
21 |                                      get_psychstim_path,
22 |                                      get_pupil_hdf_path)
23 | 
24 | 
25 | class PupilDiameterLickingFigure:
26 |     @staticmethod
27 |     def color_of_trial(trial):
28 |         if trial.isgo:
29 |             if trial.correct: return "green"
30 |             else:             return "palegreen"
31 |         else:
32 |             if trial.correct: return "red"
33 |             else:             return "darksalmon"
34 | 
35 |     def __init__(self,exp_path):
36 |         timeline_path  = get_timeline_path(exp_path)
37 |         psychstim_path = get_psychstim_path(exp_path)
38 |         hdf_path       = get_pupil_hdf_path(exp_path)
39 |         structs        = _get_trial_structs(psychstim_path)
40 |         self.trials             = [SparseTrial(struct) for struct in structs]
41 |         self.eyecam_frame_times = get_eyecam_frame_times(timeline_path)
42 |         self.licking_times      = get_lick_times(timeline_path)
43 |         self.pupil_diameters    = get_pupil_size_at_each_eyecam_frame(hdf_path)
44 |         self.render()
45 |         
46 |     def render(self):
47 |         seaborn.set_style("dark")
48 |         self.fig, ax  = plt.subplots()
49 |         y_val_for_licks = np.full(self.licking_times.shape, 6)
50 |         ax.scatter(self.licking_times,
51 |                    y_val_for_licks,
52 |                    c = "black")
53 |         ax.plot(self.eyecam_frame_times,
54 |                 self.pupil_diameters)
55 |         trial_starts = np.array([trial.start_stimulus for trial in self.trials])-1
56 |         trial_ends  = trial_starts + 5
57 |         colors       = [self.color_of_trial(trial) for trial in self.trials]
58 |         ymax,ymin = ax.get_ylim()
59 |         for trial_start,trial_end,c in zip(trial_starts,trial_ends,colors):
60 |             rect = Rectangle((trial_start,ymin),
61 |                              trial_end - trial_start,
62 |                              ymax - ymin,
63 |                              color = c,
64 |                              fill = True,
65 |                              alpha = 0.2)
66 |             ax.add_patch(rect)
67 |     def show(self):
68 |         self.fig.show()
69 |          
70 |         
71 |         
72 |         


\DataVisualisation\synch_sanity_check_figure.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Jul 17 11:45:31 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | import os
8  | 
9  | import matplotlib.pyplot as plt
10 | import numpy as np
11 | import cv2
12 | 
13 | from accdatatools.Timing.synchronisation import (get_lick_state_by_frame,
14 |                                                 get_eyecam_frame_times,
15 |                                                 get_nearest_frame_to_each_timepoint,
16 |                                                 get_neural_frame_times)
17 | from accdatatools.ProcessPupil.size import get_pupil_size_at_each_eyecam_frame
18 | from accdatatools.Observations.trials import get_trials_in_recording
19 | 
20 | 
21 | def video(path, nframes = -1):
22 |     '''
23 |     A generator that runs through a video and yields each frame as a 
24 |     (width,height,3) uint8 numpy array. Use it like this:
25 |         >>>for frame in video('example.mp4'):
26 |         >>>    do_something_to(frame)
27 |     '''
28 |     cap = cv2.VideoCapture(path)
29 |     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
30 |     frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
31 |     frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
32 |     buffer = np.empty((frame_height, frame_width, 3), np.dtype('uint8'))
33 |     i = 0
34 |     previous_frame_capture_was_successful = True
35 |     if nframes > 0:
36 |         #If you only want the first n frames
37 |         frame_count = nframes
38 | 
39 |     while (i < frame_count  and previous_frame_capture_was_successful):
40 |         previous_frame_capture_was_successful,buffer = cap.read()
41 |         i+=1
42 |         yield buffer
43 |     cap.release()
44 |     cv2.waitKey(0)
45 |     return
46 | 
47 | class EyeVideoSyncFigure:
48 |     def __init__(self, eyecam_video_file, behav_video_file, matlab_timeline_file, dlc_h5_file, 
49 |                  exp_path):
50 |         self.eyecam_video_file = eyecam_video_file
51 |         self.behav_video_file = behav_video_file
52 |         self.fig,axes = plt.subplots(nrows = 2, ncols = 3)
53 |         
54 |         ((self.behavvid_axis,self.eyevid_axis,self.pupilsize_axis),
55 |          (self.trial_axis,self.lick_axis,self.neuropil_axis)) = axes
56 |         
57 |         self.all_axes = axes.flatten()
58 |         
59 |         self.frame_times = get_eyecam_frame_times(matlab_timeline_file, dlc_h5_file)
60 |         self.pupil_sizes = get_pupil_size_at_each_eyecam_frame(dlc_h5_file)
61 |         self.frame_times = self.frame_times[:self.pupil_sizes.shape[0]]
62 |         self.licks = get_lick_state_by_frame(matlab_timeline_file, self.frame_times)
63 | 
64 |         self.trials, self.recording = get_trials_in_recording(exp_path,
65 |                                               ignore_dprime=True,
66 |                                               return_se = True)
67 |         
68 |         self.neuropil_traces = self.get_neuropil_for_each_eyecam_frame(
69 |             matlab_timeline_file)
70 |         
71 |         nframes = self.recording.ops['nframes']
72 |         neural_frame_times = get_neural_frame_times(matlab_timeline_file,nframes)
73 |         self.neural_frame_times = neural_frame_times
74 |     
75 |         self.trial_state = self.get_trial_state_for_each_eyecam_frame()
76 |         
77 |         # assert self.frame_times.shape == self.licks.shape
78 |         # assert self.frame_times.shape == self.pupil_sizes.shape
79 |         # assert self.frame_times.shape[0] == len(self.trial_state)
80 |         # assert self.frame_times.shape[0] == self.neuropil_traces.shape[0]
81 |         
82 |     def get_trial_state_for_each_eyecam_frame(self):
83 |         '''
84 |         Array of Trial objects or None indexed by eyecam frame.
85 |         Trial object if the frame occurs in the start_stimulus -> end response
86 |         period, else None.
87 | 
88 |         Returns
89 |         -------
90 |         result : array of object
91 |         '''
92 |         result = []
93 |         for time in self.times_of_closest_neural:
94 |             for trial in self.trials:
95 |                 if trial.is_occuring(time):
96 |                     result.append(trial)
97 |                     break
98 |             else:
99 |                 result.append(None)
100|         return result
101|     
102|     def get_neuropil_for_each_eyecam_frame(self, matlab_timeline_file):
103|         '''
104|         Get the neuropil brightness at the time each eyecam frame was captured.
105| 
106|         Parameters
107|         ----------
108|         matlab_timeline_file : TYPE
109|             DESCRIPTION.
110| 
111|         Returns
112|         -------
113|         neuropil_series : array of float, shape (timepoints,K,K)
114|             Array of neuropil traces, reshaped into KxK squares for 
115|             displaying with imshow.
116| 
117|         '''
118|         #Each value of this array is a neural frame's index,
119|         #each index of this list is an eyecam frame's index,
120|         #such that the series are closest to aligned!
121|         nframes = self.recording.ops['nframes']
122|         neural_frame_times = get_neural_frame_times(matlab_timeline_file,nframes)
123|         closest_neural_to_each_eyecam = get_nearest_frame_to_each_timepoint(
124|             neural_frame_times,
125|             self.frame_times)
126|         self.times_of_closest_neural = neural_frame_times[closest_neural_to_each_eyecam]
127|         #So now we can get the neuropil df/f at the time each eyecam frame
128|         #was captured
129|         neuropil_series = self.recording.Fneu[:,closest_neural_to_each_eyecam]
130|         #But I want to put this in an imshow format...so time for some hacky
131|         #garbage
132|         rois, timepoints  = neuropil_series.shape
133|         #find the largest lesser square number and its root
134|         root_rois_to_plot = np.floor(rois**0.5)
135|         root_rois_to_plot = root_rois_to_plot.astype(int)
136|         rois_to_plot = root_rois_to_plot**2
137|         #lop off extraneous neuropil regions
138|         neuropil_series = neuropil_series[:rois_to_plot,:]
139|         neuropil_series = neuropil_series.reshape(root_rois_to_plot,
140|                                                   root_rois_to_plot,
141|                                                   -1)
142|         neuropil_series = neuropil_series.transpose(2,0,1)
143|         return neuropil_series
144|         
145|         
146|         
147|             
148|     
149|     def render_frames(self):
150|         '''
151|         Renders a figure for each frame in the eyecam video file.
152|         Usage is as a generator, eg:
153|             >>>for figure in self.render_frames():
154|             >>>    do_stuff_with(figure)
155| 
156|         Yields
157|         ------
158|         matplotlib figure
159| 
160|         '''
161|         for idx,(eye_vid_frame, behav_frame) in enumerate(zip(video(self.eyecam_video_file),video(self.behav_video_file))):
162| 
163|             is_licking = self.licks[idx]
164|             pupil_size = self.pupil_sizes[idx]
165|             trial = self.trial_state[idx]
166|             neuropil = self.neuropil_traces[idx]
167|             
168|             if idx==0:
169|                 eyeframe = self.eyevid_axis.imshow(eye_vid_frame)
170|                 self.eyevid_axis.set_title("Eye Camera")
171|                 
172|                 xmin, xmax = self.lick_axis.get_xlim()
173|                 ymin, ymax = self.lick_axis.get_ylim()
174|                 self.lick_axis.set_title("Licking state")
175|                 licktext = self.lick_axis.text(xmin+0.05,
176|                                     (ymax-ymin)/2, 
177|                                     "Lick" if is_licking else "",
178|                                     color = "black",
179|                                     fontsize = '24',
180|                                     va = 'center')
181|                 
182|                 self.pupilsize_axis.set_title("Pupil size (not to scale)")
183|                 self.pupilsize_axis.set_xlim([-100,100])
184|                 self.pupilsize_axis.set_ylim([-100,100])
185|                 self.pupilsize_axis.set_aspect('equal', adjustable = 'box')
186|                 if pupil_size != np.nan:
187|                     radius = (pupil_size/np.pi)**0.5
188|                     pupil_image = plt.Circle((0,0),radius,
189|                                              color = "black")
190|                     self.pupilsize_axis.add_artist(pupil_image)
191|                 else:
192|                     pupil_image = plt.Circle((0,0),0,
193|                                              color = "black")
194|                     self.pupilsize_axis.add_artist(pupil_image)
195|                     
196|                 
197|                 
198|                 self.trial_axis.set_title("Trial state")
199|                 xmin, xmax = self.lick_axis.get_xlim()
200|                 ymin, ymax = self.lick_axis.get_ylim()
201|                 if trial != None:
202|                     trial_text = self.trial_axis.text(xmin+0.05,
203|                                          ymin + 0.1,
204|                                          ("Left\n" if trial.isleft else "Right\n") + 
205|                                           ("Go\n" if trial.isgo else "No-Go\n") + 
206|                                           "Trial",
207|                                           color = 'green' if trial.isgo else "red",
208|                                           fontsize = '24')
209|                 else:
210|                     trial_text = self.trial_axis.text(xmin+0.05,
211|                                                       ymin + 0.1,
212|                                                       "",
213|                                                       color = "black",
214|                                                       fontsize = '24')
215|                 
216|                 self.behavvid_axis.set_title("Body Camera")
217|                 bodyframe = self.behavvid_axis.imshow(behav_frame)
218|                 
219|                 self.neuropil_axis.set_title("Neuropil brightness")
220|                 vmin = np.percentile(myfigure.neuropil_traces,1)
221|                 vmax = np.percentile(myfigure.neuropil_traces,99)
222|                 neuropil_image = self.neuropil_axis.imshow(neuropil,
223|                                                            vmin = vmin,
224|                                                            vmax = vmax)
225|                 
226|                 for axis in self.all_axes:
227|                     axis.set_xticks([])
228|                     axis.set_yticks([])
229|             else:
230|                 eyeframe.set_data(eye_vid_frame)
231|                 self.eyevid_axis.draw_artist(eyeframe)
232|                 
233|                 bodyframe.set_data(behav_frame)
234|                 self.behavvid_axis.draw_artist(bodyframe)
235| 
236|                 pupil_image.set_radius((pupil_size/np.pi)**0.5)
237|                 self.pupilsize_axis.draw_artist(pupil_image)
238|                 
239|                 licktext.set_text("Lick! :D" if is_licking else "")
240|                 licktext.set_color("green" if is_licking else "red")
241|                 self.lick_axis.draw_artist(licktext)
242|                 if trial != None:
243|                     trial_text.set_text(("Left\n" if trial.isleft else "Right\n") + 
244|                                               ("Go\n" if trial.isgo else "No-Go\n") + 
245|                                               "Trial")
246|                     trial_text.set_color('green' if trial.isgo else "red")
247|                 else:
248|                     trial_text.set_text("")
249|                 self.trial_axis.draw_artist(trial_text)
250|                 
251|                 neuropil_image.set_data(neuropil)
252|                 self.neuropil_axis.draw_artist(neuropil_image)
253|                 
254|                 self.fig.canvas.update()
255|                 self.fig.canvas.flush_events()
256|             yield self.fig
257|         plt.close('all')
258|         return
259|         
260|         
261|         
262|     def dump_all_frames_to_dir(self,dirpath):
263|         '''
264|         Create a figure for each frame in the eyecam video and then dump
265|         all of them into dirpath as png files.
266| 
267|         Parameters
268|         ----------
269|         dirpath : str
270|             path to target directory. If this doesn't exist it will be created.
271| 
272|         Returns
273|         -------
274|         None.
275| 
276|         '''
277|         try:
278|             os.mkdir(dirpath)
279|         except FileExistsError:
280|             pass
281|         for idx,frame in enumerate(self.render_frames()):
282|             target_path = os.path.join(dirpath,f"{idx}.png")
283|             frame.savefig(target_path)
284|     
285|     def test_render(self):
286|         '''Render a figure from the first frame in a window'''
287|         for figure in self.render_frames():
288|             figure.show()
289|             break
290| 
291| def video_from_dir_of_frames(dirpath):
292|         os.system(
293|           "C:\\Users\\viviani\\ffmpeg\\bin\\.\\ffmpeg.exe -r 20 -f image2 -i"+
294|           f" {dirpath}\\%d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p"+
295|           f" {dirpath}.gif")
296|         
297| if __name__=="__main__":
298|     eyecam_video_file = ("C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/videos/"+
299|                          "2016-10-07_03_CFEB027_eye.mp4")
300|     behav_video_file  = "H:/Local_Repository/CFEB027/2016-10-07_03_CFEB027/2016-10-07_03_CFEB027_behav.mp4"
301|     timeline_file     = "H:/Local_Repository/CFEB027/2016-10-07_03_CFEB027/2016-10-07_03_CFEB027_Timeline.mat"
302|     exp_path          = "H:/Local_Repository/CFEB027/2016-10-07_03_CFEB027"
303|     h5_file           = ("C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/videos/"+
304|                 "2016-10-07_03_CFEB027_eyeDLC_resnet50_micepupilsJul9shuffle1_1030000.h5")
305|     myfigure = EyeVideoSyncFigure(eyecam_video_file, behav_video_file, timeline_file, h5_file, exp_path)
306|     myfigure.dump_all_frames_to_dir("C:/Users/viviani/Desktop/framedump")
307|     video_from_dir_of_frames("C:/Users/viviani/Desktop/framedump")


\DataVisualisation\whole_experiment_figure.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Jun 23 13:15:20 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import os
7  | 
8  | from rastermap.mapping import Rastermap
9  | import matplotlib.pyplot as plt
10 | from matplotlib.patches import Rectangle
11 | import numpy as np
12 | import seaborn
13 | 
14 | 
15 | from accdatatools.Observations.recordings import Recording
16 | from accdatatools.Observations.trials import get_trials_in_recording
17 | from accdatatools.Timing.synchronisation import get_neural_frame_times, get_lick_state_by_frame
18 | from accdatatools.Utils.map_across_dataset import apply_to_all_one_plane_recordings
19 | from accdatatools.Utils.convienience import item
20 | 
21 | 
22 | 
23 | 
24 | class ExperimentFigure:
25 |     def __init__(self,exp_path, merge = True):
26 |         '''
27 |         A nice heatmap figure for all
28 |         neurons' responses over the course of a trial.
29 |         
30 | 
31 |         Parameters
32 |         ----------
33 |         exp_path : string
34 |             The root path of an experiment.
35 | 
36 |         '''
37 |         seaborn.set_style("dark")
38 |         print(f"Loading data from {exp_path}")
39 |         self.trials, self.recording = get_trials_in_recording(exp_path, return_se=True)
40 |         print("Running rastermap on fluorescence data")
41 |         r = Rastermap()
42 | 
43 |         #Sort by rastermap embedding
44 |         print("Sorting traces by rastermap ordering")
45 |         self.dF_on_F = self.recording.dF_on_F
46 |         self.dF_on_F[np.isnan(self.dF_on_F)] = 1
47 |         r.fit(self.dF_on_F)
48 |         self.dF_on_F = self.dF_on_F[r.isort]
49 | 
50 |         #Show overall plot
51 |         timeline_path  = os.path.join(exp_path,
52 |                                   item(
53 |                                       [s for s in os.listdir(exp_path) if 'Timeline.mat' in s]))
54 |         
55 |         print("Fetching Frame Times...")
56 |         frame_times = get_neural_frame_times(timeline_path, self.recording.ops["nframes"])
57 |         print("Fetching licking information...")
58 |         self.licks = get_lick_state_by_frame(timeline_path, frame_times)
59 |         print("Aligning frames with trials...")
60 |         self.start_idxs, self.end_idxs = self.get_trial_attributes(frame_times)
61 |         print("...done")
62 | 
63 |     def get_trial_attributes(self, frame_times):
64 |         start_times = np.array([trial.start_stimulus for trial in self.trials]) - 1
65 |         end_times   = start_times + 5
66 |         start_idxs  = frame_times.searchsorted(start_times)
67 |         end_idxs    = frame_times.searchsorted(end_times)
68 |         if np.any(end_idxs>self.recording.dF_on_F.shape[-1]):
69 |             raise ValueError("Trial not contained in recording")
70 |         return (start_idxs,end_idxs)
71 |     
72 |     def show(self,start=0,end=-1):
73 |         fig, ax = plt.subplots()
74 |         if end==-1: end = self.dF_on_F.shape[1]-1
75 |         max_brightness = np.percentile(self.dF_on_F[:,start:end],99)
76 |         artist = ax.imshow(np.clip(self.dF_on_F[:,start:end],-0.2,max_brightness), 
77 |                            origin = 'lower')
78 |         fig.colorbar(artist, orientation = 'vertical')
79 |         ax.set_xlabel("Time (Frames)")
80 |         ax.set_ylabel("ROIs (PCA-sorted)")
81 |         for trial, trial_start, trial_end in zip(self.trials, self.start_idxs,self.end_idxs):
82 |             if trial_start>start and trial_end<end:
83 |                 rect = Rectangle((trial_start,-10 if trial.isleft else -5),
84 |                                  5*5,5,
85 |                                  color = "green" if trial.isgo else "red",
86 |                                  fill = True if trial.correct else False)
87 |                 ax.add_patch(rect)
88 |         lick_frame = np.nonzero(self.licks)
89 |         ax.vlines(lick_frame, -15,-10)
90 |         ax.set_xlim(0,350)
91 |         fig.show()
92 |         return fig
93 | 
94 | class PupilExperimentFigure(ExperimentFigure):
95 |     #TODO
96 |     pass
97 | 
98 | class NeuropilExperimentFigure(ExperimentFigure):
99 |     '''
100|     A heatmap of all the neuropil regions over an experiment.
101|     
102|     Parameters
103|     ----------
104|     exp_path : string
105|         The root path of an experiment.
106|     '''
107|     def __init__(self,exp_path):
108|         seaborn.set_style("dark")
109|         print(f"Loading data from {exp_path}")
110|         self.trials, self.recording = get_trials_in_recording(exp_path, return_se=True,
111|                                                               ignore_dprime=True)
112|         print("Running rastermap on fluorescence data")
113|         r = Rastermap()
114|         r.fit(self.recording.Fneu)
115|         #Sort by rastermap embedding
116|         print("Sorting traces by rastermap ordering")
117|         self.dF_on_F = self.recording.Fneu[r.isort]
118|         timeline_path  = os.path.join(exp_path,
119|                                   item(
120|                                       [s for s in os.listdir(exp_path) if 'Timeline.mat' in s]))
121|         
122|         print("Fetching Frame Times...")
123|         frame_times = get_neural_frame_times(timeline_path, self.recording.ops["nframes"])
124|         print("Fetching licking information...")
125|         self.licks = get_lick_state_by_frame(timeline_path, frame_times)
126|         print("Aligning frames with trials...")
127|         self.start_idxs, self.end_idxs = self.get_trial_attributes(frame_times)
128|         print("...done")
129| 
130| class RawExperimentFigure(ExperimentFigure):
131|     '''
132|     A heatmap of all the raw fluorescence traces over an experiment.
133|     
134|     Parameters
135|     ----------
136|     exp_path : string
137|         The root path of an experiment.
138|     '''
139|     def __init__(self,exp_path):
140|         seaborn.set_style("dark")
141|         print(f"Loading data from {exp_path}")
142|         self.trials, self.recording = get_trials_in_recording(exp_path, return_se=True,
143|                                                               ignore_dprime=True)
144|         print("Running rastermap on fluorescence data")
145|         r = Rastermap()
146|         r.fit(self.recording.F)
147|         #Sort by rastermap embedding
148|         print("Sorting traces by rastermap ordering")
149|         self.dF_on_F = self.recording.F[r.isort]
150|         timeline_path  = os.path.join(exp_path,
151|                                   item(
152|                                       [s for s in os.listdir(exp_path) if 'Timeline.mat' in s]))
153|         
154|         print("Fetching Frame Times...")
155|         frame_times = get_neural_frame_times(timeline_path, self.recording.ops["nframes"])
156|         print("Fetching licking information...")
157|         self.licks = get_lick_state_by_frame(timeline_path, frame_times)
158|         print("Aligning frames with trials...")
159|         self.start_idxs, self.end_idxs = self.get_trial_attributes(frame_times)
160|         print("...done")
161| 
162| 
163| if __name__=="__main__":
164|     fig = RawExperimentFigure(
165|         "D:/Local_Repository/CFEB027/2016-10-07_03_CFEB027")
166|     fig.show()


\DataVisualisation\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\DeepLabCutPipeline\cmdline_utils.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Jul 16 17:19:36 2020
3  | 
4  | @author: viviani
5  | 
6  | This is a script designed to be run with:
7  |     $python -i dlc_cmdline.py
8  |     
9  | to make it pleasant to perform deeplabcut operations from the
10 | command line!
11 | """
12 | 
13 | import os
14 | import numpy as np
15 | import deeplabcut as dlc
16 | 
17 | 
18 | 
19 | root = "C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/"
20 | config_path = root + "config.yaml"
21 | vid_dir = "C:/Users/viviani/Desktop/allvideos/"
22 | vids = [vid_dir + file for file in os.listdir(vid_dir) if ".mp4" in file]


\DeepLabCutPipeline\create_testing_dataset.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sat Jul 25 10:48:23 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | import deeplabcut as dlc
8  | 
9  | from accdatatools.DeepLabCutPipeline.cmdline_utils import config_path as machine_learning_config_path
10 | 
11 | #Deeplabcut doesn't have inbuilt tools for making seperate testing
12 | #datasets...so we're going to have to hack something together.
13 | #The authors have provided some instruction for how to make this happen
14 | #see here: https://forum.image.sc/t/how-to-move-partial-labeled-frame-to-a-new-project-how-to-quickly-evaluate-a-trained-dataset-on-analyzing-new-frames-without-retraining/29793
15 | #and here: https://forum.image.sc/t/is-there-a-way-to-evaluate-a-deeplabcut-network-on-data-not-present-in-the-test-training-set/32222
16 | 
17 | 
18 | 
19 | video_list = ["C:/Users/viviani/Desktop/2017-02-08_01_CFEB040_eye.mp4"]
20 | testing_config = "C:\\Users\\viviani\\Desktop\\testing_dataset_factory-viviani-2020-07-27\\config.yaml"
21 | 
22 | 
23 | 
24 | if __name__=="__main__":
25 |     # dlc.create_new_project("testing_dataset_factory",
26 |     #                         "viviani",
27 |     #                         videos = video_list,
28 |     #                         working_directory = "C:/Users/viviani/Desktop",
29 |     #                         copy_videos=True)
30 |     # dlc.extract_frames(testing_config, mode='automatic', algo='kmeans', crop=False)
31 |     dlc.label_frames(testing_config)
32 |     dlc.analyze_time_lapse_frames(machine_learning_config_path,
33 |     "C:/Users/viviani/Desktop/testing_dataset_factory-viviani-2020-07-27/labeled-data/2017-02-08_01_CFEB040_eye",
34 |     save_as_csv=True)


\DeepLabCutPipeline\instantiate_model.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun Jul  5 14:31:53 2020
3  | 
4  | @author: Vivian Imbriotis
5  | 
6  | Create a new DLC project.
7  | """
8  | 
9  | 
10 | from .Utils.map_across_dataset import apply_to_all_one_plane_recordings
11 | import deeplabcut as dlc
12 | import os
13 | 
14 | def get_video(exp_path):
15 |     for file in os.listdir(exp_path):
16 |         if "eye.mp4" in file:
17 |             return os.path.join(exp_path,file)
18 |     raise ValueError(f"No eye.mp4 found in {exp_path}")
19 | 
20 | def get_ls_of_all_videos(drive):
21 |     ls = []
22 |     add2ls = lambda path:ls.append(get_video(path))
23 |     apply_to_all_one_plane_recordings(drive, add2ls)
24 |     return ls
25 | 
26 | 
27 | if __name__=="__main__":
28 |     project_name = "mousepupils"
29 |     creator      = "viviani"
30 |     vids         = get_ls_of_all_videos("H:\\")
31 |     dlc.create_new_project(project_name,
32 |                            creator,
33 |                            vids,
34 |                            copy_vids = True)
35 |     root = "C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/"
36 |     config_path = root + "config.yaml"
37 |     vid_dir = root + "videos/"
38 |     vids = [vid_dir + file for file in os.listdir(vid_dir) if ".mp4" in file]
39 |     with open(config_path,"w") as file:
40 |         with open("config.yaml","r") as source:
41 |             for line in source:
42 |                 file.write(line)
43 |     dlc.label_frames(config_path,vids)
44 |     #Once frames are labelled, call dlc.create_training_dataset
45 |     #and then dlc.train_network


\DeepLabCutPipeline\validate_model_with_testing_dataset.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jul 27 17:09:59 2020
3  | 
4  | @author: viviani
5  | """
6  | import os
7  | 
8  | import pandas as pd
9  | from deeplabcut.pose_estimation_tensorflow.evaluate import pairwisedistances
10 | 
11 | 
12 | 
13 | #Deeplabcut doesn't have inbuilt tools for making seperate testing
14 | #datasets...so we're going to have to hack something together.
15 | #The authors have provided some instruction for how to make this happen
16 | #see here: https://forum.image.sc/t/how-to-move-partial-labeled-frame-to-a-new-project-how-to-quickly-evaluate-a-trained-dataset-on-analyzing-new-frames-without-retraining/29793
17 | #and here: https://forum.image.sc/t/is-there-a-way-to-evaluate-a-deeplabcut-network-on-data-not-present-in-the-test-training-set/32222
18 | 
19 | def get_combined_data(training_data_file, machine_results_file):        
20 |     Data = pd.read_hdf(training_data_file,
21 |             "df_with_missing")
22 |     long_names = Data.index.values
23 |     short_names = [os.path.split(name)[1] for name in long_names]
24 |     rename_dict = {long:short for long,short in zip(long_names,short_names)}
25 |     Data = Data.rename(rename_dict)
26 |     DataMachine = pd.read_hdf(machine_results_file, "df_with_missing")
27 |     DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T
28 |     return DataCombined
29 | 
30 | def get_error(training_data_file,  machine_results_file, certainty_cutoff=0.3):
31 |     combined_data = get_combined_data(training_data_file, 
32 |                                       machine_results_file,
33 |                                       )
34 |     return pairwisedistances(combined_data,
35 |                              scorer1 = 'viviani',
36 |                              scorer2 = "DLC_resnet50_micepupilsJul9shuffle1_1030000",
37 |                              pcutoff = certainty_cutoff)[1]
38 | 
39 | if __name__=="__main__":
40 |     os.chdir("C:/Users/viviani/Desktop/testing_dataset_factory-viviani-2020-07-27/labeled-data/2017-02-08_01_CFEB040_eye")
41 |     error = get_error(
42 |         "CollectedData_viviani.h5",
43 |         "2017-02-08_01_CFEB040_eyeDLC_resnet50_micepupilsJul9shuffle1_1030000.h5"
44 |         )
45 |     print(f"Root mean squared error was {(error**2).mean().mean()**0.5}")


\DeepLabCutPipeline\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\Observations\recordings.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon May 11 19:53:44 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | 
8  | import os
9  | import seaborn
10 | 
11 | import numpy as np
12 | import matplotlib.pyplot as plt
13 | 
14 | from accdatatools.Utils.path import get_timeline_path, get_exp_id
15 | from accdatatools.Timing.synchronisation import get_lick_times
16 | from accdatatools.ProcessFluorescence.data_cleaning import merge_rois
17 | 
18 | import accdatatools.ProcessFluorescence.df_on_f_calculations as df
19 | 
20 | 
21 | class Recording:
22 |     def __init__(self, path):
23 |         '''
24 |         
25 | 
26 |         Parameters
27 |         ----------
28 |         path : str
29 |             The path to the experiment folder (which must already have been 
30 |             processed by suite2p).
31 | 
32 |         '''
33 |         self.exp_path = path
34 |         suite2p_path = os.path.join(path,"suite2p","plane0")
35 |         self.__path = suite2p_path
36 |         cwd = os.getcwd()
37 |         try:
38 |             os.chdir(suite2p_path)
39 |             
40 |             #load in all the suite2p stuff
41 |             self.stat = np.load("stat.npy", allow_pickle = True)
42 |             self.ops  = np.load("ops.npy", allow_pickle = True).item()
43 |             self.F    = np.load("F.npy")
44 |             #drop ROIs that are everywhere zero (why do these exist?)
45 |             valid_idxs = np.count_nonzero(self.F,axis=-1)!=0
46 |             self.F = self.F[valid_idxs]
47 |             # #Sometimes small values are negative which messes with division by F0
48 |             # self.F    = np.abs(self.F)
49 |             self.Fneu = np.load("Fneu.npy")
50 |             self.Fneu = self.Fneu[valid_idxs]
51 |             # #same here
52 |             # self.Fneu = np.abs(self.Fneu)
53 |     
54 |             #Calculate deltaF/F
55 |             self.Fcorr = df.subtract_neuropil_trace(self.F, self.Fneu)
56 |             self.F0 = df.get_smoothed_running_minimum(self.Fcorr)
57 |             self.dF_on_F = df.get_df_on_f0(self.Fcorr,self.F0)
58 |             
59 |             #Get the logged form
60 |             self.logged_dF = df.log_transform(self.dF_on_F)
61 |             
62 |             #Get the deconvoluted spiking data and then binarise it
63 |             self.spks_unbinarized = np.load('spks.npy')
64 |             self.spks = (np.load('spks.npy') > 0)
65 |             self.spks = self.spks[valid_idxs]
66 |             self.iscell = np.load("iscell.npy")[:,0].astype(np.bool)
67 |             self.iscell = self.iscell[valid_idxs]
68 |             
69 |             #Merge together highly correlated ROIs
70 |             self.dF_on_F, self.spks,self.iscell = merge_rois(self.dF_on_F,
71 |                                                              self.spks,
72 |                                                              self.iscell)
73 |             
74 |             timeline_path = get_timeline_path(self.exp_path)
75 |             self.lick_times = get_lick_times(timeline_path)
76 |             
77 |             #Lastly we need to know in how many prior recordings this mouse
78 |             #has appeared. To do this we need to go up a level to
79 |             #the mouse's folder, then see which have dates that happened
80 |             #before this one:
81 |             mouse_dir,exp_id = os.path.split(self.exp_path)
82 |             folders = sorted(os.listdir(mouse_dir))
83 |             folders_preceding_this_one = folders[:folders.index(exp_id)]
84 |             self.trial_number = len(folders_preceding_this_one) + 1
85 |             
86 |         finally:
87 |             os.chdir(cwd)
88 |     
89 |     def gen_iscell(self):
90 |         F_Fneu_ratio = np.fromiter((np.sum(x[0])/np.sum(x[1]) for x in zip(self.F,self.Fneu)),
91 |                                         dtype = np.double)
92 |         skew = np.fromiter((x["skew"] for x in self.stat),
93 |                                 dtype = np.float32)
94 |         iscell_neuropil_criterion = F_Fneu_ratio > 1.05
95 |         iscell_skew_criterion = skew > np.percentile(self.skew,5)
96 |         
97 |         #Both criteria must be met for ROI to be included in analysis
98 |         iscell = np.logical_and(iscell_neuropil_criterion, 
99 |                                 iscell_skew_criterion)
100|         iscell = iscell.astype(np.int16)
101|         return np.stack((iscell,iscell)).transpose()
102|     
103|     def _overwrite_iscell(self):
104|         iscell = self.gen_iscell()
105|         np.save(
106|             os.path.join(self.__path,
107|                          "iscell.npy"),
108|             iscell)
109|     
110| 
111| 
112|     def plot_cell_pipeline(self,cell_id):
113|         seaborn.set()
114|         fig,ax = plt.subplots(nrows = 5, ncols = 1, tight_layout = True,
115|                               figsize = [6.4,8])
116|         ax[0].set_title("Raw F Trace")
117|         ax[0].plot(self.F[cell_id])
118|         ax[1].set_title("Raw Fneu Trace")
119|         ax[1].plot(self.Fneu[cell_id])
120|         ax[2].set_title("Fluor Trace corrected by Fneu")
121|         ax[2].plot(self.Fcorr[cell_id])
122|         ax[3].set_title("F0 with sliding window minimum")
123|         ax[3].plot(self.F0[cell_id])
124|         ax[4].set_title("dF/F0")
125|         ax[4].plot(self.dF_on_F[cell_id])
126|         return fig
127|     
128|     def plot_many_cells(self,condition = 'iscell'):
129|         seaborn.set()
130|         fig,ax = plt.subplots(nrows = 17, ncols = 1, constrained_layout=True,
131|                               figsize = [5,8.5])
132|         if condition in ("iscell","is cell","cell"):
133|             condition = self.iscell
134|             fig.suptitle("Random ROIs included under metric")
135|         elif condition in ("not cell", "notcell","ncell"):
136|             condition = np.logical_not(self.iscell)
137|             fig.suptitle("Random ROIs excluded under metric")
138|         series = (self.dF_on_F[condition])
139|         series = [series[x] for x in range(len(series))]
140|         for a in ax:
141|             idx = np.random.randint(len(series))
142|             val = series.pop(idx)
143|             a.plot(val)
144|             a.set_xticks([])
145|         return fig
146| 
147| 
148| if __name__=="__main__":
149|     from accdatatools.Utils.path import get_exp_path
150|     rec = Recording((r"H:\Local_Repository\CFEB026\2016-09-23_02_CFEB026"))


\Observations\roi_trials.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Jun  9 20:16:38 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import seaborn
7  | import pickle as pkl
8  | import pandas as pd
9  | import matplotlib.pyplot as plt
10 | from accdatatools.Observations.trials import dump_all_trials_in_dataset_to_pkl_file
11 | import numpy as np
12 | seaborn.set()
13 | 
14 | 
15 | class ROITrialResponse:
16 |     def __init__(self, trial, idx, roi_identifier, trial_identifier):
17 |         #Copy all the attributes from the parent trial
18 |         self.__dict__ = trial.__dict__.copy()
19 |         #But we only want one ROI's data, so discard the others'
20 |         self.dF_on_F = trial.dF_on_F[idx,:]
21 |         self.spks    = trial.spks[idx,:]
22 |         
23 |         #Then get nice string representations of the ROI and the recording
24 |         self.ROI_ID = self.recording.split("\\")[-1] + f" {roi_identifier}"
25 |         self.trial_id = self.recording.split("\\")[-1] + f" {trial_identifier}"
26 |     def to_dict(self):
27 |         return {
28 |             'ROI_ID':self.ROI_ID,
29 |             'Trial_ID':self.trial_id,
30 |             'go': self.isgo,
31 |             'side': 'left' if self.isleft else ('right' if self.isright else 'unknown'),
32 |             'correct': self.correct,
33 |             'affirmative': self.affirmative,
34 |             'rel_start_stim': self.rel_start_stim,
35 |             "rel_start_resp": self.rel_start_resp,
36 |             "rel_end_resp": self.rel_end_resp,
37 |             "trial_duration": self.duration,
38 |             "dF_on_F": self.dF_on_F,
39 |             "lick_rate": self.licks
40 |             }
41 |     def to_unrolled_records(self):
42 |         start_idx = int((self.rel_start_stim - 1)*30//6)
43 |         end_idx = start_idx + 30*(1+2+2)//6 - 1
44 |         #check that these idxs exist (since slice notation doesn't raise
45 |         #  any IndexErrors)
46 |         self.dF_on_F[start_idx]
47 |         self.dF_on_F[end_idx]
48 |         results = []
49 |         for time, (df, spk, lick) in enumerate(zip(
50 |                 self.dF_on_F[start_idx:end_idx],
51 |                 self.spks[start_idx:end_idx],
52 |                 self.licks[start_idx:end_idx])):
53 |             results.append(
54 |                 {
55 |                 'ROI_ID':self.ROI_ID,
56 |                 'Trial_ID':self.trial_id,
57 |                 'go': 'TRUE' if self.isgo else 'FALSE',
58 |                 'side_is_left': 'TRUE' if self.isleft else 'FALSE',
59 |                 'correct': 'TRUE' if self.correct else 'FALSE',
60 |                 'affirmative': 'TRUE' if self.affirmative else 'FALSE',
61 |                 "time": time/(30//6),
62 |                 "dF_on_F": df,
63 |                 "Neuron_Firing": 'TRUE' if spk>0 else 'FALSE',
64 |                 "lick_during_frame": 'TRUE' if (lick > 0) else 'FALSE'
65 |                 }
66 |                 )
67 |         return results
68 | 
69 | class ROIActivitySummary:
70 |     '''
71 |     A visual summary of an ROI's behaviour across trials.'
72 | 
73 |     Parameters
74 |     ----------
75 |     roi_trial_responses : Iterable of ROITrialResponse objects
76 |         The responses for which to generate a summary.
77 | 
78 |     '''
79 |     
80 |     def __init__(self, roi_trial_responses):
81 |         ls = roi_trial_responses #For brevity; doing list comprehensions
82 |         left_correct  = [t for t in ls if t.isleft and t.correct]
83 |         left_wrong    = [t for t in ls if t.isleft and not t.correct]
84 |         right_correct = [t for t in ls if t.isright and t.correct]
85 |         right_wrong   = [t for t in ls if t.isright and not t.correct]
86 |         datasets = (left_correct,left_wrong,right_correct,right_wrong)
87 |         titles = ["Left Correct", "Left Incorrect",
88 |                   "Right Correct", "Right Incorrect"]
89 |         self.fig, self.ax = plt.subplots(nrows = 2, ncols = 2, 
90 |                                          constrained_layout=True,
91 |                                          )
92 |         for axes, data, title in zip(self.ax.flatten(), datasets, titles):
93 |             for trial in data:
94 |                 start_idx = int((trial.rel_start_stim - 1)*30//6)
95 |                 end_idx = start_idx + 30*(1+2+2)//6 - 1
96 |                 trial.dF_on_F[start_idx]
97 |                 trial.dF_on_F[end_idx]
98 |                 trace = trial.dF_on_F[start_idx:end_idx]
99 |                 axes.plot(trace, color='black', alpha = 0.25)
100|             ylim = axes.get_ylim()
101|             stimulus = plt.Rectangle(
102|                 xy = (1*30//6,ylim[0]),
103|                 width = 2*30//6,
104|                 height = ylim[1] - ylim[0],
105|                 color = 'blue',
106|                 alpha = 0.5)
107|             axes.add_patch(stimulus)
108|             axes.set_title(title)
109|         self.fig.suptitle(f"Activity of {ls[0].ROI_ID}")
110|         self.show()
111|     def show(self):
112|         self.fig.show()
113| 
114| class ROIActivitySummaryFactory:
115|     def __init__(self, ls_of_rois):
116|         self.ls_of_rois = ls_of_rois
117|         self.roi_ids = [r.ROI_ID for r in self.ls_of_rois]
118|         self.roi_ids = list(set(self.roi_ids))
119|         self.roi_ids.sort()
120|     def plot(self,ID):
121|         if type(ID)==int:
122|             ID = self.roi_ids[ID]
123|         elif type(ID)==str:
124|             pass
125|         else:
126|             raise ValueError(f'ID must be string or int, not {type(ID)}')
127|         ROIActivitySummary([r for r in self.ls_of_rois if r.ROI_ID==ID])
128|     def __call__(self,N):
129|         self.plot(N)
130| 
131| def axon_responses_from_trial(trial, trial_id):
132|     res = []
133|     for ROI_idx, ROI_identifier in enumerate(trial.ROI_identifiers):
134|         res.append(ROITrialResponse(
135|             trial,ROI_idx,ROI_identifier, trial_id)
136|             )
137|     return res
138| 
139| def ls_of_ROIs_from_ls_of_Trials(ls, destructive = True):
140|     res = []
141|     for idx, trial in enumerate(ls):
142|         res.extend(axon_responses_from_trial(trial, idx))
143|     return res
144|     
145| def dataframe_from_ls_of_ROIs(ls):
146|     return pd.DataFrame.from_records(ROI.to_dict() for ROI in ls)
147| 
148| def flat_dataframe_from_ls_of_ROIs(ls):
149|     records = []
150|     for roi in ls:
151|         records.extend(roi.to_unrolled_records())
152|     return pd.DataFrame(records)
153|     
154| def load_all_trials(path):
155|     all_trials = []
156|     with open(path,'rb') as file:
157|         while True:
158|             try:
159|                  all_trials.append(
160|                      pkl.load(file)
161|                      )
162|             except EOFError:
163|                 break
164|     return all_trials
165| 
166| 
167| 
168| 
169| if __name__ == "__main__":
170|     from accdatatools.Observations.trials import get_trials_in_recording
171|     from accdatatools.Utils.path import get_exp_path
172|     trials = get_trials_in_recording(get_exp_path("2016-11-01_03_CFEB027","D:\\"))
173|     rois = ls_of_ROIs_from_ls_of_Trials(trials)
174|     del trials
175|     plotter = ROIActivitySummaryFactory(rois)


\Observations\trials.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun Jun  7 11:45:31 2020
3  | 
4  | @author: Vivian Imbriotis
5  | 
6  | Tools for Trial-based handling of the data.
7  | A "trial" is a single stimulus/response pair, where a mouse is presented
8  | with a stimulus, attempts to respond in a go/nogo manner, and is 
9  | rewarded appropriately.
10 | Includes a Trial class encapsulating a single trial event, a constructor
11 | that takes a recording's parent directory, a figure generation class,
12 | and a main function for dumping all trial objects constructable from the
13 | dataset to a binary file.'
14 | """
15 | 
16 | import numpy as np
17 | import pandas as pd
18 | import matplotlib.pyplot as plt
19 | #import seaborn
20 | import os
21 | import pickle as pkl
22 | 
23 | from accdatatools import GLOBALS
24 | 
25 | from accdatatools.Utils.map_across_dataset import apply_to_all_one_plane_recordings
26 | from accdatatools.Utils.deeploadmat import loadmat
27 | from accdatatools.Utils.convienience import item
28 | from accdatatools.DataCleaning.determine_dprime import calc_d_prime
29 | from accdatatools.Timing.synchronisation import get_neural_frame_times,get_lick_state_by_frame
30 | from accdatatools.Observations.recordings import Recording
31 | 
32 | class SparseTrial:
33 |     '''
34 |     A trial object without the informations from the various datastreams,
35 |     containing only the trial's attributes. Neglects information about the
36 |     dF/F of neurons, licking information, and pupil activity that occured
37 |     during the trial.
38 |     
39 |     Parameters
40 |     ----------
41 |     struct : scipy mat_struct object
42 |         The structs contained in each experiment's psychstim.mat file within
43 |         the stims array.
44 |     tolerant : bool, optional
45 |         Whether to accept structs missing some field. Avoid using this as much
46 |         as possible, since it might break other code, but sometimes you
47 |         need information about trials that are missing fields like correct
48 |         or StartTime. The default is False.
49 |     '''
50 |     def __init__(self,struct,tolerant = False):
51 |         self.complete       = True #The object is complete until a field is missing
52 |         self.stim_id        = struct.stimID
53 |         self.istest         = self.stim_id in {GLOBALS.TESTLEFT, GLOBALS.TESTRIGHT}
54 |         
55 |         #Get basic stimulus attributes
56 |         struct_attr_names = ('stimside','go','contrast')
57 |         self_attr_names = ('isright','isgo','contrast')
58 |         for self_attr, struct_attr in zip(self_attr_names,struct_attr_names):
59 |             if hasattr(struct.stimAttributes,struct_attr):
60 |                 setattr(self,self_attr,
61 |                         getattr(struct.stimAttributes,struct_attr))
62 |             elif tolerant:
63 |                 setattr(self,self_attr,None)
64 |                 self.complete = False
65 |             else:
66 |                 raise ValueError(
67 |                     f"Intolerant Trial failed to find {struct_attr} in matstruct"
68 |                     )
69 |         self.isleft = not self.isright if self.isright!=None else None
70 |         try:
71 |             self.correct        = struct.correct
72 |         except AttributeError as e:
73 |             if tolerant:
74 |                 self.correct = None
75 |                 self.complete = False
76 |             else:
77 |                 raise e
78 |         self.affirmative    = ((self.correct and self.isgo) or 
79 |                                (not self.correct and not self.isgo))
80 |         
81 |         
82 |         #Get absolute timing information
83 |         struct_attr_names = ('StartTrial','StimulusStart','ResponseStart','ResponseEnd','EndClearDelay')
84 |         self_attr_names = ('start_trial','start_stimulus','start_response','end_response','end_trial')
85 |         for self_attr, struct_attr in zip(self_attr_names,struct_attr_names):
86 |             if hasattr(struct.timing,struct_attr):
87 |                 setattr(self,self_attr,getattr(struct.timing,
88 |                                                struct_attr))
89 |             elif tolerant:
90 |                 setattr(self,self_attr,None)
91 |                 self.complete = False
92 |             else:
93 |                 raise ValueError(f"Intolerant Trial failed to find {struct_attr} in matlab struct")
94 |             
95 |         #Get relative timing as sugar
96 |         if self.start_stimulus!=None and self.start_trial!=None:
97 |                 self.rel_start_stim = self.start_stimulus - self.start_trial
98 |         else: self.rel_start_stim=None
99 |         if self.start_response!=None and self.start_trial!=None:
100|                 self.rel_start_resp = self.start_response - self.start_trial
101|         else:self.rel_start_resp = None
102|         if self.end_response!=None and self.start_trial!=None:
103|                 self.rel_end_resp   = self.end_response   - self.start_trial
104|         else: self.rel_end_resp = None
105|         if self.end_trial!=None and self.start_trial!=None:
106|                 self.duration       = self.end_trial      - self.start_trial
107|         else: self.duration = None
108|         
109|     def is_occuring(self, time, include_quiescent = False):
110|         '''
111|         Parameters
112|         ----------
113|         time : float
114|             A time (in the timebasis used in the experiment's Timeline.mat).
115|         include_quiescent : bool, optional
116|             Whether to consider the quiescent period. The default is False.
117| 
118| 
119|         Returns
120|         -------
121|         occuring : bool
122|             True if trial is occuring at time else false.
123| 
124|         '''
125|         
126|         if all((include_quiescent, time > self.start_trial, time < self.end_trial)):
127|             return True
128|         elif time>self.start_stimulus and time<self.end_response:
129|                 return True
130|         else:
131|             return False
132|     def to_dict(self):
133|         return {
134|             'test': self.istest,
135|             'go': self.isgo,
136|             'side': 'left' if self.isleft else ('right' if self.isright else 'unknown'),
137|             'correct': self.correct,
138|             'affirmative': self.affirmative,
139|             'contrast':self.contrast,
140|             'rel_start_stim': self.rel_start_stim,
141|             "rel_start_resp": self.rel_start_resp,
142|             "rel_end_resp": self.rel_end_resp,
143|             "trial_duration": self.duration
144|             }
145| 
146| 
147| class Trial(SparseTrial):
148|     '''
149|     Encapusulates a single trial event. Contains information about the
150|     dF/F of neurons, licking information, and pupil activity that occured
151|     during the trial.
152|     '''
153|     
154|     def __init__(self,exp_path,struct,statistic_extractor,frame_times,
155|                  licks):
156|         
157|         if os.path.isdir(exp_path):
158|             self.exp_path  = exp_path
159|         else: 
160|             raise ValueError('recording must be an existing directory')
161|         
162|         #Get all the trial attributes and timing information via inheritance
163|         super().__init__(struct)
164|         
165|         #Get the df/f, deconvoluted firing, and licking
166|         #traces of ROIs during the trial (licking info is shared
167|         #by all ROIs in a trial)
168|         self.dF_on_F, self.spks, self.licks  = self.get_traces(statistic_extractor,
169|                                                     frame_times,
170|                                                     licks)
171| 
172|         
173|         #If there's no timepoints in dF_on_F, something has gone wrong:
174|         if self.dF_on_F.shape[1]<1:
175|             raise ValueError('Trial not contained in recording')
176|         
177|         self.ROI_identifiers = np.argwhere(statistic_extractor.iscell)
178|         
179|     def get_traces(self,statistic_extractor, frame_times, licks):
180|         all_traces = statistic_extractor.dF_on_F[statistic_extractor.iscell]
181|         all_spks   = statistic_extractor.spks[statistic_extractor.iscell]
182|         #We need the indexes of the frames corresponding to trial
183|         #start and end times
184|         start_idx = frame_times.searchsorted(self.start_stimulus - 1)
185|         end_idx   = start_idx + 26
186|         if end_idx>all_traces.shape[1]:
187|             raise ValueError("Trial not contained in recording")
188|         
189|         self.start_idx = start_idx
190|         self.end_idx = end_idx
191|         return (all_traces[:,start_idx:end_idx], all_spks[:,start_idx:end_idx],
192|                 licks[start_idx:end_idx])
193| 
194|         
195|     def to_dict(self):
196|         result = super.to_dict()
197|         result.update(
198|             {"dF_on_F": self.dF_on_F,
199|             "lick_rate": self.licks,
200|             'Trial_ID':self.trial_id,}
201|             )
202|         return result
203|     def __repr__(self):
204|         trialtype = GLOBALS.TRIALTYPE[self.stim_id]
205|         response = 'correct' if self.correct else 'incorrect'
206|         return f'{trialtype} trial with {response} response'
207|     def plot(self):
208|         seaborn.set()
209|         fig, ax = plt.subplots()
210|         #Plot all the ROI responses plus a mean
211|         for ROI in self.dF_on_F:
212|             ax.plot(ROI, alpha = 0.25, color = "black")
213|         ax.plot(np.mean(ROI,axis=0), alpha = 1, color = "orange")
214|         
215|         ylim = ax.get_ylim()
216|         #Add rectangles indicating stimulus visibility and response
217|         response = plt.Rectangle(
218|             xy = (self.rel_start_resp*30//6,ylim[0]),
219|             width = (self.rel_end_resp - self.rel_start_resp)*30//6,
220|             height = ylim[1],
221|             color = 'green' if self.affirmative else 'red',
222|             alpha = 0.5)
223|         stimulus = plt.Rectangle(
224|             xy = (self.rel_start_stim*30//6,ylim[0]),
225|             width = 2*30//6,
226|             height = ylim[1],
227|             color = 'green' if self.isgo else 'red',
228|             alpha = 0.5)
229|         ax.add_patch(response)
230|         ax.add_patch(stimulus)
231|         fig.show()
232| 
233| 
234|     
235| def _get_trial_structs(psychstim_path):
236|     '''
237|     Get matlab structs for each trial recorded in an experiement.
238| 
239|     Parameters
240|     ----------
241|     psychstim_path : str
242|         A psychstim.mat file in a parent directory.
243| 
244|     Returns
245|     -------
246|     trials : list of matlab struct objects
247|     '''
248|     matfile = loadmat(psychstim_path)
249|     expData = matfile["expData"]
250|     trialData = expData["trialData"]
251|     trials = []
252|     for trial in trialData:
253|         trials.append(trial)
254|     return trials
255| 
256| def get_trials_in_recording(exp_path, return_se=False, ignore_dprime=False,
257|                             se = None, suppress_dprime_error=False):
258|     '''
259|     Retrieve all the trials in a recording as Trial objects
260| 
261|     Parameters
262|     ----------
263|     exp_path : String
264|         Path to the experiment folder.
265|     return_se : Bool, optional
266|         Whether to also return a StatisticExtractor object for the 
267|         whole experiment. The default is False.
268| 
269|     Returns
270|     -------
271|     [Trial] or ([Trial], StatisticExtractor)
272| 
273|     '''
274|     
275|     #Get the appropriate paths for the suite2p info, the timeline,
276|     #and the trial metadata
277|     files          = os.listdir(exp_path)
278|     s2p_path       = os.path.join(exp_path,'suite2p','plane0')
279|     timeline_path  = os.path.join(exp_path,
280|                                   item(
281|                                       [s for s in files if 'Timeline.mat' in s]))
282|     psychstim_path = os.path.join(exp_path,
283|                                   item(
284|                                       [s for s in files if 'psychstim.mat' in s]))
285|     trials = []
286|     if calc_d_prime(psychstim_path)>1 or ignore_dprime:
287|         if se==None:
288|             se      = Recording(exp_path)
289|         #We need the total number of frames:
290|         nframes = se.ops["nframes"]
291|         times   = get_neural_frame_times(timeline_path,nframes)
292|         structs = _get_trial_structs(psychstim_path)
293|         licks = get_lick_state_by_frame(timeline_path, times)
294|         
295|         for struct in structs:
296|             trial = Trial(exp_path,struct,se,times, licks)
297|             trials.append(trial)
298|         return trials if not return_se else (trials,se)
299|     elif suppress_dprime_error:
300|         return None if not return_se else (None,None)
301|     else:
302|         raise ValueError("Dprime below 1")
303| 
304| def add_trials_in_rec_to_file(filestream, exp_path):
305|     for trial in get_trials_in_recording(exp_path):
306|         pkl.dump(trial,filestream)
307| 
308| 
309| def dump_all_trials_in_dataset_to_pkl_file(drive = 'E:\\',
310|                                            path = "all_one_plane_trials.pkl"):
311|     all_trials = open("all_one_plane_trials.pkl",'wb')
312|     add_trials_to_file = (lambda exp_path:
313|                               add_trials_in_rec_to_file(
314|                                   all_trials, exp_path))
315|     apply_to_all_one_plane_recordings(drive,add_trials_to_file)
316|     all_trials.close()
317| 


\Observations\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\ProcessFluorescence\data_cleaning.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Aug  7 15:06:01 2020
3  | 
4  | @author: viviani
5  | 
6  | It's necessary to remove duplicate ROIs, ie ROIs that correspond to different
7  | spacial sections of the same axon. This is likely to occur, for example, as
8  | the axon weaves in and out of the plane of section.
9  | """
10 | 
11 | 
12 | import pandas as pd
13 | import numpy as np
14 | import networkx as nx
15 | import matplotlib.pyplot as plt
16 | import seaborn as sb
17 | from random import randint
18 | 
19 | 
20 | def merge_rois(roi_array, spks=None, iscell=None):
21 |     '''
22 |     Merge together ROI traces with a pairwise pearson's R above 0.9.
23 |     
24 |     Parameters
25 |     ----------
26 |     roi_array : Array of float of shape (rois, timepoints)
27 |         An array of dF/F0 values indexed by time and roi
28 | 
29 |     Returns
30 |     -------
31 |     merged_roi_array : Array of float of shape (merged_rois, timepoints)
32 |         merged_rois <= rois.
33 | 
34 |     '''
35 |     rois, timepoints = roi_array.shape
36 |     df = pd.DataFrame(data = roi_array.transpose(), 
37 |                       index = range(timepoints),
38 |                       columns = range(rois))
39 |     output, graph = merge_correlated_columns(df, return_graph=True)
40 |     merged_roi_array = output.to_numpy().transpose()
41 |     
42 |     if spks is not None and iscell is not None:
43 |         df = pd.DataFrame(data = spks.transpose(), 
44 |                   index = range(timepoints),
45 |                   columns = range(rois))
46 |         merged_spks_array = merge_correlated_columns(df,graph=graph).to_numpy().transpose()>0
47 |         df = pd.DataFrame(data = iscell[np.newaxis,:], 
48 |                   index = (0,),
49 |                   columns = range(rois))
50 |         merged_iscell_array = merge_correlated_columns(df,graph=graph).to_numpy().reshape((-1))>0
51 |         return (merged_roi_array,merged_spks_array,merged_iscell_array)
52 |     return merged_roi_array
53 | 
54 | 
55 | 
56 | def merge_correlated_columns(df, graph = None, return_graph=False):
57 |     new_df = pd.DataFrame()
58 |     if type(graph)==type(None):
59 |         adjacency_matrix = get_adj_matr(df)
60 |         nodes            = adjacency_matrix.columns
61 |         edges            = ls_of_edges_from_adj_matr(adjacency_matrix)
62 |         graph            = construct_graph_from(nodes,
63 |                                                 edges)
64 |     for component in nx.connected_components(graph):
65 |         name = ", ".join(str(component))
66 |         name = f"[{name}]"
67 |         new_df[name] = df[component].mean(axis='columns')
68 |     return new_df if not return_graph else (new_df, graph)
69 |         
70 | 
71 | 
72 | def ls_of_edges_from_adj_matr(adj_matr):
73 |     return adj_matr[adj_matr > 0].stack().index.tolist()
74 | 
75 | 
76 | def get_adj_matr(df, cutoff = 0.9):
77 |     corrs            = df.corr()
78 |     adjacency_matrix = (corrs - np.eye(corrs.shape[0])) > cutoff
79 |     return adjacency_matrix
80 | 
81 | 
82 | def construct_graph_from(nodes,edges):
83 |     graph = nx.Graph()
84 |     graph.add_nodes_from(nodes)
85 |     graph.add_edges_from(edges)
86 |     return graph
87 | 
88 | 
89 | 
90 | 
91 | def unit_test1():
92 |     N = 100
93 |     N_CLUSTERS = 4
94 |     MAX_COPIES = 4
95 |     MIN_COPIES = 2
96 |     NOISE = 0.36
97 |     
98 |     for i in range(100):
99 |         df = pd.DataFrame()
100|         name = 0
101|         for i in range(N_CLUSTERS):
102|             base = np.random.uniform(0,1,size=N)
103|             for j in range(randint(MIN_COPIES,MAX_COPIES)):
104|                 df[str(name)]   = base + np.random.uniform(-NOISE/2,NOISE/2,size=N)
105|                 name+=1
106|         
107|         
108|         adjacency_matrix = get_adj_matr(df)
109|         nodes            = adjacency_matrix.columns
110|         edges            = ls_of_edges_from_adj_matr(adjacency_matrix)
111|         graph            = construct_graph_from(nodes,
112|                                                 edges)
113| 
114|         new_df = merge_correlated_columns(df)
115|         
116|         
117|         
118|         new_corrs    = new_df.corr()
119|         new_adj_matr = get_adj_matr(new_df)
120|         new_edges    = ls_of_edges_from_adj_matr(new_adj_matr)
121|         
122|         new_graph = construct_graph_from(new_adj_matr.columns,
123|                                          new_edges)
124|         
125|         #Evaluate Results
126|         initially_unconnected = not adjacency_matrix.any(axis=None)
127|         finally_unconnected   = not new_adj_matr.any(axis=None)
128|         
129|         
130|         if not initially_unconnected:
131|             try: assert finally_unconnected
132|             except AssertionError as e:
133|                 # fig,ax = plt.subplots(ncols = 2, figsize = (12,6))
134|                 # ax[0].set_title("Initial Graph")
135|                 # ax[1].set_title("Final Graph")
136|                 # nx.draw_networkx(graph, with_labels = True, ax=ax[0])        
137|                 # nx.draw_networkx(new_graph, with_labels = True, ax=ax[1])
138|                 # fig.show()
139|                 # raise e
140|                 pass
141|     
142|     fig,ax = plt.subplots(ncols = 2, nrows = 2, figsize = (12,6))
143|     ax[0][0].set_title("Time Series before merging")
144|     ax[0][0].imshow(df.to_numpy().transpose())
145|     ax[0][1].set_title("Time Series after merging")
146|     ax[0][1].imshow(new_df.to_numpy().transpose())
147|     ax[1][0].set_title("Correlation Graph (before merging)")
148|     ax[1][1].set_title("Correlation Graph (after merging)")
149|     nx.draw_networkx(graph, nx.kamada_kawai_layout(graph), 
150|                      with_labels = True, ax=ax[1][0])        
151|     nx.draw_networkx(new_graph, nx.kamada_kawai_layout(new_graph),
152|                      with_labels = True, ax=ax[1][1])
153|     fig.show()
154| 
155| def unit_test2():
156|     N = 100
157|     ROIS = 100
158|     TIMEPOINTS = 200
159|     NUM_CLUSTERS = 4
160|     NUM_PER_CLUSTER = 15
161|     NOISE = 0.3
162|     
163|     clusters = NUM_CLUSTERS
164|     i = 0
165|     data = np.zeros((ROIS,TIMEPOINTS))
166|     while i < ROIS:
167|         base = np.random.uniform(0,1,size=TIMEPOINTS)
168|         if clusters>0:
169|             for _ in range(NUM_PER_CLUSTER):
170|                 data[i] = base + np.random.uniform(-NOISE/2,NOISE/2,size=TIMEPOINTS)
171|                 i+=1
172|             clusters -= 1
173|         else:
174|             data[i] = base
175|             i+=1
176| 
177|     data_merged = merge_rois(data)
178|     fig,ax = plt.subplots(ncols = 2, figsize = (12,6))
179|     ax[0].set_title("Time Series before merging")
180|     ax[0].imshow(data)
181|     ax[1].set_title("Time Series after merging")
182|     ax[1].imshow(data_merged)
183|     fig.show()
184|     return data
185|     
186| 
187| if __name__=="__main__":
188|     sb.set_style("dark")
189|     data = unit_test2()


\ProcessFluorescence\df_on_f_calculations.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Aug  7 15:06:01 2020
3  | 
4  | @author: viviani
5  | """
6  | import numpy as np
7  | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
8  | from scipy.stats import linregress
9  | from scipy.optimize import minimize
10 | 
11 | 
12 | 
13 | # def subtract_neuropil_trace(F, Fneu, alpha = 0.8):
14 | #     subtracted_trace =  F - alpha*Fneu
15 | #     #add values until the lowest point has unit fluorescence
16 | #     return subtracted_trace
17 | 
18 | def subtract_neuropil_trace(F, Fneu):
19 |     thetas = HubelRegressor.regress_all(Fneu,F)
20 |     betas = thetas[:,1]
21 |     subtracted_trace =  F - Fneu*betas[:,None]
22 |     shifts = ramp(-1*np.min(subtracted_trace, axis = -1))[:,None]
23 |     subtracted_trace += shifts #Shift up explicitylu s.t. no Fcell < 0
24 |     return subtracted_trace
25 | 
26 | def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 300):
27 |     mode = 'nearest'
28 |     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode=mode),
29 |                             tau2,
30 |                             mode = 'reflect')
31 |     return result
32 | 
33 | def get_df_on_f0(F,F0=None):
34 |     if type(F0)!=type(None):
35 |         return (F - F0) / F0
36 |     else:
37 |         F0 = get_smoothed_running_minimum(F)
38 |         return get_df_on_f0(F,F0)
39 | 
40 | def log_transform(dF_on_F):
41 |     m = np.min(dF_on_F)
42 |     return np.log(dF_on_F - (m<0)*(m) + 1e-16)
43 | 
44 | class HubelRegressor:
45 |     default_k = 100
46 |     @classmethod
47 |     def loss(cls,params, x, y, k=None, delta=None):
48 |         if k is None: k = cls.default_k
49 |         if delta is None:
50 |             delta = np.std(y)
51 |         guess = params[0] + params[1] * x
52 |         residuals = y - guess #data points above line are positive
53 |         upper_cost = (residuals>0)*(delta**2 * (np.sqrt(1 + (residuals/delta)**2)-1))
54 |         lower_cost = (residuals < 0)*(k)*(residuals**(2))
55 |         return np.sum(upper_cost + lower_cost)
56 |     @classmethod
57 |     def grad(cls,params, x, y, k=None, delta = None):
58 |         if k is None: k = cls.default_k
59 |         if delta is None:
60 |             delta = np.std(y)/2
61 |         guess = params[0] + params[1] * x
62 |         residuals = y - guess #data points above line are positive
63 |         per_upper_residual = (residuals > 0)*(residuals / np.sqrt(1+(residuals/delta)**2))
64 |         per_lower_residual = (residuals < 0)*(2*k*residuals)
65 |         outer_derivative  = np.sum(per_upper_residual + per_lower_residual)
66 |         theta_0 = np.sum(outer_derivative)*(-1)
67 |         theta_1 = np.sum(outer_derivative*(-x))
68 |         grad = (theta_0, theta_1)
69 |         return grad
70 |     @classmethod
71 |     def guess(cls,x, y):
72 |         slope, intercept, r_value, p_value, std_err = linregress(x,y)
73 |         return (intercept, slope)
74 |     @classmethod
75 |     def regress(cls,x, y):
76 |         start_params = cls.guess(x, y)
77 |         k = cls.default_k
78 |         reg = None
79 |         with np.errstate(over='raise', divide = 'raise'):
80 |             while True:
81 |                 try:
82 |                     reg = minimize(cls.loss,
83 |                                x0 = start_params,
84 |                                jac = cls.grad,
85 |                                args = (x, y, k),
86 |                                bounds = ((None,None), (0, None)),
87 |                                method = "L-BFGS-B")
88 |                     k+=1
89 |                     break
90 |                 except FloatingPointError:
91 |                     if reg: break
92 |                     else: k -= 1
93 |                     
94 |         intercept = reg.x[0] - ramp(np.max(x*reg.x[1] - y))
95 |         return (intercept, reg.x[1])
96 |     @classmethod
97 |     def regress_all(cls,X,Y):
98 |         res = []
99 |         for x,y in zip(X,Y):
100|             res.append(cls.regress(x,y))
101|         return np.array(res)
102|     
103| 
104| def ramp(X):
105|     return X*(X>0)


\ProcessFluorescence\NovelRegressionDevelopment\df_on_f_novel_regression_scratchpad.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | """
3  | Created on Sun Aug 30 09:59:00 2020
4  | 
5  | @author: Vivian Imbriotis
6  | """
7  | 
8  | import numpy as np
9  | import seaborn as sns
10 | from scipy.optimize import minimize, check_grad, approx_fprime
11 | from scipy.stats import linregress, norm
12 | import matplotlib.pyplot as plt
13 | from matplotlib.animation import FuncAnimation
14 | from mpl_toolkits.mplot3d import Axes3D
15 | from sklearn.linear_model import TheilSenRegressor
16 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
17 | 
18 | from accdatatools.Observations.recordings import Recording
19 | import pickle as pkl
20 | 
21 | #Load a recording
22 | try:
23 |     with open(r"C:/Users/viviani/desktop/cache.pkl","rb") as file:
24 |         rec = pkl.load(file)
25 | except (FileNotFoundError, EOFError):
26 |     with open(r"C:/Users/viviani/desktop/cache.pkl","wb") as file:
27 |         rec = Recording(r"D:\Local_Repository\CFEB013\2016-05-31_02_CFEB013")
28 |         pkl.dump(rec,file)
29 | 
30 | def heaviside(X,k=200):
31 |     '''
32 |     Analytic approximation of the heaviside function.
33 |     Approaches Heaviside(x) as K goes to +inf
34 |     '''
35 |     return 1/(1+np.exp(-2*k*X))
36 | 
37 | def ramp(X):
38 |     return X*(X>0)
39 | 
40 | def squash(X,a=100):
41 |     '''
42 |     Squashes X from positive reals to the [0,1) half-open
43 |     interval.
44 |     '''
45 |     return 1 - np.exp(-(X/a)**2)
46 | 
47 | 
48 | def asymmetric_ramp_loss(params, x, y):
49 |     guess = params[0] + params[1] * x
50 |     residuals = y - guess #data points above line are positive
51 |     upper_cost = np.sum(residuals[residuals > 0] / np.sum(residuals > 0)  )
52 |     lower_cost = np.sum((residuals < 0))
53 |     cost = upper_cost + lower_cost
54 |     return cost
55 | 
56 | def asymmetric_ramp_grad(params, x, y, diracs = False):
57 |     N = len(x) if hasattr(x,"__len__") else 1
58 |     guess = params[0] + params[1] * x
59 |     residuals = y - guess #data points above line are positive
60 |     if diracs:
61 |         theta_0 = -np.sum((residuals > 0) + K*dirac(residuals))
62 |         theta_1 = -np.sum(x*[residuals > 0] + x*K*dirac(residuals))
63 |     else:
64 |         theta_0 = -np.sum((residuals > 0))
65 |         theta_1 = -np.sum(x*[residuals > 0])
66 |     return (theta_0, theta_1)
67 | 
68 | K = 1
69 |     
70 | def squashed_loss(params, x = rec.Fneu[4], y = rec.F[4], v=True, k = 1, a = 1):
71 |     '''
72 |     As a one-liner this is
73 |     (1-exp(-ramp(x)**2)) + N*1/(1 + exp(2Kx)) + ramp(-x)
74 |     where x is a residual, summed for all residuals
75 |     '''
76 |     guess = params[0] + params[1] * x
77 |     residuals = y - guess #data points above line are positive
78 |     residuals_cost = np.sum(squash(ramp(residuals),a))
79 |     negativity_cost = np.sum(x.shape*heaviside(-residuals,k) + ramp(-residuals))
80 |     cost = residuals_cost + negativity_cost
81 |     return cost
82 | 
83 | def squashed_grad(params, x = rec.Fneu[4], y = rec.F[4], k = 1, a = 1):
84 |     N = len(x) if hasattr(x,"__len__") else 1
85 |     guess = params[0] + params[1] * x
86 |     residuals = y - guess #data points above line are positive
87 |     term1 = 2*residuals*np.exp(-((np.sum(ramp(residuals)))/a)**2)/(a**2)*heaviside(residuals,k)
88 |     log_term2 = np.log(2*k*N) + 2*k*residuals - 2*np.log(1+np.exp(2*k*residuals))
89 |     term2 = np.exp(log_term2)
90 |     term3 = heaviside(residuals,k)
91 |     outer_derivative = term1 + term2 + term3
92 |     theta_0 = np.sum(outer_derivative) * (-1)
93 |     theta_1 = np.sum(outer_derivative * (-x))
94 |     return (theta_0, theta_1)
95 | 
96 | def quadratic_loss(params,x = rec.Fneu[4], y = rec.F[4],k=4,v=False):
97 |     guess = params[0] + params[1] * x
98 |     residuals = y - guess #data points above line are positive
99 |     upper_cost = np.sum(ramp(residuals)**2)
100|     lower_cost = np.sum(k*ramp(-residuals)*(2))
101|     cost = upper_cost + lower_cost
102|     return cost
103| 
104| def quadratic_grad(params,x = rec.Fneu[4], y = rec.F[4],k=4,v=False):
105|     guess = params[0] + params[1] * x
106|     res = y - guess #data points above line are positive
107|     outer_derivative = (res>0)*(2*res)+(res<0)*(k*res**(2))
108|     theta_0 = np.sum(outer_derivative)*(-1)
109|     theta_1 = np.sum(outer_derivative*(-x))
110|     grad = (theta_0, theta_1)
111|     return grad
112| 
113| def normalize(x,y):
114|     vector = np.array((x,y))
115|     norms = np.linalg.norm(vector,axis=0)
116|     return vector / norms
117| 
118| 
119| dirac = norm(0,1e-2).pdf
120| 
121| 
122| def guess(x, y):
123|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
124|     return (intercept, slope)
125| 
126| class ParabolicRegressor:
127|     default_k = 70
128|     @classmethod
129|     def loss(cls,params,x, y, k=None, v=False):
130|         if k is None: k = cls.default_k
131|         guess = params[0] + params[1] * x
132|         residuals = y - guess #data points above line are positive
133|         upper_cost = np.sum(ramp(residuals)**2)
134|         lower_cost = np.sum(2*ramp(-residuals)**(2))
135|         cost = upper_cost + lower_cost
136|         return cost
137|     @classmethod
138|     def grad(cls,params,x, y,k=None,v=False):
139|         if k is None: k = cls.default_k
140|         guess = params[0] + params[1] * x
141|         res = y - guess #data points above line are positive
142|         outer_derivative = (res>0)*(2*res)+(res<0)*(2*k*res**(2*k-1))
143|         theta_0 = np.sum(outer_derivative)*(-1)
144|         theta_1 = np.sum(outer_derivative*(-x))
145|         grad = (theta_0, theta_1)
146|         return grad
147|     @classmethod
148|     def guess(cls,x, y):
149|         slope, intercept, r_value, p_value, std_err = linregress(x,y)
150|         return (intercept, slope)
151|     @classmethod
152|     def regress(cls,x, y):
153|         start_params = cls.guess(x, y)
154|         k = cls.default_k
155|         reg = None
156|         with np.errstate(over='raise'):
157|             while True:
158|                 try:
159|                     reg = minimize(cls.loss,
160|                                x0 = start_params,
161|                                jac = cls.grad,
162|                                args = (x, y, k),
163|                                bounds = ((None,None), (0, None)),
164|                                method = "L-BFGS-B")
165|                     k+=1
166|                     break
167|                 except FloatingPointError:
168|                     if reg: break
169|                     else: k -= 1
170|                     
171|         intercept = reg.x[0] - ramp(np.max(reg.x[0]+x*reg.x[1] - y))
172|         return (intercept, reg.x[1])
173| 
174| 
175| class HubelRegressor:
176|     default_k = 100
177|     @classmethod
178|     def loss(cls,params, x, y, k=None, delta=None):
179|         if k is None: k = cls.default_k
180|         if delta is None:
181|             delta = np.std(y)/2
182|         guess = params[0] + params[1] * x
183|         residuals = y - guess #data points above line are positive
184|         upper_cost = (residuals>0)*(delta**2 * (np.sqrt(1 + (residuals/delta)**2)-1))
185|         lower_cost = (residuals < 0)*(k)*(residuals**(2))
186|         return np.sum(upper_cost + lower_cost)
187|     @classmethod
188|     def grad(cls,params, x, y, k=None, delta = None):
189|         if k is None: k = cls.default_k
190|         if delta is None:
191|             delta = np.std(y)/2
192|         guess = params[0] + params[1] * x
193|         residuals = y - guess #data points above line are positive
194|         per_upper_residual = (residuals > 0)*(residuals / np.sqrt(1+(residuals/delta)**2))
195|         per_lower_residual = (residuals < 0)*(2*k*residuals)
196|         outer_derivative  = np.sum(per_upper_residual + per_lower_residual)
197|         theta_0 = np.sum(outer_derivative)*(-1)
198|         theta_1 = np.sum(outer_derivative*(-x))
199|         grad = (theta_0, theta_1)
200|         return grad
201|     @classmethod
202|     def guess(cls,x, y):
203|         slope, intercept, r_value, p_value, std_err = linregress(x,y)
204|         return (intercept, slope)
205|     @classmethod
206|     def regress(cls,x, y):
207|         start_params = cls.guess(x, y)
208|         k = cls.default_k
209|         reg = None
210|         with np.errstate(over='raise', divide = 'raise'):
211|             while True:
212|                 try:
213|                     reg = minimize(cls.loss,
214|                                x0 = start_params,
215|                                jac = cls.grad,
216|                                args = (x, y, k),
217|                                bounds = ((None,None), (0, None)),
218|                                method = "L-BFGS-B")
219|                     k+=1
220|                     break
221|                 except FloatingPointError:
222|                     if reg: break
223|                     else: k -= 1
224|                     
225|         intercept = reg.x[0] - ramp(np.max(reg.x[0]+x*reg.x[1] - y))
226|         return (intercept, reg.x[1])
227| 
228| def underline_regression(x, y, method = "ramp"):
229|     start_params = guess(x, y)
230|     if method=="ramp":
231|         reg = minimize(asymmetric_ramp_loss,
232|                    x0 = start_params,
233|                    args = (x, y),
234|                    bounds = ((None, None), (0, None)),
235|                    method = "Powell")
236|     elif method == 'quadratic' or method == "parabolic":
237|         reg = ParabolicRegressor.regress(x,y)
238|         return reg
239|     elif method == "squashed":
240|         reg = minimize(squashed_loss,
241|                    x0 = start_params,
242|                    jac = squashed_grad,
243|                    args = (x, y),
244|                    bounds = ((None,None), (0, 1)),
245|                    method = "L-BFGS-B")
246|     elif method == "median":
247|         y = y.reshape(-1, 1)
248|         X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
249|         reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
250|         offset = np.min(subtract_bg(y, x, [reg.coef_[0],reg.coef_[1]]))
251|         return np.array([reg.coef_[0] + offset, reg.coef_[1]])
252|     elif method == "huber":
253|         reg = HubelRegressor.regress(x,y)
254|         return reg
255|     return (reg.x[0], reg.x[1])
256| 
257| 
258| def robust_regression(x, y):
259|     y = y.reshape(-1, 1)
260|     X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
261|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
262| 
263|     return (reg.coef_[0], reg.coef_[1])
264| 
265| def subtract_bg(f, bg, theta):
266|     return f - theta[0] - (theta[1] * bg)
267| 
268| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
269|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
270|                             tau2,
271|                             mode = 'reflect')
272|     return result
273| 
274| def get_df_on_f0(F,F0=None):
275|     if type(F0)!=type(None):
276|         return (F - F0) / F0
277|     else:
278|         F0 = get_smoothed_running_minimum(F)
279|         return get_df_on_f0(F,F0)
280| 
281| 
282| class UnderlineRegressionFigure:
283|     color = plt.rcParams['axes.prop_cycle'].by_key()['color']
284|     # color[0] = 'blue'
285|     # color[1] = 'orange'
286|     # color[2] = 'green'
287|     # color[3] = 'red'
288|     def __init__(self, F, Fneu, methods):
289|         sns.set_style("dark")
290|         sns.set_context("paper")
291|         
292|         thetas = []
293|         df_traces = []
294|         for method in methods:
295|             theta = underline_regression(Fneu, F, method)
296|             underline_f = subtract_bg(F, Fneu, theta)
297|             underline_df_f = get_df_on_f0(underline_f)
298|             thetas.append(theta)
299|             df_traces.append(underline_df_f)
300|         
301|         self.fig = plt.figure(figsize=(8,4),tight_layout=True)
302|         self.axes = []
303|         regression_axis = self.fig.add_axes((0.075,0.1,0.4,0.85))
304|         self.axes.append(regression_axis)
305|         regression_axis.plot(Fneu, F, 'o', color = self.color[0])
306|         regression_axis.set_ylabel('Cell Fluorescence')
307|         regression_axis.set_xlabel('Neuropil Fluorescence')
308|         method_axes_height = 0.9/len(methods)
309|         for idx, (color, df_trace, theta, method) in enumerate(zip(self.color[1:],df_traces,thetas,methods)):
310|             print(theta)
311|             regression_axis.plot(Fneu, theta[0] + theta[1]*Fneu, label = method,
312|                                  color = color)
313|             method_axis = self.fig.add_axes((0.55,0.05 + idx*method_axes_height,
314|                                              0.40,method_axes_height))
315|             method_axis.plot(df_trace, label = method,
316|                           color = color)
317|             method_axis.set_ylabel('Predicted DF/F')
318|             method_axis.set_xticks([])
319|             method_axis.legend(loc='upper right')
320|             self.axes.append(method_axis)
321|             
322|     def show(self):
323|         self.fig.show()
324| 
325| def plot_loss_functions():
326|     X = np.linspace(-3,3,1000)
327|     params = (0,0)
328|     ramp_loss = [asymmetric_ramp_loss(params,np.array(0),np.array(x)) for x in X]
329|     quad_loss = [quadratic_loss(params,np.array(0),np.array(x),k=10) for x in X]
330|     hubel_loss = [HubelRegressor.loss(params,np.array(0),np.array(x),k=10, delta=0.5) for x in X]
331|     print(hubel_loss)
332|     fig, ax = plt.subplots(ncols = 3)
333|     ax[0].plot(X,ramp_loss)
334|     ax[0].set_title("Heaviside/L1 loss")
335|     ax[1].plot(X,quad_loss)
336|     ax[1].set_title("Asymmetric L2 loss")
337|     ax[2].plot(X,hubel_loss)
338|     ax[2].set_title("L2/Pseudo-Huber loss")
339|     for a in ax:
340|         a.set_ylabel("Loss")
341|         a.set_xlabel("Residual")
342|         a.set_ylim((0,6))
343|     fig.show()
344| 
345| def plot_huber_loss():
346|     fig,ax = plt.subplots(figsize=(4,4),constrained_layout=True)
347|     X = np.linspace(-3,3,1000)
348|     params = (0,0)
349|     hubel_loss = [HubelRegressor.loss(params,np.array(0),np.array(x),k=10, delta=0.5) for x in X]
350|     ax.plot(X,hubel_loss)
351|     ax.set_ylabel("Loss")
352|     ax.set_xlabel("Residual ($\hat(F) -F$)")
353|     ax.set_ylim((0,6))
354|     fig.show()
355| 
356| if __name__=="__main__":
357|     plt.close('all')
358|     plot_huber_loss()
359|     # UnderlineRegressionFigure(rec.F[57],rec.Fneu[57], methods = ("ramp","quadratic","huber")).show()


\ProcessFluorescence\NovelRegressionDevelopment\df_on_f_novel_regression_scratchpad_copy.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | """
3  | Created on Sun Aug 30 09:59:00 2020
4  | 
5  | @author: Vivian Imbriotis
6  | """
7  | 
8  | import numpy as np
9  | import seaborn as sns
10 | from scipy.optimize import minimize, check_grad, approx_fprime
11 | from scipy.stats import linregress, norm
12 | import matplotlib.pyplot as plt
13 | from matplotlib.animation import FuncAnimation
14 | from mpl_toolkits.mplot3d import Axes3D
15 | from sklearn.linear_model import TheilSenRegressor
16 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
17 | 
18 | from accdatatools.Observations.recordings import Recording
19 | import pickle as pkl
20 | 
21 | #Load a recording
22 | try:
23 |     with open(r"C:/Users/viviani/desktop/cache.pkl","rb") as file:
24 |         rec = pkl.load(file)
25 | except (FileNotFoundError, EOFError):
26 |     with open(r"C:/Users/viviani/desktop/cache.pkl","wb") as file:
27 |         rec = Recording(r"D:\Local_Repository\CFEB013\2016-05-31_02_CFEB013")
28 |         pkl.dump(rec,file)
29 | 
30 | def heaviside(X,k=200):
31 |     '''
32 |     Analytic approximation of the heaviside function.
33 |     Approaches Heaviside(x) as K goes to +inf
34 |     '''
35 |     return 1/(1+np.exp(-2*k*X))
36 | 
37 | def ramp(X):
38 |     return X*(X>0)
39 | 
40 | def squash(X,a=100):
41 |     '''
42 |     Squashes X from positive reals to the [0,1) half-open
43 |     interval.
44 |     '''
45 |     return 1 - np.exp(-(X/a)**2)
46 | 
47 | 
48 | def asymmetric_ramp_loss(params, x, y):
49 |     guess = params[0] + params[1] * x
50 |     residuals = y - guess #data points above line are positive
51 |     upper_cost = np.sum(residuals[residuals > 0] / np.sum(residuals > 0)  )
52 |     lower_cost = np.sum((residuals < 0))
53 |     cost = upper_cost + lower_cost
54 |     return cost
55 | 
56 | def asymmetric_ramp_grad(params, x, y, diracs = False):
57 |     N = len(x) if hasattr(x,"__len__") else 1
58 |     guess = params[0] + params[1] * x
59 |     residuals = y - guess #data points above line are positive
60 |     if diracs:
61 |         theta_0 = -np.sum((residuals > 0) + K*dirac(residuals))
62 |         theta_1 = -np.sum(x*[residuals > 0] + x*K*dirac(residuals))
63 |     else:
64 |         theta_0 = -np.sum((residuals > 0))
65 |         theta_1 = -np.sum(x*[residuals > 0])
66 |     return (theta_0, theta_1)
67 | 
68 | K = 1
69 |     
70 | def squashed_loss(params, x = rec.Fneu[4], y = rec.F[4], v=True, k = 1, a = 1):
71 |     '''
72 |     As a one-liner this is
73 |     (1-exp(-ramp(x)**2)) + N*1/(1 + exp(2Kx)) + ramp(-x)
74 |     where x is a residual, summed for all residuals
75 |     '''
76 |     guess = params[0] + params[1] * x
77 |     residuals = y - guess #data points above line are positive
78 |     residuals_cost = np.sum(squash(ramp(residuals),a))
79 |     negativity_cost = np.sum(x.shape*heaviside(-residuals,k) + ramp(-residuals))
80 |     cost = residuals_cost + negativity_cost
81 |     return cost
82 | 
83 | def squashed_grad(params, x = rec.Fneu[4], y = rec.F[4], k = 1, a = 1):
84 |     N = len(x) if hasattr(x,"__len__") else 1
85 |     guess = params[0] + params[1] * x
86 |     residuals = y - guess #data points above line are positive
87 |     term1 = 2*residuals*np.exp(-((np.sum(ramp(residuals)))/a)**2)/(a**2)*heaviside(residuals,k)
88 |     log_term2 = np.log(2*k*N) + 2*k*residuals - 2*np.log(1+np.exp(2*k*residuals))
89 |     term2 = np.exp(log_term2)
90 |     term3 = heaviside(residuals,k)
91 |     outer_derivative = term1 + term2 + term3
92 |     theta_0 = np.sum(outer_derivative) * (-1)
93 |     theta_1 = np.sum(outer_derivative * (-x))
94 |     return (theta_0, theta_1)
95 | 
96 | def quadratic_loss(params,x = rec.Fneu[4], y = rec.F[4],k=4,v=False):
97 |     guess = params[0] + params[1] * x
98 |     residuals = y - guess #data points above line are positive
99 |     upper_cost = np.sum(ramp(residuals)**2)
100|     lower_cost = np.sum(ramp(-residuals)**(2*k))
101|     cost = upper_cost + lower_cost
102|     return cost
103| 
104| def quadratic_grad(params,x = rec.Fneu[4], y = rec.F[4],k=4,v=False):
105|     guess = params[0] + params[1] * x
106|     res = y - guess #data points above line are positive
107|     outer_derivative = (res>0)*(2*res)+(res<0)*(2*k*res**(2*k-1))
108|     theta_0 = np.sum(outer_derivative)*(-1)
109|     theta_1 = np.sum(outer_derivative*(-x))
110|     grad = (theta_0, theta_1)
111|     return grad
112| 
113| def normalize(x,y):
114|     vector = np.array((x,y))
115|     norms = np.linalg.norm(vector,axis=0)
116|     return vector / norms
117| 
118| 
119| dirac = norm(0,1e-2).pdf
120| 
121| 
122| def guess(x, y):
123|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
124|     return (intercept, slope)
125| 
126| class ParabolicRegressor:
127|     default_k = 30
128|     @classmethod
129|     def loss(cls,params,x, y, k=None, v=False):
130|         if k is None: k = cls.default_k
131|         guess = params[0] + params[1] * x
132|         residuals = y - guess #data points above line are positive
133|         upper_cost = np.sum(ramp(residuals)**2)
134|         lower_cost = np.sum(ramp(-residuals)**(2*k))
135|         cost = upper_cost + lower_cost
136|         return cost
137|     @classmethod
138|     def grad(cls,params,x, y,k=None,v=False):
139|         if k is None: k = cls.default_k
140|         guess = params[0] + params[1] * x
141|         res = y - guess #data points above line are positive
142|         outer_derivative = (res>0)*(2*res)+(res<0)*(2*k*res**(2*k-1))
143|         theta_0 = np.sum(outer_derivative)*(-1)
144|         theta_1 = np.sum(outer_derivative*(-x))
145|         grad = (theta_0, theta_1)
146|         return grad
147|     @classmethod
148|     def guess(cls,x, y):
149|         slope, intercept, r_value, p_value, std_err = linregress(x,y)
150|         return (intercept, slope)
151|     @classmethod
152|     def regress(cls,x, y):
153|         start_params = cls.guess(x, y)
154|         k = cls.default_k
155|         reg = None
156|         with np.errstate(over='raise'):
157|             while True:
158|                 try:
159|                     reg = minimize(cls.loss,
160|                                x0 = start_params,
161|                                jac = cls.grad,
162|                                args = (x, y, k),
163|                                bounds = ((None,None), (0, None)),
164|                                method = "L-BFGS-B")
165|                     k+=1
166|                 except FloatingPointError:
167|                     if reg: break
168|                     else: k -= 1
169|                     
170|         intercept = reg.x[0] - ramp(np.max(reg.x[0]+x*reg.x[1] - y))
171|         return (intercept, reg.x[1])
172| 
173| 
174| class HubelRegressor:
175|     default_k = 30
176|     @classmethod
177|     def loss(cls,params, x, y, k=None, delta=None):
178|         if k is None: k = cls.default_k
179|         if delta is None:
180|             delta = np.std(y)/2
181|         guess = params[0] + params[1] * x
182|         residuals = y - guess #data points above line are positive
183|         upper_cost = (residuals>0)*(delta**2 * (np.sqrt(1 + (residuals/delta)**2)-1))
184|         lower_cost = (residuals < 0)*(residuals**(2*k))
185|         return np.sum(upper_cost + lower_cost)
186|     @classmethod
187|     def grad(cls,params, x, y, k=None, delta = None):
188|         if k is None: k = cls.default_k
189|         if delta is None:
190|             delta = np.std(y)/2
191|         guess = params[0] + params[1] * x
192|         residuals = y - guess #data points above line are positive
193|         per_upper_residual = (residuals > 0)*(residuals / np.sqrt(1+(residuals/delta)**2))
194|         per_lower_residual = (residuals < 0)*(2*k*residuals**(2*k-1))
195|         outer_derivative  = np.sum(per_upper_residual + per_lower_residual)
196|         theta_0 = np.sum(outer_derivative)*(-1)
197|         theta_1 = np.sum(outer_derivative*(-x))
198|         grad = (theta_0, theta_1)
199|         return grad
200|     @classmethod
201|     def guess(cls,x, y):
202|         slope, intercept, r_value, p_value, std_err = linregress(x,y)
203|         return (intercept, slope)
204|     @classmethod
205|     def regress(cls,x, y):
206|         start_params = cls.guess(x, y)
207|         k = cls.default_k
208|         reg = None
209|         with np.errstate(over='raise', divide = 'raise'):
210|             while True:
211|                 try:
212|                     reg = minimize(cls.loss,
213|                                x0 = start_params,
214|                                jac = cls.grad,
215|                                args = (x, y, k),
216|                                bounds = ((None,None), (0, None)),
217|                                method = "L-BFGS-B")
218|                     k+=1
219|                 except FloatingPointError:
220|                     if reg: break
221|                     else: k -= 1
222|                     
223|         intercept = reg.x[0] - ramp(np.max(reg.x[0]+x*reg.x[1] - y))
224|         return (intercept, reg.x[1])
225| 
226| def underline_regression(x, y, method = "ramp"):
227|     start_params = guess(x, y)
228|     if method=="ramp":
229|         reg = minimize(asymmetric_ramp_loss,
230|                    x0 = start_params,
231|                    args = (x, y),
232|                    bounds = ((None, None), (0, None)),
233|                    method = "Powell")
234|     elif method == 'quadratic' or method == "parabolic":
235|         reg = ParabolicRegressor.regress(x,y)
236|         return reg
237|     elif method == "squashed":
238|         reg = minimize(squashed_loss,
239|                    x0 = start_params,
240|                    jac = squashed_grad,
241|                    args = (x, y),
242|                    bounds = ((None,None), (0, 1)),
243|                    method = "L-BFGS-B")
244|     elif method == "median":
245|         y = y.reshape(-1, 1)
246|         X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
247|         reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
248|         offset = np.min(subtract_bg(y, x, [reg.coef_[0],reg.coef_[1]]))
249|         return np.array([reg.coef_[0] + offset, reg.coef_[1]])
250|     elif method == "huber":
251|         reg = HubelRegressor.regress(x,y)
252|         return reg
253|     return (reg.x[0], reg.x[1])
254| 
255| 
256| def robust_regression(x, y):
257|     y = y.reshape(-1, 1)
258|     X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
259|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
260| 
261|     return (reg.coef_[0], reg.coef_[1])
262| 
263| def subtract_bg(f, bg, theta):
264|     return f - theta[0] - (theta[1] * bg)
265| 
266| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
267|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
268|                             tau2,
269|                             mode = 'reflect')
270|     return result
271| 
272| def get_df_on_f0(F,F0=None):
273|     if type(F0)!=type(None):
274|         return (F - F0) / F0
275|     else:
276|         F0 = get_smoothed_running_minimum(F)
277|         return get_df_on_f0(F,F0)
278| 
279| 
280| class UnderlineRegressionFigure:
281|     color = plt.rcParams['axes.prop_cycle'].by_key()['color']
282|     # color[0] = 'blue'
283|     # color[1] = 'orange'
284|     # color[2] = 'green'
285|     # color[3] = 'red'
286|     def __init__(self, F, Fneu, methods):
287|         sns.set_style("dark")
288|         sns.set_context("paper")
289|         
290|         thetas = []
291|         df_traces = []
292|         for method in methods:
293|             theta = underline_regression(Fneu, F, method)
294|             underline_f = subtract_bg(F, Fneu, theta)
295|             print(np.mean(underline_f))
296|             underline_df_f = get_df_on_f0(underline_f)
297|             thetas.append(theta)
298|             df_traces.append(underline_df_f)
299|         
300|         self.fig = plt.figure(figsize=(8,4),tight_layout=True)
301|         self.axes = []
302|         regression_axis = self.fig.add_axes((0.075,0.1,0.4,0.85))
303|         self.axes.append(regression_axis)
304|         regression_axis.plot(Fneu, F, 'o', color = self.color[0])
305|         regression_axis.set_ylabel('Cell Fluorescence')
306|         regression_axis.set_xlabel('Neuropil Fluorescence')
307|         method_axes_height = 0.9/len(methods)
308|         for idx, (color, df_trace, theta, method) in enumerate(zip(self.color[1:],df_traces,thetas,methods)):
309|             print(theta)
310|             regression_axis.plot(Fneu, theta[0] + theta[1]*Fneu, label = method,
311|                                  color = color)
312|             method_axis = self.fig.add_axes((0.55,0.05 + idx*method_axes_height,
313|                                              0.40,method_axes_height))
314|             method_axis.plot(df_trace, label = method,
315|                           color = color)
316|             method_axis.set_ylabel('Predicted DF/F')
317|             method_axis.set_xticks([])
318|             method_axis.legend(loc='upper right')
319|             self.axes.append(method_axis)
320|             
321|     def show(self):
322|         self.fig.show()
323| 
324| def plot_loss_functions():
325|     X = np.linspace(-3,3,1000)
326|     params = (0,0)
327|     ramp_loss = [asymmetric_ramp_loss(params,np.array(0),np.array(x)) for x in X]
328|     quad_loss = [quadratic_loss(params,np.array(0),np.array(x),k=10) for x in X]
329|     hubel_loss = [HubelRegressor.loss(params,np.array(0),np.array(x),k=10, delta=0.5) for x in X]
330|     squashed = [squashed_loss(params,np.array(0),np.array(x)) for x in X]
331|     print(hubel_loss)
332|     fig, ax = plt.subplots(ncols = 4)
333|     ax[0].plot(X,ramp_loss)
334|     ax[1].plot(X,quad_loss)
335|     ax[2].plot(X,hubel_loss)
336|     ax[3].plot(X,squashed)
337|     for a in ax:
338|         a.set_ylim((0,6))
339|     fig.show()
340| 
341| 
342| 
343| plt.close('all')
344| plot_loss_functions()
345| # UnderlineRegressionFigure(rec.F[57],rec.Fneu[57], methods = ("ramp","quadratic","huber")).show()


\ProcessFluorescence\NovelRegressionDevelopment\df_on_f_novel_regression_test.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | """
3  | Created on Sun Aug 30 09:59:00 2020
4  | 
5  | @author: Vivian Imbriotis
6  | """
7  | 
8  | import numpy as np
9  | import seaborn as sns
10 | from scipy.optimize import minimize, check_grad, approx_fprime
11 | from scipy.stats import linregress, norm
12 | import matplotlib.pyplot as plt
13 | from matplotlib.animation import FuncAnimation
14 | from mpl_toolkits.mplot3d import Axes3D
15 | from sklearn.linear_model import TheilSenRegressor
16 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
17 | 
18 | from accdatatools.Observations.recordings import Recording
19 | import pickle as pkl
20 | 
21 | DEFAULT_K = 20
22 | 
23 | 
24 | #Load a recording
25 | try:
26 |     with open(r"C:/Users/viviani/desktop/cache.pkl","rb") as file:
27 |         rec = pkl.load(file)
28 | except (FileNotFoundError, EOFError):
29 |     with open(r"C:/Users/viviani/desktop/cache.pkl","wb") as file:
30 |         rec = Recording(r"D:\Local_Repository\CFEB013\2016-05-31_02_CFEB013")
31 |         pkl.dump(rec,file)
32 | 
33 | 
34 | 
35 | def ramp(X):
36 |     return X*(X>0)
37 | 
38 | dirac = norm(0,10).pdf
39 | 
40 | 
41 | def original_loss(params, x, y):
42 |     guess = params[0] + params[1] * x
43 |     residuals = y - guess #data points above line are positive
44 |     upper_cost = np.sum(residuals[residuals > 0] / np.sum(residuals > 0)  )
45 |     lower_cost = np.sum(K * (residuals < 0))
46 |     cost = upper_cost + lower_cost
47 |     return cost
48 | 
49 | 
50 | def original_grad(params, x, y, diracs = False):
51 |     guess = params[0] + params[1] * x
52 |     residuals = y - guess #data points above line are positive
53 |     if diracs:
54 |         theta_0 = -np.sum((residuals > 0) + K*dirac(residuals))
55 |         theta_1 = -np.sum(x*[residuals > 0] + x*K*dirac(residuals))
56 |     else:
57 |         theta_0 = -np.sum((residuals > 0))
58 |         theta_1 = -np.sum(x*[residuals > 0])
59 |     return (theta_0, theta_1)
60 | 
61 | 
62 | 
63 | K = 1
64 |     
65 | 
66 | 
67 | def qloss(params,x = rec.Fneu[4], y = rec.F[4],k=DEFAULT_K,v=False):
68 |     guess = params[0] + params[1] * x
69 |     residuals = y - guess #data points above line are positive
70 |     upper_cost = np.sum(ramp(residuals)**2)
71 |     lower_cost = np.sum(ramp(-residuals)**(2*k))
72 |     cost = upper_cost + lower_cost
73 |     return cost
74 | 
75 | def qgrad(params,x = rec.Fneu[4], y = rec.F[4],k=DEFAULT_K,v=False):
76 |     guess = params[0] + params[1] * x
77 |     res = y - guess #data points above line are positive
78 |     outer_derivative = (res>0)*(2*res)+(res<0)*(2*k*res**(2*k-1))
79 |     theta_0 = np.sum(outer_derivative)*(-1)
80 |     theta_1 = np.sum(outer_derivative*(-x))
81 |     grad = (theta_0, theta_1)
82 |     return grad
83 | 
84 | def normalize(x,y):
85 |     vector = np.array((x,y))
86 |     norms = np.linalg.norm(vector,axis=0)
87 |     return vector / norms
88 | 
89 | def plot_quadratic_loss(i):
90 |     plt.close('all')
91 |     X = np.arange(-200, 300, 12.5)
92 |     Y = np.arange(-5, 5, 0.25)
93 |     fig2, ax = plt.subplots()
94 |     X, Y = np.meshgrid(X, Y)
95 |     U,V = np.vectorize(lambda x,y:qgrad(
96 |         (x,y),rec.Fneu[i,0],rec.F[i,0]))(X,Y)
97 |     U,V = normalize(U,V)
98 |     ax.quiver(X,Y,-U,-V, angles = "xy")
99 |     ax.set_xlabel("Intercept")
100|     ax.set_ylabel("Slope")
101|     ax.set_title("-Grad(Loss)")
102|     Z = np.vectorize(lambda x,y:qloss((x,y),rec.Fneu[i,0],rec.F[i,0],v=True))(X,Y)
103|     fig = plt.figure()
104|     ax = fig.gca(projection='3d')
105|     ax.plot_surface(X,Y,np.log(Z))
106|     ax.set_xlabel("Intercept")
107|     ax.set_ylabel("Slope")
108|     ax.set_zlabel("Log Loss")
109|     ax.set_title("Loss Function")
110|     plt.show()
111| 
112| 
113| 
114| def guess(x, y):
115|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
116|     return (intercept, slope)
117| 
118| def underline_regression(x, y, method = "Powell"):
119|     start_params = guess(x, y)
120|     if method=="Powell":
121|         reg = minimize(original_loss,
122|                    x0 = start_params,
123|                    args = (x, y),
124|                    bounds = ((None, None), (0, None)),
125|                    method = "Powell")
126|     elif method in ("BFGS","L-BFGS-B"):
127|         reg = minimize(qloss,
128|                    x0 = start_params,
129|                    jac = qgrad,
130|                    args = (x, y),
131|                    bounds = ((None,None), (0, None)),
132|                    method = "L-BFGS-B")
133|         print(reg.message)
134|     else:
135|         raise ValueError()
136|     return (reg.x[0], reg.x[1])
137| 
138| 
139| def subtract_bg(f, bg, theta):
140|     return f - ( theta[0] + theta[1] * bg)
141| 
142| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
143|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
144|                             tau2,
145|                             mode = 'reflect')
146|     return result
147| 
148| 
149| def get_df_on_f0(F,F0=None):
150|     if type(F0)!=type(None):
151|         return (F - F0) / F0
152|     else:
153|         F0 = get_smoothed_running_minimum(F)
154|         return get_df_on_f0(F,F0)
155| 
156| 
157| class UnderlineRegressionFigure:
158|     color = plt.rcParams['axes.prop_cycle'].by_key()['color']
159|     # color[0] = 'blue'
160|     # color[1] = 'orange'
161|     # color[2] = 'green'
162|     # color[3] = 'red'
163|     def __init__(self, recording, roi_number, method = 'BFGS'):
164|         sns.set_style("dark")
165|         sns.set_context("paper")
166|         robust = method in ('robust','seigel','theil sen')
167|         f = np.stack((np.zeros(recording.F[roi_number,:].shape),
168|                       recording.F[roi_number,:], 
169|                       recording.Fneu[roi_number,:]
170|                               ),axis=-1)
171|         if method in ("BFGS","grad descent", "gradient descent"):
172|             alt_theta = underline_regression(f[:,2], f[:,1],method="BFGS")
173|             alt_f = subtract_bg(f[:,1], f[:,2], alt_theta)
174|             alt_df_f = get_df_on_f0(alt_f)
175|         elif method in ("cg","CG"):
176|             alt_theta = underline_regression(f[:,2], f[:,1],method="CG")
177|             alt_f = subtract_bg(f[:,1], f[:,2], alt_theta)
178|             alt_df_f = get_df_on_f0(alt_f)
179|         else:
180|             alt_theta = underline_regression(f[:,2], f[:,1],method=method)
181|             alt_f = subtract_bg(f[:,1], f[:,2], alt_theta)
182|             alt_df_f = get_df_on_f0(alt_f)
183|             
184|         
185|         underline_theta = underline_regression(f[:,2], f[:,1])
186|         underline_f = subtract_bg(f[:,1], f[:,2], underline_theta)
187|         underline_df_f = get_df_on_f0(underline_f)
188|         
189|         self.fig = plt.figure(figsize=(8,4),tight_layout=True)
190|         
191|         regression_axis = self.fig.add_axes((0.075,0.1,0.4,0.85))
192|         regression_axis.plot(f[:,2], f[:,1], 'o', color = self.color[0])
193| 
194|         regression_axis.plot(f[:,2], alt_theta[0] + alt_theta[1]*f[:,2], label = f'Underline Regression with {method} method',
195|                              color = self.color[1])
196|         regression_axis.plot(f[:,2], underline_theta[0] + underline_theta[1]*f[:,2], label = 'Underline Regression with Powell method',
197|                              color = self.color[2])
198|         regression_axis.set_ylabel('Cell Fluorescence')
199|         regression_axis.set_xlabel('Neuropil Fluorescence')
200|         regression_axis.legend(loc='upper left')
201| 
202|         
203|         alt_axis = self.fig.add_axes((0.55,0.5,0.4,0.4))
204| 
205|         alt_axis.plot(alt_df_f, label = f'Underline Regression with {method} Method',
206|                       color = self.color[1])
207|         alt_axis.set_ylabel('DF/F')
208|         alt_axis.set_xticks([])
209|         alt_axis.legend(loc='upper right')
210|         underline_axis = self.fig.add_axes((0.55,0.1,0.4,0.4))
211|         underline_axis.plot(underline_df_f, label = 'Underline Regression with Powell Method',
212|                             color=self.color[2])
213|         underline_axis.set_ylabel('DF/F')
214|         underline_axis.set_xlabel('Sample')
215|         underline_axis.set_xticks([])
216|         underline_axis.legend(loc='upper right')
217|         
218|         self.axes = (regression_axis, alt_axis, underline_axis)
219|         self.f = f
220|     def show(self):
221|         self.fig.show()
222| 
223| 
224| class ThreeKindsOfRegressionFigure(UnderlineRegressionFigure):
225|     def __init__(self,recording,roi_number):
226|         super().__init__(recording,roi_number)
227|         (regression_axis, alt_axis, underline_axis) = self.axes
228|         f = self.f
229|         alt_axis.set_position((0.55,0.37,0.4,0.27),which='original')
230|         underline_axis.set_position((0.55,0.1,0.4,0.27),which = 'original')
231|         
232|         boring_theta = guess(f[:,2], f[:,1])
233|         boring_f = subtract_bg(f[:,1], f[:,2], boring_theta)
234|         boring_df_f = get_df_on_f0(boring_f)
235|         regression_axis.plot(f[:,2], boring_theta[0] + boring_theta[1]*f[:,2], 
236|                              label = 'OLS Regression',
237|                              color = self.color[3])
238|         regression_axis.legend()
239|         boring_axis = self.fig.add_axes((0.55,0.64,0.4,0.27))
240|         boring_axis.plot(boring_df_f, label = 'OLS Regresison', color = self.color[3])
241|         boring_axis.set_ylabel('DF/F')
242|         boring_axis.set_xticks([])
243|         boring_axis.legend(loc='upper right')
244| 
245| 
246| class DescentFigure:
247|     def __init__(self,recording = rec, i = 4):
248|         self.f = np.stack((np.zeros(recording.F[i,:].shape),
249|           recording.F[i,:], 
250|           recording.Fneu[i,:]
251|                   ),axis=-1)
252|         self.X = rec.Fneu[i]
253|         self.Y = rec.F[i]
254|         # self.attempts = list(reversed(self.regress().allvecs))
255|         # self.fig,ax = plt.subplots()
256|         # ax.plot(self.f[:,2], self.f[:,1], 'o')
257|         # self.line, = ax.plot([],[])
258|         # self.animation = FuncAnimation(self.fig,self.update_line,
259|         #                                len(self.attempts), blit=True,
260|         #                                interval = 500,
261|         #                                repeat_delay = 1000)
262| 
263|     def regress(self):
264|         x,y = self.X, self.Y
265|         start_params = guess(x, y)
266|         reg = minimize(qloss,
267|                    x0 = start_params,
268|                    jac = qgrad,
269|                    args = (x, y),
270|                    bounds = ((None, None), (0, None)),
271|                    method = "L-BFGS-B")
272|         return reg
273| 
274| plt.close('all')
275| 
276| 
277| ThreeKindsOfRegressionFigure(rec,4).show()


\ProcessFluorescence\NovelRegressionDevelopment\df_on_f_novel_regression_validation_with_created_data.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Sep  1 11:12:15 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | from time import time
7  | import numpy as np
8  | import matplotlib.pyplot as plt
9  | from scipy.stats import linregress
10 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
11 | from sklearn.linear_model import TheilSenRegressor
12 | from df_on_f_novel_regression_test import underline_regression
13 | from vectorised_underline_regression import UnderlineRegressor
14 | import seaborn as sns
15 | 
16 | ###SETTINGS###
17 | 
18 | np.random.seed(72)
19 | 
20 | 
21 | 
22 | 
23 | sns.set_style("dark")
24 | 
25 | def make_data(n_points = 10000):
26 |     spikes = make_spikes(n_points)
27 |     bg = make_bg(n_points)
28 |     data = {}
29 |     data["frame_n"] = np.arange(0, n_points)
30 |     data["ground_truth"] = spikes
31 |     data["bg"] = bg
32 |     data["slope"] = np.random.random()+0.5
33 |     data["raw"] = data["ground_truth"] + data["slope"]*bg
34 |     return data
35 | 
36 | def make_spikes(n_points = 10000):
37 |     spike_prob = 0.01
38 |     spike_size_mean = 1
39 |     kernel = make_kernel()
40 |     spike_times = 1.0 * (np.random.rand(n_points-kernel.size+1) < spike_prob)
41 |     spike_amps = np.random.lognormal(mean = spike_size_mean, sigma = 0.5, size=spike_times.size) * spike_times
42 |     spikes = np.convolve(spike_amps, kernel) + 5*(np.random.randn(n_points) / 2) + 100
43 |     return spikes
44 | 
45 | def make_bg(n_points = 10000):
46 |     return np.sin(np.arange(0, n_points)/(6.28*100))/2 + np.random.randn(n_points)/2 + 10
47 | 
48 | 
49 | def make_kernel(tau = 10):
50 |     length = tau*8
51 |     time = np.arange(0, length)
52 |     return np.exp(-time/tau)
53 |    
54 | 
55 | #Just plotting and helper functions below
56 | 
57 | 
58 | def plot_data_creation_process():
59 |     data = make_data()
60 |     fig, ax = plt.subplots(nrows = 3,tight_layout= True)
61 |     ax[0].set_title("Cell Fluorescence")
62 |     ax[0].plot(data["frame_n"],data["ground_truth"])
63 |     ax[1].set_title("Background Fluorescence")
64 |     ax[1].plot(data["frame_n"],data["bg"])
65 |     ax[2].set_title("Measured ROI Fluorescence")
66 |     ax[2].plot(data["frame_n"],data["raw"])
67 |     fig.show()
68 | 
69 | def plot_data():
70 |     data = make_data()
71 |     data = proc_data(data)
72 |     fig, ax = plt.subplots(nrows = 3, ncols = 2,figsize = (15, 9))
73 | 
74 |     ax[0][0].plot(data["frame_n"], data["raw"])
75 |     ax[0][0].set_ylabel('Raw F')
76 |     ax[0][0].set_xlabel('Frame Number')
77 | 
78 |     ax[0][1].plot(data["bg"], data["raw"], 'o')
79 |     ax[0][1].plot(data["bg"], data["NRR_theta"][0] + data["NRR_theta"][1]*data["bg"], label = 'Naive Robust Regression')
80 |     ax[0][1].plot(data["bg"], data["RR_theta"][0] + data["RR_theta"][1]*data["bg"], label = 'Robust Regression')
81 |     ax[0][1].set_ylabel('Total Fluorescence')
82 |     ax[0][1].set_xlabel('Neuropil Fluorescence')
83 |     ax[0][1].legend(loc='upper left')
84 | 
85 |     ax[1][0].plot(data["frame_n"], data["ground_truth"])
86 |     ax[1][0].set_ylabel("Ground Truth")
87 | 
88 |     ax[1][1].plot(data["frame_n"], data["NRR_df_on_f"])
89 |     ax[1][1].set_ylabel("Naive Robust Reg df/f")
90 | 
91 |     ax[2][0].plot(data["frame_n"], data["RR_df_on_f"])
92 |     ax[2][0].set_ylabel("Robust Regression df/f")
93 | 
94 |     ax[2][1].plot(data["ground_truth"], data["RR_df_on_f"], 'o')
95 |     ax[2][1].plot(data["ground_truth"], data["ground_truth"], label= 'Unity')
96 |     ax[2][1].set_title("MeanSquareError = " + str(calc_error(data)))
97 |     ax[2][1].set_xlabel("Ground Truth")
98 |     ax[2][1].set_ylabel("Robust Regression Aproach")
99 |     ax[2][1].legend(loc='upper left')
100|     fig.show()
101| 
102| def compare_approaches(df = False):
103|     data = make_data()
104|     data = proc_data(data)
105|     fig, ax = plt.subplots(nrows = 4, ncols = 2,figsize = (15, 12),
106|                            tight_layout=True)
107|     
108|     ax[0][0].plot(data["frame_n"], data["gt_df_on_f"]if df else data["ground_truth"])
109|     ax[0][0].set_ylabel(f"Ground Truth {'dF/F0' if df else 'Fcell'}")
110| 
111|     ax[0][1].plot(data["bg"], data["raw"], 'o')
112|     ax[0][1].plot(data["bg"], data["RR_theta"][0] + data["RR_theta"][1]*data["bg"], label = 'Robust Regression')
113|     ax[0][1].plot(data["bg"], data["CR_theta"][0] + data["CR_theta"][1]*data["bg"], label = 'Biased L1 Regression (Powell Method)')
114|     ax[0][1].plot(data["bg"], data["PR_theta"][0] + data["PR_theta"][1]*data["bg"], label = 'Biased L2 Regression (BFGS Method)')
115|     ax[0][1].set_ylabel('Total Fluorescence')
116|     ax[0][1].set_xlabel('Neuropil Fluorescence')
117|     ax[0][1].legend(loc='upper left')
118| 
119| 
120|     ax[1][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
121|                   data["RR_df_on_f"] if df else data["RR_bg_subtracted"], 'o')
122|     ax[1][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
123|                   data["gt_df_on_f"]if df else data["ground_truth"], 
124|                   label= 'Unity')
125|     ax[1][1].set_title(f"MeanSquareError = {calc_error(data,df)[0]:.8f}")
126|     ax[1][1].set_xlabel("Ground Truth")
127|     ax[1][1].set_ylabel("Robust Regression Approach")
128|     ax[1][1].legend(loc='upper left')
129| 
130|     ax[1][0].plot(data["frame_n"], 
131|                   data["RR_df_on_f"] if df else data["RR_bg_subtracted"])
132|     ax[1][0].set_ylabel(f"Robust Regression {'dF/F0' if df else 'Fcell'}")
133| 
134|     ax[2][0].plot(data["frame_n"], 
135|                   data["CR_df_on_f"] if df else data["CR_bg_subtracted"])
136|     ax[2][0].set_ylabel(f"L1-Heaviside Regression {'dF/F0' if df else 'Fcell'}")
137| 
138|     ax[2][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
139|                   data["CR_df_on_f"] if df else data["CR_bg_subtracted"], 'o')
140|     ax[2][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
141|                   data["gt_df_on_f"]if df else data["ground_truth"], 
142|                   label= 'Unity')
143|     ax[2][1].set_title(f"MeanSquareError = {calc_error(data,df)[1]:.8f}")
144|     ax[2][1].set_xlabel("Ground Truth")
145|     ax[2][1].set_ylabel("Custom Regression Approach 1")
146|     ax[2][1].legend(loc='upper left')
147| 
148|     ax[3][0].plot(data["frame_n"], 
149|                   data["PR_df_on_f"] if df else data["PR_bg_subtracted"])
150|     ax[3][0].set_ylabel(f"Biased L2 vs L2k norm Regression {'dF/F0' if df else 'Fcell'}")
151| 
152|     ax[3][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
153|                   data["PR_df_on_f"] if df else data["PR_bg_subtracted"], 'o')
154|     ax[3][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
155|                   data["gt_df_on_f"]if df else data["ground_truth"], 
156|                   label= 'Unity')
157|     ax[3][1].set_title(f"MeanSquareError = {calc_error(data,df)[2]:.8f}")
158|     ax[3][1].set_xlabel("Ground Truth")
159|     ax[3][1].set_ylabel("Custom Regression Approach 2")
160|     ax[3][1].legend(loc='upper left')
161|     fig.show()
162| 
163| 
164| def time_approaches():
165|     N = np.linspace(80,100000,10).astype(int)
166|     robust_regression_times = np.zeros(N.shape)
167|     powell_times = np.zeros(N.shape)
168|     L2_norm_times = np.zeros(N.shape)
169|     for idx,n_points in enumerate(N):
170|         print(idx)
171|         t1=np.zeros(10); t2=np.zeros(10); t3=np.zeros(10); t4=np.zeros(10)
172|         for i in range(1):
173|             data = make_data(n_points)
174|             t1[i] = time()
175|             robust_regression(data["bg"], data["raw"])
176|             t2[i] = time()
177|             custom_regression(data["bg"], data["raw"])
178|             t3[i] = time()
179|             parabolic_regression(data["bg"], data["raw"])
180|             t4[i] = time()
181|         robust_regression_times[idx] = (t2-t1).mean()
182|         powell_times[idx] = (t3-t2).mean()
183|         L2_norm_times[idx] = (t4-t3).mean()
184|     fig,ax = plt.subplots(nrows = 3, sharex=True)
185|     ax[0].set_title("Robust Regression Time Complexity")
186|     ax[0].plot(N,robust_regression_times)
187|     ax[1].set_title("Powell Method Time Complexity")
188|     ax[1].plot(N,powell_times)
189|     ax[1].set_ylabel("Execution Time")
190|     ax[2].set_title("BFGS Method Time Complexity")
191|     ax[2].plot(N,L2_norm_times)
192|     ax[2].set_xlabel("Number of data points")
193|     fig.show()
194| 
195|         
196| 
197| def proc_data(data):
198|     data["gt_df_on_f"] = get_df_on_f0(data["ground_truth"])
199|     data["NRR_theta"] = naive_robust_regression(data["bg"], data["raw"])
200|     data["NRR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["NRR_theta"])
201|     data["NRR_df_on_f"] = get_df_on_f0(data["NRR_bg_subtracted"])
202| 
203|     data["RR_theta"] = robust_regression(data["bg"], data["raw"])
204|     data["RR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["RR_theta"])
205|     data["RR_df_on_f"] = get_df_on_f0(data["RR_bg_subtracted"])
206|     
207|     data["CR_theta"] = custom_regression(data["bg"], data["raw"])
208|     data["CR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["CR_theta"])
209|     data["CR_df_on_f"] = get_df_on_f0(data["CR_bg_subtracted"])
210| 
211|     data["PR_theta"] = parabolic_regression(data["bg"], data["raw"])
212|     data["PR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["PR_theta"])
213|     data["PR_df_on_f"] = get_df_on_f0(data["PR_bg_subtracted"])
214|     return data
215| 
216| def calc_error(data,df):
217|     ground_truth = data["gt_df_on_f"]if df else data["ground_truth"]
218|     return (np.sum((ground_truth - (data["RR_df_on_f"] if df else data["RR_bg_subtracted"]))**2)/ground_truth.size,
219|             np.sum((ground_truth - (data["CR_df_on_f"] if df else data["CR_bg_subtracted"]))**2)/ground_truth.size,
220|             np.sum((ground_truth - (data["PR_df_on_f"] if df else data["PR_bg_subtracted"]))**2)/ground_truth.size)
221| 
222| def ols(x,y):
223|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
224|     return np.array([intercept, slope])
225| 
226| def naive_robust_regression(x, y):
227|     y = y.reshape(-1, 1)
228|     X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
229|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
230| 
231|     return np.array([reg.coef_[0], reg.coef_[1]])
232| 
233| def robust_regression(x, y):
234|     y_reshape = y.reshape(-1, 1)
235|     X = np.vstack((np.ones(y_reshape.shape).transpose(), x.reshape(-1, 1).transpose()))
236|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y_reshape))
237| 
238|     # subtracted_data = subtract_bg(y, x, [reg.coef_[0], reg.coef_[1]])
239|     subtracted_data = y - reg.coef_[0] - reg.coef_[1]*x
240|     offset = np.min(subtracted_data)
241| 
242|     return np.array([reg.coef_[0]+offset, reg.coef_[1]])
243| 
244| 
245| def custom_regression(x, y):
246|     return underline_regression(x,y, method = "Powell")
247| 
248| def parabolic_regression(x,y):
249|     return UnderlineRegressor.regress(x,y)
250| 
251| def subtract_bg(f, bg, theta):
252|     # return f - ( theta[0] + theta[1] * bg)
253|     return f - theta[1]*bg
254| 
255| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
256|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
257|                             tau2,
258|                             mode = 'reflect')
259|     return result
260| 
261| def get_df_on_f0(F,F0=None):
262|     if not F0 is None:
263|         return (F - F0) / F0
264|     else:
265|         F0 = get_smoothed_running_minimum(F)
266|         return get_df_on_f0(F,F0)
267| 
268| 
269| 
270| compare_approaches(df=True)


\ProcessFluorescence\NovelRegressionDevelopment\negative_value_detection_script.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Aug 28 19:23:32 2020
3  | 
4  | @author: viviani
5  | """
6  | import numpy as np
7  | 
8  | from accdatatools.Observations.recordings import Recording
9  | from accdatatools.Utils.map_across_dataset import iterate_across_recordings
10 | 
11 | for exp in iterate_across_recordings(drive="H:\\"):
12 |     print(f"\rProcessing {exp}...")
13 | 
14 |     try:
15 |         a = Recording(exp)
16 |         if np.any(a.F<0) and np.any(a.Fneu<0):
17 |             for i in range(a.F.shape[0]):
18 |                 if np.any(a.F<0) and np.any(a.Fneu<0):
19 |                     df = pd.DataFrame
20 |                     df["F"] = a.F[i,:]
21 |                     df["Fneu"] = a.Fneu[i,:]
22 |                     df.to_csv(
23 |                         "C:/users/viviani/desktop/single_recording_F_Fneu.csv"
24 |                         )
25 |         else:
26 |             print(f"min F = {np.min(a.F).item():.1f}"+
27 |                   f"min Fneu = {np.min(a.Fneu).item():.1F}")
28 |     except Exception as e:
29 |         pass
30 | else:
31 |     del a
32 |     print("\nNot located in any experiment")


\ProcessFluorescence\NovelRegressionDevelopment\untitled0.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Wed Sep  2 11:30:00 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | import numpy as np
8  | import matplotlib.pyplot as plt
9  | from scipy.stats import linregress
10 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
11 | from sklearn.linear_model import TheilSenRegressor
12 | 
13 | np.random.seed(42)
14 | 
15 | def make_data(n_points = 10000):
16 |     spikes = make_spikes(n_points)
17 |     bg = make_bg(n_points)
18 |     data = {}
19 |     data["frame_n"] = np.arange(0, n_points)
20 |     data["ground_truth"] = spikes
21 |     data["bg"] = bg
22 |     data["slope"] = np.random.random()+0.5
23 |     data["raw"] = data["ground_truth"] + data["slope"]*bg
24 |     return data
25 | 
26 | def make_spikes(n_points = 10000):
27 |     spike_prob = 0.01
28 |     spike_size_mean = 1
29 |     kernel = make_kernel()
30 |     spike_times = 1.0 * (np.random.rand(n_points-kernel.size+1) < spike_prob)
31 |     spike_amps = np.random.lognormal(mean = spike_size_mean, sigma = 0.5, size=spike_times.size) * spike_times
32 |     spikes = np.convolve(spike_amps, kernel) + np.random.randn(n_points) / 2 + 10
33 |     return spikes
34 | 
35 | def make_bg(n_points = 10000):
36 |     return np.sin(np.arange(0, n_points)/(6.28*10))/2 + np.random.randn(n_points)/2
37 | 
38 | 
39 | def make_kernel(tau = 10):
40 |     length = tau*8
41 |     time = np.arange(0, length)
42 |     return np.exp(-time/tau)
43 |    
44 | 
45 | #Just plotting and helper functions below
46 | 
47 | 
48 | def plot_data():
49 |     data = make_data()
50 | 
51 |     data = proc_data(data)
52 |    
53 |     plt.figure(figsize = (15, 9))
54 |     plt.subplot(321)
55 |     plt.plot(data["frame_n"], data["raw"])
56 |     plt.ylabel('Raw F')
57 |     plt.xlabel('Frame Number')
58 |    
59 |     plt.subplot(322)
60 |     plt.plot(data["bg"], data["raw"], 'o')
61 |     plt.plot(data["bg"], data["NRR_theta"][0] + 10*data["NRR_theta"][1] + data["NRR_theta"][1]*data["bg"], label = 'Naive Robust Regression')
62 |     plt.plot(data["bg"], data["RR_theta"][0] + data["RR_theta"][1]*data["bg"], label = 'Robust Regression')
63 | 
64 |  
65 |     plt.ylabel('Total Fluorescence')
66 |     plt.xlabel('Neuropil Fluorescence')
67 |     plt.legend(loc='upper left')
68 |  
69 |     plt.subplot(323)
70 |     plt.plot(data["frame_n"], data["ground_truth"])
71 |     plt.ylabel("Ground Truth")
72 |    
73 |     plt.subplot(324)
74 |     plt.plot(data["frame_n"], data["NRR_df_on_f"])
75 |     plt.ylabel("Naive Robust Reg df/f")
76 | 
77 |     plt.subplot(325)
78 |     plt.plot(data["frame_n"], data["RR_df_on_f"])
79 |     plt.ylabel("Robust Regression df/f")
80 | 
81 |     plt.subplot(326)
82 |     plt.plot(data["ground_truth"], data["RR_df_on_f"], 'o')
83 |     plt.plot(data["ground_truth"], data["ground_truth"], label= 'Unity')
84 |     plt.title("MeanSquareError = " + str(calc_error(data)))
85 |     plt.xlabel("Ground Truth")
86 |     plt.ylabel("Robust Regression Aproach")
87 |     plt.legend(loc='upper left')
88 |    
89 | 
90 |    
91 | 
92 | plt.show(block = False)
93 | 
94 | def proc_data(data):
95 |     data["NRR_theta"] = naive_robust_regression(data["bg"], data["raw"])
96 |     data["NRR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["NRR_theta"])
97 |     data["NRR_df_on_f"] = get_df_on_f0(data["NRR_bg_subtracted"])
98 | 
99 |     data["RR_theta"] = robust_regression(data["bg"], data["raw"])
100|     data["RR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["RR_theta"])
101|     data["RR_df_on_f"] = get_df_on_f0(data["RR_bg_subtracted"])
102|     return data
103| 
104| def calc_error(data):
105|     return np.sum((data["ground_truth"] - data["RR_df_on_f"])**2)/data["ground_truth"].size
106| 
107| def ols(x,y):
108|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
109|     return np.array([intercept, slope])
110| 
111| def naive_robust_regression(x, y):
112|     y = y.reshape(-1, 1)
113|     X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
114|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
115| 
116|     return np.array([reg.coef_[0], reg.coef_[1]])
117| 
118| def robust_regression(x, y):
119|     y_reshape = y.reshape(-1, 1)
120|     X = np.vstack((np.ones(y_reshape.shape).transpose(), x.reshape(-1, 1).transpose()))
121|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y_reshape))
122| 
123|     subtracted_data = subtract_bg(y, x, [reg.coef_[0], reg.coef_[1]])
124|     offset = np.min(subtracted_data)
125| 
126|     return np.array([reg.coef_[0]+offset, reg.coef_[1]])
127| 
128| 
129| 
130| def subtract_bg(f, bg, theta):
131|     return f - ( theta[0] + theta[1] * bg)
132| 
133| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
134|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
135|                             tau2,
136|                             mode = 'reflect')
137|     return result
138| 
139| def get_df_on_f0(F,F0=None):
140|     if type(F0)!=type(None):
141|         return (F - F0) / F0
142|     else:
143|         F0 = get_smoothed_running_minimum(F)
144|         return get_df_on_f0(F,F0)
145| 
146| 
147| 
148| 
149| plot_data()


\ProcessFluorescence\NovelRegressionDevelopment\untitled2.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Sep  3 12:56:01 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | 
8  | from time import time
9  | import numpy as np
10 | import matplotlib.pyplot as plt
11 | from scipy.stats import linregress
12 | from scipy.ndimage.filters import minimum_filter1d, uniform_filter1d
13 | from sklearn.linear_model import TheilSenRegressor, HuberRegressor
14 | from df_on_f_novel_regression_scratchpad import underline_regression
15 | import seaborn as sns
16 | 
17 | ###SETTINGS###
18 | 
19 | 
20 | 
21 | 
22 | sns.set_style("dark")
23 | 
24 | def make_data(n_points = 10000):
25 |     spikes = make_spikes(n_points)
26 |     bg = make_bg(n_points)
27 |     data = {}
28 |     data["frame_n"] = np.arange(0, n_points)
29 |     data["ground_truth"] = spikes
30 |     data["bg"] = bg
31 |     data["slope"] = np.random.random()+0.5
32 |     data["raw"] = data["ground_truth"] + data["slope"]*bg + 5*np.random.rand(n_points)
33 |     return data
34 | 
35 | def make_spikes(n_points = 10000):
36 |     spike_prob = 0.01
37 |     spike_size_mean = 1
38 |     kernel = make_kernel()
39 |     spike_times = 1.0 * (np.random.rand(n_points-kernel.size+1) < spike_prob)
40 |     spike_amps = np.random.lognormal(mean = spike_size_mean, sigma = 0.5, size=spike_times.size) * spike_times
41 |     spikes = 5*np.convolve(spike_amps, kernel) + 8 + 6*np.random.rand(n_points)
42 |     return spikes
43 | 
44 | def make_bg(n_points = 10000):
45 |     return 6*(np.sin(np.arange(0, n_points)/(6.28))/2) + 30 + 10*np.random.rand(n_points)
46 | 
47 | 
48 | def make_kernel(tau = 10):
49 |     length = tau*8
50 |     time = np.arange(0, length)
51 |     return np.exp(-time/tau)
52 |    
53 | 
54 | #Just plotting and helper functions below
55 | 
56 | 
57 | def plot_data_creation_process(colors = plt.rcParams['axes.prop_cycle'].by_key()['color']):
58 |     data = make_data(1000)
59 |     data = proc_data(data)
60 |     
61 |     #construct plot layout with row titles:
62 |     nrows = 5
63 |     ncols = 2
64 |     fig, big_axes = plt.subplots(nrows = nrows, ncols = 1,tight_layout= True,
65 |                            figsize = (8,9))
66 |     for row, big_ax in enumerate(big_axes, start=1):
67 |         big_ax.axis("off")
68 |         big_ax._frameon = False
69 |     ax = np.empty((len(big_axes),ncols),dtype=object)
70 |     for row in range(len(big_axes)):
71 |         ax[row][0] = fig.add_subplot(len(big_axes),ncols,ncols*row+1)
72 |         ax[row][1] = fig.add_subplot(len(big_axes),ncols,ncols*row+2)
73 |     
74 |     for axis in ax.flatten():
75 |         axis.set_xticks([])
76 |         
77 | 
78 |     for axis in ax[2:,0]:
79 |         axis.set_yticks([])
80 |         axis.set_xlabel("Fneu")
81 |         axis.set_ylabel("F")
82 |     big_axes[0].set_title("Ground Truth")
83 |     ax[0][0].set_title("Cell Fluorescence")
84 |     ax[0][0].plot(data["frame_n"], data["ground_truth"])
85 |     ax[0][1].set_title("dF/F0")
86 |     ax[0][1].plot(data["frame_n"], data["gt_df_on_f"])
87 |     big_axes[1].set_title("Background Contamination")
88 |     ax[1][0].set_title("Neuropil Fluorescence")
89 |     ax[1][0].plot(data["frame_n"], data["bg"])
90 |     ax[1][1].set_title("Measured Fluorescence")
91 |     ax[1][1].plot(data["frame_n"], data["raw"])
92 |     
93 |     
94 |     big_axes[2].set_title("Translated Robust Regression (Pseudo-Huber Estimated)")
95 |     ax[2][0].plot(data["bg"],data["raw"], 'o')
96 |     ax[2][0].plot(data["bg"], data["H_theta"][0] + data["H_theta"][1]*data["bg"])
97 |     ax[2][1].plot(data["frame_n"],get_df_on_f0(data["raw"]-data["H_theta"][1]*data["bg"]))
98 |     
99 |     big_axes[3].set_title("Asymmetric L1 norm regression with Powell method")
100|     ax[3][0].plot(data["bg"],data["raw"], 'o')
101|     ax[3][0].plot(data["bg"], data["CR_theta"][0] + data["CR_theta"][1]*data["bg"])
102|     ax[3][1].plot(data["frame_n"],get_df_on_f0(data["raw"]-data["CR_theta"][1]*data["bg"]))
103| 
104|     big_axes[4].set_title("Asymmetric L2 norm regression with L-BFGS-B method")
105|     ax[4][0].plot(data["bg"],data["raw"], 'o')
106|     ax[4][0].plot(data["bg"], data["PR_theta"][0] + data["PR_theta"][1]*data["bg"], label = 'Robust Regression')
107|     ax[4][1].plot(data["frame_n"],get_df_on_f0(data["raw"]-data["PR_theta"][1]*data["bg"]))
108|     
109|     
110|     for axis in ax[0:2,:].flatten():
111|         axis.set_ylim((-0.5,None))
112|     fig.show()
113| 
114| def plot_data():
115|     data = make_data()
116|     data = proc_data(data)
117|     fig, ax = plt.subplots(nrows = 3, ncols = 2,figsize = (15, 9))
118| 
119|     ax[0][0].plot(data["frame_n"], data["raw"])
120|     ax[0][0].set_ylabel('Raw F')
121|     ax[0][0].set_xlabel('Frame Number')
122| 
123|     ax[0][1].plot(data["bg"], data["raw"], 'o')
124|     ax[0][1].plot(data["bg"], data["NRR_theta"][0] + data["NRR_theta"][1]*data["bg"], label = 'Naive Robust Regression')
125|     ax[0][1].plot(data["bg"], data["RR_theta"][0] + data["RR_theta"][1]*data["bg"], label = 'Robust Regression')
126|     ax[0][1].set_ylabel('Total Fluorescence')
127|     ax[0][1].set_xlabel('Neuropil Fluorescence')
128|     ax[0][1].legend(loc='upper left')
129| 
130|     ax[1][0].plot(data["frame_n"], data["ground_truth"])
131|     ax[1][0].set_ylabel("Ground Truth")
132|     
133| 
134|     ax[1][1].plot(data["frame_n"], data["NRR_df_on_f"])
135|     ax[1][1].set_ylabel("Naive Robust Reg df/f")
136| 
137|     ax[2][0].plot(data["frame_n"], data["RR_df_on_f"])
138|     ax[2][0].set_ylabel("Robust Regression df/f")
139| 
140|     ax[2][1].plot(data["ground_truth"], data["RR_df_on_f"], 'o')
141|     ax[2][1].plot(data["ground_truth"], data["ground_truth"], label= 'Unity')
142|     ax[2][1].set_title("MeanSquareError = " + str(calc_error(data)))
143|     ax[2][1].set_xlabel("Ground Truth")
144|     ax[2][1].set_ylabel("Robust Regression Aproach")
145|     ax[2][1].legend(loc='upper left')
146|     fig.show()
147| 
148| def compare_approaches(df = False):
149|     data = make_data()
150|     data = proc_data(data)
151|     fig, ax = plt.subplots(nrows = 4, ncols = 2,figsize = (15, 12),
152|                            tight_layout=True)
153|     
154|     ax[0][0].plot(data["frame_n"], data["gt_df_on_f"]if df else data["ground_truth"])
155|     ax[0][0].set_ylabel(f"Ground Truth {'dF/F0' if df else 'Fcell'}")
156| 
157|     ax[0][1].plot(data["bg"], data["raw"], 'o')
158|     ax[0][1].plot(data["bg"], data["RR_theta"][0] + data["RR_theta"][1]*data["bg"], label = 'Robust Regression')
159|     ax[0][1].plot(data["bg"], data["CR_theta"][0] + data["CR_theta"][1]*data["bg"], label = 'Biased L1 Regression (Powell Method)')
160|     ax[0][1].plot(data["bg"], data["PR_theta"][0] + data["PR_theta"][1]*data["bg"], label = 'Biased L2 Regression (BFGS Method)')
161|     ax[0][1].set_ylabel('Total Fluorescence')
162|     ax[0][1].set_xlabel('Neuropil Fluorescence')
163|     ax[0][1].legend(loc='upper left')
164| 
165| 
166|     ax[1][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
167|                   data["RR_df_on_f"] if df else data["RR_bg_subtracted"], 'o')
168|     ax[1][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
169|                   data["gt_df_on_f"]if df else data["ground_truth"], 
170|                   label= 'Unity')
171|     ax[1][1].set_title(f"MeanSquareError = {calc_error(data,df)[0]:.8f}")
172|     ax[1][1].set_xlabel("Ground Truth")
173|     ax[1][1].set_ylabel("Robust Regression Approach")
174|     ax[1][1].legend(loc='upper left')
175| 
176|     ax[1][0].plot(data["frame_n"], 
177|                   data["RR_df_on_f"] if df else data["RR_bg_subtracted"])
178|     ax[1][0].set_ylabel(f"Robust Regression {'dF/F0' if df else 'Fcell'}")
179| 
180|     ax[2][0].plot(data["frame_n"], 
181|                   data["CR_df_on_f"] if df else data["CR_bg_subtracted"])
182|     ax[2][0].set_ylabel(f"L1-Heaviside Regression {'dF/F0' if df else 'Fcell'}")
183| 
184|     ax[2][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
185|                   data["CR_df_on_f"] if df else data["CR_bg_subtracted"], 'o')
186|     ax[2][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
187|                   data["gt_df_on_f"]if df else data["ground_truth"], 
188|                   label= 'Unity')
189|     ax[2][1].set_title(f"MeanSquareError = {calc_error(data,df)[1]:.8f}")
190|     ax[2][1].set_xlabel("Ground Truth")
191|     ax[2][1].set_ylabel("Custom Regression Approach 1")
192|     ax[2][1].legend(loc='upper left')
193| 
194|     ax[3][0].plot(data["frame_n"], 
195|                   data["PR_df_on_f"] if df else data["PR_bg_subtracted"])
196|     ax[3][0].set_ylabel(f"Biased L2 vs L2k norm Regression {'dF/F0' if df else 'Fcell'}")
197| 
198|     ax[3][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
199|                   data["PR_df_on_f"] if df else data["PR_bg_subtracted"], 'o')
200|     ax[3][1].plot(data["gt_df_on_f"]if df else data["ground_truth"], 
201|                   data["gt_df_on_f"]if df else data["ground_truth"], 
202|                   label= 'Unity')
203|     ax[3][1].set_title(f"MeanSquareError = {calc_error(data,df)[2]:.8f}")
204|     ax[3][1].set_xlabel("Ground Truth")
205|     ax[3][1].set_ylabel("Custom Regression Approach 2")
206|     ax[3][1].legend(loc='upper left')
207|     fig.show()
208| 
209| 
210| def time_approaches():
211|     N = np.linspace(80,100000,10).astype(int)
212|     robust_regression_times = np.zeros(N.shape)
213|     powell_times = np.zeros(N.shape)
214|     L2_norm_times = np.zeros(N.shape)
215|     for idx,n_points in enumerate(N):
216|         print(idx)
217|         t1=np.zeros(10); t2=np.zeros(10); t3=np.zeros(10); t4=np.zeros(10)
218|         for i in range(1):
219|             data = make_data(n_points)
220|             t1[i] = time()
221|             robust_regression(data["bg"], data["raw"])
222|             t2[i] = time()
223|             custom_regression(data["bg"], data["raw"])
224|             t3[i] = time()
225|             parabolic_regression(data["bg"], data["raw"])
226|             t4[i] = time()
227|         robust_regression_times[idx] = (t2-t1).mean()
228|         powell_times[idx] = (t3-t2).mean()
229|         L2_norm_times[idx] = (t4-t3).mean()
230|     fig,ax = plt.subplots(nrows = 3, sharex=True)
231|     ax[0].set_title("Robust Regression Time Complexity")
232|     ax[0].plot(N,robust_regression_times)
233|     ax[1].set_title("Powell Method Time Complexity")
234|     ax[1].plot(N,powell_times)
235|     ax[1].set_ylabel("Execution Time")
236|     ax[2].set_title("BFGS Method Time Complexity")
237|     ax[2].plot(N,L2_norm_times)
238|     ax[2].set_xlabel("Number of data points")
239|     fig.show()
240| 
241|         
242| 
243| def proc_data(data, do_all=True):
244|     data["gt_df_on_f"] = get_df_on_f0(data["ground_truth"])
245|     if do_all:
246|         data["NRR_theta"] = naive_robust_regression(data["bg"], data["raw"])
247|         data["NRR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["NRR_theta"])
248|         data["NRR_df_on_f"] = get_df_on_f0(data["NRR_bg_subtracted"])
249|     
250|     t1 = time()
251|     data["RR_theta"] = robust_regression(data["bg"], data["raw"])
252|     data["RR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["RR_theta"])
253|     data["RR_df_on_f"] = get_df_on_f0(data["RR_bg_subtracted"])
254|     data["RR_RMSE"] = np.sum((data["gt_df_on_f"] - data["RR_df_on_f"])**2)**0.5
255|     data["RR_time"] = time() - t1
256|     
257|     t2 = time()
258|     data["TH_theta"] = translated_huber_regression(data["bg"], data["raw"])
259|     data["TH_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["TH_theta"])
260|     data["TH_df_on_f"] = get_df_on_f0(data["TH_bg_subtracted"])
261|     data["TH_RMSE"] = np.sum((data["gt_df_on_f"] - data["TH_df_on_f"])**2)**0.5
262|     data["TH_time"] = time() - t2
263|     
264|     t3 = time()
265|     data["CR_theta"] = custom_regression(data["bg"], data["raw"])
266|     data["CR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["CR_theta"])
267|     data["CR_df_on_f"] = get_df_on_f0(data["CR_bg_subtracted"])
268|     data["CR_RMSE"] = np.sum((data["gt_df_on_f"] - data["CR_df_on_f"])**2)**0.5
269|     data["CR_time"] = time() - t3
270| 
271|     t4 = time()
272|     data["PR_theta"] = parabolic_regression(data["bg"], data["raw"])
273|     data["PR_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["PR_theta"])
274|     data["PR_df_on_f"] = get_df_on_f0(data["PR_bg_subtracted"])
275|     data["PR_RMSE"] = np.sum((data["gt_df_on_f"] - data["PR_df_on_f"])**2)**0.5
276|     data["PR_time"] = time() - t4
277| 
278|     t5 = time()
279|     data["H_theta"] = huber_regression(data["bg"], data["raw"])
280|     data["H_bg_subtracted"] = subtract_bg(data["raw"], data["bg"], data["H_theta"])
281|     data["H_df_on_f"] = get_df_on_f0(data["H_bg_subtracted"])
282|     data["H_RMSE"] = np.sum((data["gt_df_on_f"] - data["H_df_on_f"])**2)**0.5
283|     data["H_time"] = time() - t5
284|     return data
285| 
286| def calc_error(data,df):
287|     ground_truth = data["gt_df_on_f"]if df else data["ground_truth"]
288|     return (np.sum((ground_truth - (data["RR_df_on_f"] if df else data["RR_bg_subtracted"]))**2)/ground_truth.size,
289|             np.sum((ground_truth - (data["CR_df_on_f"] if df else data["CR_bg_subtracted"]))**2)/ground_truth.size,
290|             np.sum((ground_truth - (data["PR_df_on_f"] if df else data["PR_bg_subtracted"]))**2)/ground_truth.size)
291| 
292| def ols(x,y):
293|     slope, intercept, r_value, p_value, std_err = linregress(x,y)
294|     return np.array([intercept, slope])
295| 
296| def naive_robust_regression(x, y):
297|     y = y.reshape(-1, 1)
298|     X = np.vstack((np.ones(y.shape).transpose(), x.reshape(-1, 1).transpose()))
299|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y))
300| 
301|     return np.array([reg.coef_[0], reg.coef_[1]])
302| 
303| def robust_regression(x, y):
304|     y_reshape = y.reshape(-1, 1)
305|     X = np.vstack((np.ones(y_reshape.shape).transpose(), x.reshape(-1, 1).transpose()))
306|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y_reshape))
307| 
308|     # subtracted_data = subtract_bg(y, x, [reg.coef_[0], reg.coef_[1]])
309|     subtracted_data = y - reg.coef_[0] - reg.coef_[1]*x
310|     offset = np.min(subtracted_data)
311| 
312|     return np.array([reg.coef_[0]+offset, reg.coef_[1]])
313| 
314| def translated_huber_regression(x,y):
315|     y_reshape = y.reshape(-1, 1)
316|     X = np.vstack((np.ones(y_reshape.shape).transpose(), x.reshape(-1, 1).transpose()))
317|     reg = TheilSenRegressor(random_state=0).fit(X.transpose(), np.ravel(y_reshape))
318| 
319|     # subtracted_data = subtract_bg(y, x, [reg.coef_[0], reg.coef_[1]])
320|     subtracted_data = y - reg.coef_[0] - reg.coef_[1]*x
321|     offset = np.min(subtracted_data)
322|     return np.array([reg.coef_[0]+offset, reg.coef_[1]])
323| 
324| def huber_regression(x,y):
325|     return underline_regression(x,y,method = 'huber')
326| 
327| def custom_regression(x, y):
328|     return underline_regression(x,y, method = "ramp")
329| 
330| def parabolic_regression(x,y):
331|     return underline_regression(x,y,method="parabolic")
332| 
333| def subtract_bg(f, bg, theta):
334|     # return f - ( theta[0] + theta[1] * bg)
335|     return f - theta[1]*bg
336| 
337| def get_smoothed_running_minimum(timeseries, tau1 = 30, tau2 = 100):
338|     result = minimum_filter1d(uniform_filter1d(timeseries,tau1,mode='nearest'),
339|                             tau2,
340|                             mode = 'reflect')
341|     return result
342| 
343| def get_df_on_f0(F,F0=None):
344|     if not F0 is None:
345|         return (F - F0) / F0
346|     else:
347|         F0 = get_smoothed_running_minimum(F)
348|         return get_df_on_f0(F,F0)
349| 
350| def run_simulation():
351|     RR = []
352|     TH = []
353|     CR = []
354|     PR = []
355|     H = []
356|     RR_times = []
357|     TH_times = []
358|     CR_times = []
359|     PR_times = []
360|     H_times = []
361|     for i in range(100):
362|         data = make_data()
363|         data = proc_data(data,do_all=False)
364|         RR.append(data["RR_RMSE"])
365|         TH.append(data["TH_RMSE"])
366|         CR.append(data["CR_RMSE"])
367|         PR.append(data["PR_RMSE"])
368|         H.append(data["H_RMSE"])
369|         RR_times.append(data["RR_time"])
370|         TH_times.append(data["TH_time"])
371|         CR_times.append(data["CR_time"])
372|         PR_times.append(data["PR_time"])
373|         H_times.append(data["H_time"])
374|     fig,ax = plt.subplots(nrows=5, ncols = 2,figsize = (20,12),constrained_layout=True)
375|     for (ax0,ax1),name,err,time in zip(ax, 
376|                               ("Translated Theil-Sen",
377|                                "Translated Huber",
378|                                 "Heaviside/L1 norm with Powell",
379|                                 "Asymmetric L2 norm with BFGS",
380|                                 "L2/PseudoHuber with BFGS"),
381|                               (RR,TH,CR,PR,H),
382|                               (RR_times,TH_times,CR_times,PR_times,H_times)):
383|         ax0.set_title(f"{name} Accuracy")
384|         ax0.set_ylabel("frequency")
385|         ax0.set_xlabel("RMSE (df/f units)")
386|         ax0.hist(err)
387|         ax1.set_title(f"{name} Timing")
388|         ax1.set_xlabel("Time taken to fit 10000 data points (s)")
389|         ax1.set_ylabel("frequency")
390|         ax1.hist(time)
391|     fig.show()
392|     
393|     return {'TheilSen':RR,
394|             'Huber':TH,
395|             'RampL1':CR,
396|             'AsymmetricL2':PR,
397|             'L2PseudoHuber':H,
398|             'TheilSen_times':RR_times,
399|             'Huber_times':TH_times,
400|             'RampL1_times':CR_times,
401|             'AsymmetricL2_times':PR_times,
402|             'L2PseudoHuber_times':H_times}
403| 
404| # plot_data_creation_process()
405| if __name__=="__main__":
406|     dic = run_simulation()
407|     import pandas as pd
408|     df = pd.DataFrame(dic)
409|     df.to_csv("C:/Users/Vivian Imbriotis/Desktop/bg_subtract_testing.csv")


\ProcessFluorescence\NovelRegressionDevelopment\vectorised_underline_regression.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Aug 31 18:11:20 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | 
8  | import numpy as np
9  | from scipy.optimize import minimize
10 | from scipy.stats import linregress
11 | 
12 | 
13 | 
14 | 
15 | def ramp(X):
16 |     return X*(X>0)
17 | 
18 | 
19 | class UnderlineRegressor:
20 |     default_k = 30
21 |     @classmethod
22 |     def loss(cls,params,x, y, k=None, v=False):
23 |         if k is None: k = cls.default_k
24 |         guess = params[0] + params[1] * x
25 |         residuals = y - guess #data points above line are positive
26 |         upper_cost = np.sum(ramp(residuals)**2)
27 |         lower_cost = np.sum(ramp(-residuals)**(2*k))
28 |         cost = upper_cost + lower_cost
29 |         return cost
30 |     @classmethod
31 |     def grad(cls,params,x, y,k=None,v=False):
32 |         if k is None: k = cls.default_k
33 |         guess = params[0] + params[1] * x
34 |         res = y - guess #data points above line are positive
35 |         outer_derivative = (res>0)*(2*res)+(res<0)*(2*k*res**(2*k-1))
36 |         theta_0 = np.sum(outer_derivative)*(-1)
37 |         theta_1 = np.sum(outer_derivative*(-x))
38 |         grad = (theta_0, theta_1)
39 |         return grad
40 |     @classmethod
41 |     def guess(cls,x, y):
42 |         slope, intercept, r_value, p_value, std_err = linregress(x,y)
43 |         return (intercept, slope)
44 |     @classmethod
45 |     def regress(cls,x, y):
46 |         start_params = cls.guess(x, y)
47 |         k = cls.default_k
48 |         with np.errstate(over='raise'):
49 |             while True:
50 |                 try:
51 |                     reg = minimize(cls.loss,
52 |                                x0 = start_params,
53 |                                jac = cls.grad,
54 |                                args = (x, y, k),
55 |                                bounds = ((None,None), (0, None)),
56 |                                method = "L-BFGS-B")
57 |                     k+=1
58 |                 except FloatingPointError:
59 |                     if reg: break
60 |                     else: k -= 1
61 |                     
62 |         intercept = reg.x[0] - ramp(np.max(reg.x[0]+x*reg.x[1] - y))
63 |         return (intercept, reg.x[1])
64 |     @classmethod
65 |     def regress_all(cls,X,Y):
66 |         res = []
67 |         for x,y in zip(X,Y):
68 |             res.append(cls.regress(x,y))
69 |         return np.array(res)


\ProcessLicking\kernel.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Jun 19 12:45:42 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | import pandas as pd
8  | import numpy as np
9  | 
10 | def main(path):
11 |     df = pd.read_csv(path)
12 |     df["lick_transformed"] = lick_transform(df.lick_during_frame)
13 |     df.to_csv(path)
14 |     return df
15 | 
16 | 
17 | 
18 | def get_times_since_lick(lick):
19 |     '''
20 |     Get times since a lick has last occured in frames
21 | 
22 |     Parameters
23 |     ----------
24 |     lick : [bool or int]
25 |         A list of bools of whether a lick occurs at the timepoints by which
26 |         the list is indexed.
27 | 
28 |     Returns
29 |     -------
30 |     [int]
31 |         Frames until a lick next occurs, prepended with 999.
32 | 
33 |     '''
34 |     if type(lick)!=np.ndarray:
35 |         lick = lick.to_numpy()
36 |     start_idx = np.nonzero(lick>0)[0][0]
37 |     source_array = lick[start_idx:]
38 |     prepend = (999)*np.ones(start_idx)
39 |     result = np.zeros(source_array.shape)
40 |     for timepoint, lick in enumerate(source_array):
41 |         if lick:
42 |             result[timepoint] = 0
43 |             counter = 1
44 |         else:
45 |             result[timepoint] = counter
46 |             counter += 1
47 |     return np.append(prepend, result)
48 |         
49 |         
50 | def get_times_until_lick(lick):
51 |     '''
52 |     Get times until a lick occurs in frames.
53 | 
54 |     Parameters
55 |     ----------
56 |     lick : [bool or int]
57 |         A list of bools of whether a lick occurs at the timepoints by which
58 |         the list is indexed.
59 | 
60 |     Returns
61 |     -------
62 |     [int]
63 |         Frames until a lick next occurs, appended with -999.
64 | 
65 |     '''
66 |     if type(lick)!=np.ndarray:
67 |         lick = lick.to_numpy()
68 |     end_idx = np.nonzero(lick>0)[0][-1]
69 |     source_array = lick[:end_idx+ 1]
70 |     append = (999)*np.ones(len(lick) - end_idx - 1)
71 |     result = np.zeros(source_array.shape)
72 |     for timepoint, lick in reversed(list(enumerate(source_array))):
73 |         if lick:
74 |             result[timepoint] = 0
75 |             counter = 1
76 |         else:
77 |             result[timepoint] = counter
78 |             counter += 1
79 |     return np.append(result,append)
80 | 
81 | 
82 | def collapse_lick_timing(time_since,time_to, cuttoff=10):
83 |     '''
84 |     Intended behavior: Return the smaller of time_since or
85 |     time_to at each time point, converting time_to to negative
86 |     numbers. Then if any value of the resultant array is outside
87 |     [-10,10], replace it with -999.
88 | 
89 |     Parameters
90 |     ----------
91 |     time_since : [int]
92 |         Array of times since a lick.
93 |     time_to : [int]
94 |         array of times until a lick.
95 | 
96 |     Returns
97 |     -------
98 |     timing_info : [[int]].
99 |     '''
100|     
101|     condition_list = [time_since<time_to, time_to<=time_since]
102|     timing_info = np.select(condition_list,
103|                             [-1*time_since,
104|                              time_to])
105|     timing_info = -1*timing_info
106|     timing_info[np.abs(timing_info)>cuttoff] = -999
107|     
108|     return timing_info
109| 
110| def lick_transform(lick, cuttoff=10):
111|     time_since_lick = get_times_since_lick(lick)
112|     time_until_lick  = get_times_until_lick(lick)
113|     timing_info = collapse_lick_timing(time_since_lick, 
114|                                        time_until_lick,
115|                                        cuttoff)
116|     return timing_info
117| 
118| 
119| if __name__=='__main__':
120|     df = main("C:/Users/Vivian Imbriotis/Desktop/first_1000_ROITrials.csv")


\ProcessLicking\neuron_response_to_lick_validation.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jul 20 16:10:24 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | import os
8  | 
9  | import numpy as np
10 | import matplotlib.pyplot as plt
11 | from scipy.stats import pearsonr
12 | 
13 | 
14 | from accdatatools.Utils.path import get_exp_path
15 | from accdatatools.Observations.recordings import Recording
16 | from accdatatools.Timing.synchronisation import (get_neural_frame_times,
17 |                                                  get_lick_state_by_frame)
18 | 
19 | 
20 | experiment_ID = "2016-10-07_03_CFEB027"
21 | experiment_path = get_exp_path(experiment_ID, "H:\\")
22 | 
23 | suite2p_path = os.path.join(experiment_path,
24 |                             "suite2p",
25 |                             "plane0")
26 | 
27 | 
28 | timeline_path  = os.path.join(experiment_path,
29 |                               "2016-10-07_03_CFEB027_Timeline.mat")
30 | 
31 | 
32 | exp_recording = Recording(suite2p_path)
33 | 
34 | 
35 | frame_times = get_neural_frame_times(
36 |     timeline_path,exp_recording.ops["nframes"])
37 | 
38 | licking = get_lick_state_by_frame(timeline_path, frame_times)
39 | 
40 | corrs = [pearsonr(x,licking)[0] for x in exp_recording.dF_on_F]
41 | corrs_isort = np.argsort(corrs)
42 | to_plot = exp_recording.dF_on_F[corrs_isort]
43 | fig,ax = plt.subplots()
44 | lick_frame = np.nonzero(licking)
45 | max_brightness = np.percentile(to_plot,99)
46 | ax.imshow(np.clip(to_plot[-20:],-0.2,max_brightness), origin = 'lower',
47 |           aspect = 5)
48 | ax.vlines(lick_frame, -15,-10)
49 | ax.set_xlim(0,1000)
50 | fig.show()
51 | 


\ProcessLicking\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\ProcessPupil\pupil_frame_synchronization.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jul 13 17:52:47 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | 
8  | from accdatatools.Utils.deeploadmat import loadmat
9  | from accdatatools.Utils.signal_processing import rising_or_falling_edges
10 | 
11 | import numpy as np
12 | import matplotlib.pyplot as plt
13 | 
14 | 
15 | 
16 | timeline_path = ("H:\\Local_Repository\\CFEB014\\2016-05-28_02_CFEB014\\"+
17 | "2016-05-28_02_CFEB014_Timeline.mat")
18 | 
19 | h5_path = ("C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/videos/"+
20 |            "2016-05-28_02_CFEB014_eyeDLC_resnet50_micepupilsJul9shuffle1_1030000.h5")
21 | 
22 | 
23 | 
24 | if __name__=="__main__":
25 |     plt.plot(get_frame_times(timeline_path,h5_path))


\ProcessPupil\pupil_trends_figure.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | """
3  | Created on Fri Jul 31 11:36:46 2020
4  | 
5  | @author: viviani
6  | """
7  | import matplotlib.pyplot as plt
8  | import matplotlib.cm as cm
9  | import matplotlib.colors as c
10 | import pandas as pd
11 | import numpy as np
12 | import seaborn
13 | from accdatatools.ToCSV.without_collapsing import RecordingUnroller
14 | 
15 | 
16 | seaborn.set_style("dark")
17 | 
18 | records = RecordingUnroller("H:/Local_Repository/CFEB033/2016-12-16_01_CFEB033",
19 |                        ignore_dprime = True,
20 |                        tolerate_lack_of_eye_video = False).to_unrolled_records()
21 | 
22 | df = pd.DataFrame(records)
23 | 
24 | 
25 | 
26 | 
27 | fig,ax = plt.subplots(ncols = 2, gridspec_kw={'width_ratios': [15, 26]},
28 |                       constrained_layout = True)
29 | num_trials = len(df.Trial_ID.unique())
30 | norm = c.Normalize(vmin=0,vmax = num_trials)
31 | cmap = cm.ScalarMappable(norm,'plasma')
32 | first_roi = df.ROI_ID[0]
33 | i=0
34 | x = np.arange(15,0,-1)
35 | pupils_per_timepoint = np.array([df[df.peritrial_factor==i][df.ROI_ID==first_roi].pupil_diameter.values for i in x])
36 | print(pupils_per_timepoint.shape)
37 | pupils_per_timepoint[pupils_per_timepoint=="NA"] = np.nan
38 | for idx,peritrial in enumerate(pupils_per_timepoint.transpose()):
39 |     print(f"\r{i:5} of {num_trials:5}",end=""); i+=1
40 |     peritrial = np.array(peritrial)
41 |     ax[0].plot((x)/5,peritrial,
42 |             color = cmap.to_rgba(idx,0.2))
43 | 
44 | ax[0].plot(x/5,np.nanmean(pupils_per_timepoint,axis=-1),color='black')
45 | ax[0].set_xlim((3,0.2))
46 | ax[0].set_ylim((8,25))
47 | ax[0].set_xticks([3,2,1])
48 | ax[0].set_ylabel("Pupil Diameter (pixels)")
49 | ax[0].set_xlabel("Seconds preceding stimulus onset")
50 | ax[0].set_title("Pretrial pupil behavior")
51 | 
52 | # x = np.arange(1,27)
53 | # pupils_per_timepoint = np.array(df[df.trial_factor!=-999][df.ROI_ID==first_roi].pupil_diameter.values)
54 | # pupils_per_timepoint = pupils_per_timepoint[:pupils_per_timepoint.shape[0] - pupils_per_timepoint.shape[0]%26]
55 | # trials = pupils_per_timepoint.reshape(26,-1)
56 | # print(trials.shape)
57 | # trials[trials=="NA"] = np.nan
58 | # for idx,trial in enumerate(trials.transpose()):
59 | #     ax[1].plot((x)/5,trial,
60 | #             color = cmap.to_rgba(idx,0.2))
61 | 
62 | 
63 | i=0
64 | x = np.arange(1,27)
65 | pupils_per_timepoint = [df[df.trial_factor==i][df.ROI_ID==first_roi].pupil_diameter.values for i in x]
66 | length = max(map(len, pupils_per_timepoint))
67 | pupils_per_timepoint= [list(ls)+[np.nan]*(length-len(ls)) for ls in pupils_per_timepoint]
68 | pupils_per_timepoint = np.array(pupils_per_timepoint)
69 | pupils_per_timepoint[pupils_per_timepoint=="NA"] = np.nan
70 | pupils_per_timepoint = pupils_per_timepoint.astype(float)
71 | for idx,trial in enumerate(pupils_per_timepoint.transpose()):
72 |     print(pupils_per_timepoint.shape)
73 |     print(f"\r{i:5} of {num_trials:5}",end=""); i+=1
74 |     trial = np.array(trial)
75 |     ax[1].plot((x)/5,trial,
76 |             color = cmap.to_rgba(idx,0.2))
77 | ax[1].plot(x/5,np.nanmean(pupils_per_timepoint,axis=-1),color='black')
78 | 
79 | ax[1].set_xlim((1,5))
80 | ax[1].set_ylim((8,25))
81 | ax[1].set_ylabel("Pupil Diameter (pixels)")
82 | ax[1].set_xlabel("Seconds since stimulus")
83 | ax[1].set_title("Within-trial pupil behavior")
84 | ax[1].legend()
85 | cmap._A = []
86 | cb = fig.colorbar(cmap,ax = ax, location="bottom", shrink = 0.4)
87 | cb.set_label("Trial Number")
88 | fig.show()


\ProcessPupil\pupil_trends_subtyped.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | """
3  | Created on Fri Jul 31 11:36:46 2020
4  | 
5  | @author: viviani
6  | """
7  | import matplotlib.pyplot as plt
8  | import matplotlib.cm as cm
9  | import matplotlib.colors as c
10 | import pandas as pd
11 | import numpy as np
12 | import seaborn
13 | from accdatatools.ToCSV.without_collapsing import RecordingUnroller, get_whole_dataset
14 | 
15 | def plot_trial_subset(axis,correct,go,df,cmap, normalize = False, render = True):
16 |     print(f"Initial shape of df in plot_trial_subset call is {df.shape}")
17 |     x = np.arange(1,27)
18 |     df = df[df.go==go]
19 |     df = df[df.correct==correct]
20 |     df["roi_num"] = np.fromiter(map(lambda s:s.split(" ")[-1],df.ROI_ID),int)
21 |     trial_ids = df[(df.trial_factor==1)&(df.roi_num==0)].Trial_ID.values
22 |     recordings= list(map(lambda s:s.split(" ")[0],trial_ids))
23 |     trial_ids = map(lambda s:s.split(" ")[-1],trial_ids)
24 |     trial_ids = map(int,trial_ids)
25 |     # pupils_per_timepoint = [df[df.trial_factor==i][df.roi_num==0].pupil_diameter.values for i in x]
26 |     # length = max(map(len, pupils_per_timepoint))
27 |     # pupils_per_timepoint= [list(ls)+[np.nan]*(length-len(ls)) for ls in pupils_per_timepoint]
28 |     # pupils_per_timepoint = np.array(pupils_per_timepoint)
29 |     pupils_per_timepoint = df[df.roi_num==0].pivot(index = "Trial_ID", 
30 |                                                    columns = "trial_factor", 
31 |                                                    values = "pupil_diameter"
32 |                                                    ).to_numpy()
33 |     pupils_per_timepoint[pupils_per_timepoint=="NA"] = np.nan
34 |     pupils_per_timepoint = pupils_per_timepoint.astype(float)
35 |     print(f"Shape of plottable array is {pupils_per_timepoint.shape}")
36 |     if normalize:
37 |         #means of first second
38 |         means = np.nanmean(pupils_per_timepoint[:,:5], axis = 0)
39 |         #divide each row by it's own mean!
40 |         pupils_per_timepoint = pupils_per_timepoint / means[:,None]
41 |     for idx,trial in zip(trial_ids,pupils_per_timepoint):
42 |         if render: axis.plot((x)/5,trial,
43 |                 color = cmap.to_rgba(idx,0.2))
44 |     axis.plot(x/5,np.nanmean(pupils_per_timepoint,axis=0),color='black',
45 |               label = 'Mean across trials')
46 |     axis.set_xlim((1,5))
47 |     axis.set_xlabel("time (s)")
48 |     axis.set_ylabel(
49 |         "size of pupil / initial size of pupil" if normalize else "pupil diameter (pixels)"
50 |         )
51 |     if not normalize: axis.set_ylim((0,30))
52 |     else: axis.set_ylim((0,3))
53 |     return (recordings, pupils_per_timepoint)
54 | 
55 | a = None
56 | def plot_trial_subset_with_range(axis,correct,go,df, normalize = False, 
57 |                                  range_type = "error", color= "k"):
58 |     if range_type not in ("error","deviation"):
59 |         raise ValueError("range_type must be one of 'error' or 'deviation'")
60 |     print(f"correct = {correct}; go={go}")
61 |     x = np.arange(0,25)
62 |     df = df[df.go==go]
63 |     df = df[df.correct==correct]
64 |     df["roi_num"] = np.fromiter(map(lambda s:s.split(" ")[-1],df.ROI_ID),int)
65 |     trial_ids = df[(df.trial_factor==1)&(df.roi_num==0)].Trial_ID.values
66 |     recordings= pd.Series(map(lambda s:s.split(" ")[0],trial_ids)).unique()
67 |     pupils_per_timepoint = df[df.roi_num==0][~pd.isnull(df.Trial_ID)].pivot(index = "Trial_ID", 
68 |                                                    columns = "trial_factor", 
69 |                                                    values = "pupil_diameter"
70 |                                                    ).to_numpy()
71 |     global a;
72 |     a = df[df.roi_num==0][~(df.peritrial_factor<0)]
73 |     pupils_per_peritrial = df[df.roi_num==0][~(df.peritrial_factor<0)].pivot(index = "number_of_trials_seen", 
74 |                                                    columns = "peritrial_factor", 
75 |                                                    values = "pupil_diameter"
76 |                                                    )
77 |     a = pupils_per_peritrial
78 |     print(pupils_per_timepoint.shape)
79 |     pupils_per_timepoint[pupils_per_timepoint=="NA"] = np.nan
80 |     pupils_per_timepoint = pupils_per_timepoint.astype(float)
81 |     if normalize:
82 |         means = np.nanmean(pupils_per_timepoint[:,:5], axis = -1)
83 |         #divide each row by it's own mean!
84 |         pupils_per_timepoint = pupils_per_timepoint / means[:,None]
85 |     mean = np.nanmean(pupils_per_timepoint[:,:25],axis=0)
86 |     rang = np.nanstd(pupils_per_timepoint[:,:25],axis=0)
87 |     if range_type=="error":
88 |         #Convert to standard error!
89 |         n_points = np.sum(~np.isnan(pupils_per_timepoint[:,:25]), axis = 0)
90 |         rang /= (n_points**0.5)
91 |     axis.plot(x/5,mean,color='black',
92 |               label = 'Mean across trials')
93 |     axis.fill_between(x/5,mean+rang,mean-rang,
94 |                   color = color,
95 |                   alpha = 0.3,
96 |                   label = f"Standard {range_type}")
97 |     if not normalize: axis.set_ylim((10,30)); axis.set_ylabel("Pupil diameter (pixels)")
98 |     else: axis.set_ylim((0.85,1.2)); axis.set_ylabel("Pupil diameter (normalized)")
99 |     axis.set_xlabel("Time since trial onset (s)")
100| 
101| 
102| def create_figure(df,title=None, render = True, normalize = False):
103|     print(f"Initial shape of dataframe in create_figure call is {df.shape}")
104|     seaborn.set_style("dark")
105|     #Clean up dataframe
106|     df = df[~pd.isnull(df.Trial_ID)]
107|     #Get an iterator over each peritrial period
108|     fig,ax = plt.subplots(ncols = 2, nrows = 2, constrained_layout = True,
109|                           figsize = (12,8))
110|     num_trials = 250
111|     norm = c.Normalize(vmin=0,vmax = num_trials)
112|     cmap = cm.ScalarMappable(norm,'plasma')
113|     
114|     (hit_axis, miss_axis), (fa_axis, cr_axis) = ax
115|     
116|     
117|     #HIT AXIS
118|     hits = plot_trial_subset(hit_axis, correct=True, go=True, df=df, cmap=cmap,
119|                              normalize = normalize, render = render)
120|     hit_axis.set_title("Hit Trials")
121|     hit_axis.legend()
122|     
123|     #MISS AXIS
124|     misses = plot_trial_subset(miss_axis, correct=False, go=True, df=df, cmap=cmap,
125|                                normalize = normalize, render = render)
126|     miss_axis.set_title("Miss Trials")
127|     
128|     #FALSE ALARM AXIS
129|     fas = plot_trial_subset(fa_axis, correct=False, go=False ,df=df, cmap=cmap,
130|                             normalize = normalize, render = render)
131|     fa_axis.set_title("False-Alarm Trials")
132|     
133|     #CORRECT REJECTION AXIS
134|     crs = plot_trial_subset(cr_axis, correct=True, go=False, df=df, cmap=cmap,
135|                             normalize = normalize, render = render)
136|     cr_axis.set_title("Correct-Rejection Trials")
137|     
138|     #Turn off inner axis ticks and labels
139|     hit_axis.set_xticks([])
140|     hit_axis.set_xlabel("")
141|     miss_axis.set_xticks([])    
142|     miss_axis.set_xlabel("")
143|     miss_axis.set_yticks([])
144|     miss_axis.set_ylabel("")
145|     cr_axis.set_yticks([])
146|     cr_axis.set_ylabel("")
147|     
148|     #ADD A COLORBAR
149|     cmap._A = []
150|     cb = fig.colorbar(cmap,ax = ax, location="bottom", shrink = 0.4)
151|     cb.set_label("Trial Number")
152|     if title:
153|         fig.suptitle(title)
154|     if render: fig.show()
155|     return (hits,misses,fas,crs)
156| 
157| 
158| def create_range_figure(df,title=None, normalize = False, 
159|                         range_type = "error"):
160|     seaborn.set_style("dark")
161|     df = df[~pd.isnull(df.Trial_ID)]
162|     fig,ax = plt.subplots(ncols = 2, nrows = 2, constrained_layout = True,
163|                           figsize = (12,8))
164|     (hit_axis, miss_axis), (fa_axis, cr_axis) = ax
165|     
166|     #HIT AXIS
167|     hits = plot_trial_subset_with_range(hit_axis, correct=True, go=True, df=df,
168|                              normalize = normalize, range_type = range_type, color="green")
169|     hit_axis.set_title("Hit Trials")
170|     hit_axis.legend()
171|     
172|     #MISS AXIS
173|     misses = plot_trial_subset_with_range(miss_axis, correct=False, go=True, df=df,
174|                                normalize = normalize,  range_type = range_type, color="palegreen")
175|     miss_axis.set_title("Miss Trials")
176|     
177|     #FALSE ALARM AXIS
178|     fas = plot_trial_subset_with_range(fa_axis, correct=False, go=False ,df=df,
179|                             normalize = normalize,  range_type = range_type, color="darksalmon")
180|     fa_axis.set_title("False-Alarm Trials")
181|     
182|     #CORRECT REJECTION AXIS
183|     crs = plot_trial_subset_with_range(cr_axis, correct=True, go=False, df=df,
184|                             normalize = normalize, range_type = range_type, color="red")
185|     cr_axis.set_title("Correct-Rejection Trials")
186|     
187|     
188|     #Turn off inner axis ticks and labels
189|     hit_axis.set_xticks([])
190|     hit_axis.set_xlabel("")
191|     miss_axis.set_xticks([])    
192|     miss_axis.set_xlabel("")
193|     miss_axis.set_yticks([])
194|     miss_axis.set_ylabel("")
195|     cr_axis.set_yticks([])
196|     cr_axis.set_ylabel("")
197|     
198|     if title:
199|         fig.suptitle(title)
200|     fig.show()
201|     return (hits,misses,fas,crs)
202| 
203| def perform_testing(df, render = False, normalize = False):
204|     hits,misses,fas,crs = create_figure(df,render=render, normalize = normalize)
205|     hits_df   = pd.DataFrame(data = hits[1])
206|     hits_df["recording"] = hits[0]
207|     # hits["trial_no"] = hits.index
208|     # hits = hits.melt(id_vars = "trial_no")
209|     # hits.columns = ['trial_no','trial_frame','pupil_diameter']
210|     # hits["trial_type"] = "hit"
211|     # return hits
212|     misses_df = pd.DataFrame(data=misses[1])
213|     misses_df["recording"] = misses[0]
214|     fas_df    = pd.DataFrame(data = fas[1])
215|     fas_df["recording"] = fas[0]
216|     crs_df    = pd.DataFrame(data=crs[1])
217|     crs_df["recording"] = crs[0]
218|     result = []
219|     for df,trial_type in zip((hits_df,misses_df,fas_df,crs_df),
220|                              ("hit","miss","fa","cr")):
221|         df["trial_no"] = df.index
222|         df = df.melt(id_vars = ["trial_no","recording"])
223|         df.columns = ['trial_no','recording','trial_frame','pupil_diameter']
224|         df["trial_type"] = trial_type
225|         result.append(df)
226|     df = pd.concat(result)
227|     return df
228| 
229| a = None
230| 
231| def plot_peritrial_subset(axis,correct,go,df,cmap, normalize = False, render = True):
232|     print(f"Initial shape of df in plot_trial_subset call is {df.shape}")
233|     x = np.arange(1,27)
234|     cond_list = [(df.trial_factor>0),(df.peritrial_factor>0)]
235|     df['extended_trial_factor'] = np.select(cond_list,(df.trial_factor,-1*df.peritrial_factor),default=np.nan)
236|     df["roi_num"] = np.fromiter(map(lambda s:s.split(" ")[-1],df.ROI_ID),int)
237|     
238|     
239|     trial_ids = df[(df.trial_factor==1)&(df.roi_num==0)].Trial_ID.values
240|     
241|     #We need to copy information about each trial to the pre-trial datapoints
242|     df2 = df.copy(deep=True)
243|     first_loop=True
244|     for trial in trial_ids:
245|         #Left rotate the trial mask 15 elements
246|         trial_idxs = (df.Trial_ID==trial)
247|         peritrial_idxs = trial_idxs.shift(periods = -15, fill_value=False)
248|         if first_loop:
249|             print(trial_idxs[30:50])
250|             print(peritrial_idxs[30:50])
251|             first_loop=False
252|         #Propogate trial information to pre-trial timepoints
253|         df2.loc[peritrial_idxs,'Trial_ID'] = df.loc[trial_idxs,'Trial_ID']
254|         df2.loc[peritrial_idxs,'go']       = df.loc[trial_idxs,'go']
255|         df2.loc[peritrial_idxs,'correct']  = df.loc[trial_idxs,'correct']
256|         
257|     global a
258|     a=df2
259|     df = df[df.go==go]
260|     df = df[df.correct==correct]
261|     recordings= list(map(lambda s:s.split(" ")[0],trial_ids))
262|     trial_ids = map(lambda s:s.split(" ")[-1],trial_ids)
263|     trial_ids = map(int,trial_ids)
264|     # pupils_per_timepoint = [df[df.trial_factor==i][df.roi_num==0].pupil_diameter.values for i in x]
265|     # length = max(map(len, pupils_per_timepoint))
266|     # pupils_per_timepoint= [list(ls)+[np.nan]*(length-len(ls)) for ls in pupils_per_timepoint]
267|     # pupils_per_timepoint = np.array(pupils_per_timepoint)
268|     pupils_per_timepoint = df[df.roi_num==0].pivot(index = "Trial_ID", 
269|                                                    columns = "extended_trial_factor", 
270|                                                    values = "pupil_diameter"
271|                                                    ).to_numpy()
272|     pupils_per_timepoint[pupils_per_timepoint=="NA"] = np.nan
273|     pupils_per_timepoint = pupils_per_timepoint.astype(float)
274| 
275|     print(f"Shape of plottable array is {pupils_per_timepoint.shape}")
276|     if normalize:
277|         #means of first second
278|         means = np.nanmean(pupils_per_timepoint[:,:5], axis = 0)
279|         #divide each row by it's own mean!
280|         pupils_per_timepoint = pupils_per_timepoint / means[:,None]
281|     for idx,trial in zip(trial_ids,pupils_per_timepoint):
282|         if render: axis.plot((x)/5,trial,
283|                 color = cmap.to_rgba(idx,0.2))
284|     axis.plot(x/5,np.nanmean(pupils_per_timepoint,axis=0),color='black',
285|               label = 'Mean across trials')
286|     axis.set_xlim((1,5))
287|     axis.set_xlabel("time (s)")
288|     axis.set_ylabel(
289|         "size of pupil / initial size of pupil" if normalize else "pupil diameter (pixels)"
290|         )
291|     if not normalize: axis.set_ylim((0,30))
292|     else: axis.set_ylim((0,3))
293|     return (recordings, pupils_per_timepoint)
294| 
295| 
296| def create_peritrial_figure(df,title=None, render = True, normalize = False):
297|     print(f"Initial shape of dataframe in create_figure call is {df.shape}")
298|     seaborn.set_style("dark")
299|     #Clean up dataframe
300|     df = df[~(pd.isnull(df.Trial_ID) & df.peritrial_factor<0)]
301|     #Get an iterator over each peritrial period
302|     fig,ax = plt.subplots(ncols = 2, nrows = 2, constrained_layout = True,
303|                           figsize = (12,8))
304|     num_trials = 250
305|     norm = c.Normalize(vmin=0,vmax = num_trials)
306|     cmap = cm.ScalarMappable(norm,'plasma')
307|     
308|     (hit_axis, miss_axis), (fa_axis, cr_axis) = ax
309|     
310|     
311|     #HIT AXIS
312|     hits = plot_peritrial_subset(hit_axis, correct=True, go=True, df=df, cmap=cmap,
313|                              normalize = normalize, render = render)
314|     hit_axis.set_title("Hit Trials")
315|     hit_axis.legend()
316|     
317|     #MISS AXIS
318|     misses = plot_peritrial_subset(miss_axis, correct=False, go=True, df=df, cmap=cmap,
319|                                normalize = normalize, render = render)
320|     miss_axis.set_title("Miss Trials")
321|     
322|     #FALSE ALARM AXIS
323|     fas = plot_peritrial_subset(fa_axis, correct=False, go=False ,df=df, cmap=cmap,
324|                             normalize = normalize, render = render)
325|     fa_axis.set_title("False-Alarm Trials")
326|     
327|     #CORRECT REJECTION AXIS
328|     crs = plot_peritrial_subset(cr_axis, correct=True, go=False, df=df, cmap=cmap,
329|                             normalize = normalize, render = render)
330|     cr_axis.set_title("Correct-Rejection Trials")
331|     
332|     #Turn off inner axis ticks and labels
333|     hit_axis.set_xticks([])
334|     hit_axis.set_xlabel("")
335|     miss_axis.set_xticks([])    
336|     miss_axis.set_xlabel("")
337|     miss_axis.set_yticks([])
338|     miss_axis.set_ylabel("")
339|     cr_axis.set_yticks([])
340|     cr_axis.set_ylabel("")
341|     
342|     #ADD A COLORBAR
343|     cmap._A = []
344|     cb = fig.colorbar(cmap,ax = ax, location="bottom", shrink = 0.4)
345|     cb.set_label("Trial Number")
346|     if title:
347|         fig.suptitle(title)
348|     if render: fig.show()
349|     return (hits,misses,fas,crs)
350| 
351| 
352| def perform_peritrial_testing(df, render = False, normalize = False):
353|     hits,misses,fas,crs = create_peritrial_figure(df,render=render, normalize = normalize)
354|     hits_df   = pd.DataFrame(data = hits[1])
355|     hits_df["recording"] = hits[0]
356|     # hits["trial_no"] = hits.index
357|     # hits = hits.melt(id_vars = "trial_no")
358|     # hits.columns = ['trial_no','trial_frame','pupil_diameter']
359|     # hits["trial_type"] = "hit"
360|     # return hits
361|     misses_df = pd.DataFrame(data=misses[1])
362|     misses_df["recording"] = misses[0]
363|     fas_df    = pd.DataFrame(data = fas[1])
364|     fas_df["recording"] = fas[0]
365|     crs_df    = pd.DataFrame(data=crs[1])
366|     crs_df["recording"] = crs[0]
367|     result = []
368|     for df,trial_type in zip((hits_df,misses_df,fas_df,crs_df),
369|                              ("hit","miss","fa","cr")):
370|         df["trial_no"] = df.index
371|         df = df.melt(id_vars = ["trial_no","recording"])
372|         df.columns = ['trial_no','recording','trial_frame','pupil_diameter']
373|         df["trial_type"] = trial_type
374|         result.append(df)
375|     df = pd.concat(result)
376|     return df
377|     
378| 
379| def create_heatmap_figure(df):
380|     print(f"Initial shape of dataframe in create_figure call is {df.shape}")
381|     seaborn.set_style("dark")
382|     
383|     #Clean up dataframe
384|     df = df[~pd.isnull(df.Trial_ID)]
385|     df["roi_num"] = np.fromiter(map(lambda s:s.split(" ")[-1],df.ROI_ID),int)
386|     df = df[df.roi_num==0]
387|     df[df=="NA"] = np.nan
388|     
389|     
390|     hits = df[(df.correct==1)&(df.go==1)].pivot(index = "Trial_ID", 
391|                                                columns = "trial_factor", 
392|                                                values = "pupil_diameter"
393|                                                ).dropna(how='all').to_numpy()
394|     misses = df[(df.correct==0)&(df.go==1)].pivot(index = "Trial_ID", 
395|                                                columns = "trial_factor", 
396|                                                values = "pupil_diameter"
397|                                                ).dropna(how='all').to_numpy()
398|     fas = df[(df.correct==0)&(df.go==0)].pivot(index = "Trial_ID", 
399|                                                columns = "trial_factor", 
400|                                                values = "pupil_diameter"
401|                                                ).dropna(how='all').to_numpy()
402|     crs = df[(df.correct==1)&(df.go==0)].pivot(index = "Trial_ID", 
403|                                                columns = "trial_factor", 
404|                                                values = "pupil_diameter"
405|                                                ).dropna(how='all').to_numpy()
406|     fig,ax = plt.subplots(ncols = 4, nrows = 1, constrained_layout = True,
407|                           figsize = (12,8))
408|     
409|     (hit_axis, miss_axis, fa_axis, cr_axis) = ax
410|     
411|     min_len = min(len(a) for a in (hits,misses,fas,crs))
412|     #HIT AXIS
413|     x0 = hit_axis.imshow(hits[:min_len])
414|     hit_axis.set_title("Hit")
415|     hit_axis.set_ylabel("Trial")
416|     
417|     #MISS AXIS
418|     x1 = miss_axis.imshow(misses[:min_len])
419|     miss_axis.set_title("Miss")
420|     
421|     #FALSE ALARM AXIS
422|     x2 = fa_axis.imshow(fas[:min_len])
423|     fa_axis.set_title("False-Alarm")
424|     
425|     #CORRECT REJECTION AXIS
426|     x3 = cr_axis.imshow(crs[:min_len])
427|     cr_axis.set_title("Correct-Rejection")
428|     
429|     #Turn off inner axis ticks and labels
430|     for axis in ax:
431|         axis.set_xticks([])
432|         if not axis is hit_axis:
433|             axis.set_yticks([])
434|     fig.text(0.5,0.04,"Time",ha="center")
435|     
436|     images = [x0,x1,x2,x3]
437|     # Find the min and max of all colors for use in setting the color scale.
438|     vmin = min(image.get_array().min() for image in [x0,x1,x2,x3])
439|     vmax = max(image.get_array().max() for image in [x0,x1,x2,x3])
440|     norm = c.Normalize(vmin=vmin, vmax=vmax)
441|     for im in images:
442|         im.set_norm(norm)
443|     fig.colorbar(images[0], ax=ax, orientation='vertical', fraction=.02)
444|     
445|     
446|     fig.show()
447| 
448| if __name__=="__main__":
449|     plt.close('all')
450|     # with np.errstate(all='raise'):
451|     #     df = pd.read_csv("C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv")
452|     #     create_range_figure(df,normalize=True,range_type="error")
453|     df = pd.read_csv("C:/Users/viviani/Desktop/single_experiments_for_testing/2016-11-01_03_CFEB027.csv")
454|     perform_peritrial_testing(df,render=True)
455|     
456| 


\ProcessPupil\size.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun Jul 12 14:26:31 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | 
8  | import numpy as np
9  | import matplotlib.pyplot as plt
10 | from matplotlib.patches import Ellipse
11 | import matplotlib.image as mpimg
12 | import pandas as pd
13 | import os
14 | 
15 | plt.ioff()
16 | 
17 | class FittedEyeShape:
18 |     @staticmethod
19 |     def calc_parabola_vertex(xy1, xy2, xy3):
20 |         '''
21 |         Find the coeffieients of a parabola that passes through three points.
22 | 		Adapted and modifed to get the unknowns for defining a parabola:
23 | 		http://stackoverflow.com/questions/717762/how-to-calculate-the-vertex-of-a-parabola-given-three-points
24 | 		'''
25 |         (x1,y1),(x2,y2),(x3,y3) = xy1,xy2,xy3
26 |         denom = (x1-x2) * (x1-x3) * (x2-x3);
27 |         A     = (x3 * (y2-y1) + x2 * (y1-y3) + x1 * (y3-y2)) / denom;
28 |         B     = (x3*x3 * (y1-y2) + x2*x2 * (y3-y1) + x1*x1 * (y2-y3)) / denom;
29 |         C     = (x2 * x3 * (x2-x3) * y1+x3 * x1 * (x3-x1) * y2+x1 * x2 * (x1-x2) * y3) / denom;
30 |         return A,B,C
31 |     
32 |     def __init__(self,xys,allow_invalid=False):
33 |         '''
34 |         An eye outline, based on parabolic curves.
35 |         
36 |         Parameters
37 |         ----------
38 |         xys : An arraylike of 4 points describing the lateral, superior,
39 |             medial, and inferiormost points of an eye in pixel coordinates.
40 |             Expects points to be passed in that order; lateral, superior, 
41 |             medial, inferior.
42 |         allow_invalid: bool
43 |           If True, suppress errors if the eyeshape is found to be invalid.
44 |           Instead, just set the self.valid attribue to False. Useful if
45 |           you want to plot the invalid eye shape!
46 |         
47 |         Attributes
48 |         -------
49 |         lateral:  lateral  (x,y) point
50 |         superior: superior (x,y) point
51 |         medial:   medial   (x,y) point
52 |         inferior: inferior (x,y) point
53 |         upper_parabola: function mapping x -> y for the upper half of
54 |             the eye shape, in pixel coordinates
55 |         lower_parabola: function mapping x -> y for the lower half of
56 |             the eye shape, in pixel coordinates
57 |         valid:  True unless allow_invalid and the eye shape is poor (ie would
58 |             otherwise raise an exception)
59 | 
60 |         '''
61 | 
62 |         xys = np.array(xys)
63 |         xys = xys.reshape(-1,2)
64 |         self.lateral,self.superior,self.medial,self.inferior = xys
65 | 
66 |         A,B,C = self.calc_parabola_vertex(self.lateral,self.superior,self.medial)
67 |         a,b,c = self.calc_parabola_vertex(self.lateral,self.inferior,self.medial)
68 |         self.upper_parabola = lambda x:A*x**2+B*x+C
69 |         self.lower_parabola = lambda x:a*x**2+b*x+c
70 |         
71 |         if allow_invalid:
72 |             try:
73 |                 self.throw_errors_if_absurd()
74 |                 self.valid=True
75 |             except Exception as e:
76 |                 self.valid=False
77 |         else:
78 |             self.throw_errors_if_absurd()
79 |             self.valid = True    
80 |     def throw_errors_if_absurd(self):
81 |         '''
82 |         Basic quality control considerations
83 | 
84 |         Raises
85 |         -------
86 |         ValueError: raised if points violate basic assumptions about eye shape
87 | 
88 |         '''
89 |         #The medial edge of the eye should be left of the lateral edge
90 |         if self.medial[0] > self.lateral[0]:
91 |             raise ValueError('Unacceptable Eye Shape: medial_x > lateral_x')
92 |         #The inferior edge of the eye may be above the superior edge in a 
93 |             #blink state, so we'll add a 150 pixel buffer
94 |         if self.superior[1] < self.inferior[1] - 150:
95 |             raise ValueError('Unacceptable Eye Shape: inferior_y > superior_y')
96 |         #The eye also shouldn't bee too large
97 |         if self.lateral[0] - self.medial[0] > 300:
98 |             raise ValueError('Unacceptable Eye Shape: mediolateral aspect > 300')
99 |         if self.superior[1] - self.inferior[1] > 300:
100|             raise ValueError('Unacceptable Eye Shape: superioinferior aspect>300')
101|         
102|         #Finally, the superior and inferior points should be between the medial
103|             #and lateral ones.
104|         for point in (self.lateral,self.medial):
105|             if point[0]<self.medial[0] or point[0]>self.lateral[0]:
106|                 raise ValueError("Unacceptable Eye Shape.")
107|         #Thinking now about the parabolas we generate...if they go outside
108|                 #the frame, something has probably gone wrong...
109|         x = np.linspace(self.medial[0],self.lateral[0],50)
110|         upper = self.upper_parabola(x)
111|         lower = self.lower_parabola(x)
112|         for parabola in (upper,lower):
113|             if np.any(parabola < 0) or np.any(parabola > 480):
114|                 raise ValueError("Unacceptable Eye shape: shape not in frame")
115|     def contains(self,points):
116|         '''
117|         For each point in points, checks if that point is contained in 
118|         the eye sihlouette.
119|         
120|         Parameters
121|         ----------
122|         points : Arraylike of float
123|             Passed as either a flattened array, ie [x0,y0,x1,x2]
124|             or as a Nx2 array, ie [[x1,y1],[x2,y2]]
125|             
126|         Returns
127|         -------
128|         contains : Array of Bools
129|             If points is flat, this is the same shape as points.
130|             If points is an Nx2 array, this is a flat N element array
131|             (ie one bool for each point in points)
132|             eg:
133|             >>eye.contains([1,2,3,4])     -->[True,True,False,False]
134|             >>eye.contains([[1,2],[3,4]]) -->[True,False]
135| 
136|         '''
137|         points = np.array(points)
138|         original_shape = points.shape[0]
139|         points = points.reshape(-1,2)
140|         contains = np.logical_and.reduce((
141|                (points[:,0]>self.medial[0]),
142|                (points[:,0]<self.lateral[0]),
143|                (self.upper_parabola(points[:,0])<points[:,1]),
144|                (self.lower_parabola(points[:,0])>points[:,1])))
145|         if contains.shape!=original_shape:
146|             contains = np.repeat(contains,2)
147|         return contains
148|     def plot(self,axis=None,color='red'):
149|         '''
150|         Plot the eye shape, or add the eyeshape to an existing plot
151|         
152|         Parameters
153|         ----------
154|         axis : TYPE, optional
155|             An existing axis to add this artist to or None. 
156|             The default is None.
157|         color : str, optional
158|             The matplotlib colorcode for the artist. The default is 'red'.
159|             
160|         Returns
161|         -------
162|         (Artist,Artist) pair
163|         '''
164|         x = np.linspace(self.medial[0],self.lateral[0],50)
165|         upper = self.upper_parabola(x)
166|         lower = self.lower_parabola(x)
167|         if axis==None:
168|             artist1, = plt.plot(x,upper,color=color)
169|             artist2, = plt.plot(x,lower,color=color)
170|             plt.show()
171|         else:
172|             artist1, = axis.plot(x,upper,color=color)
173|             artist2, = axis.plot(x,lower,color=color)
174|         return (artist1,artist2)
175|     def curves(self):
176|         x = np.linspace(self.medial[0],self.lateral[0],50)
177|         upper = self.upper_parabola(x)
178|         lower = self.lower_parabola(x)
179|         return (x,upper,lower)
180|     def __bool__(self):
181|         return self.valid
182| 
183| class FittedEllipse:
184|     min_points = 6
185|     def __init__(self,*args):
186|         '''
187|         A least-squares fitted ellipse.
188|         
189|         Parameters
190|         ----------
191|         *args : An arraylike of points XY (shape Nx2), 
192|                  or two arraylikes of ordinates, X and Y (shape N)
193|             The points against which to fit the ellipse.
194|             
195|         Raises
196|         ------
197|         np.linalg.LinAlgError
198|             Raised when there are an insufficient number of points
199|             to fit the ellipse, or when the resultant matrix is singular.
200|             
201|         Attributes
202|         -------
203|         centre_x: x ordinate of ellipse centre
204|         centree_y: y ordinate of ellipse centre
205|         centre:   (centre_x,centre_y)
206|         angle:   rotation of the ellipse from horizontal, in degrees
207|                    counterclockwise
208|         axes:    major and minor axes (order not guaranteed)
209|         area:    ellipse area
210|         points:  a series of points on the ellipse, for plotting
211|         '''
212|         if len(args)==1:
213|             xy = np.array(args[0])
214|             xy = xy.reshape(-1,2)
215|             x = xy[:,0:1]
216|             y = xy[:,1:]
217|         elif len(args)==2:
218|             x = args[0]
219|             y = args[1]
220|         #Require at least 6 points#
221|         if x.shape[0]<self.min_points:
222|             raise np.linalg.LinAlgError(f"Insufficient data for fitting ellipse, {self.min_points} required, received {x.shape[0]}")
223|         #The general form of a conic is Ax**2 + By**2 + Cx + Dy + E == 0
224|         D=np.hstack([x*x,x*y,y*y,x,y,np.ones(x.shape)])
225|         #But we can't just least-squares solve this matrix equation, because
226|         #we could end up with any conic! We need to constrain to the ellipse
227|         #case, ie we have a constrained minimization problem.
228| 
229|         #The algorithm to do this is from here: 
230|         #https://github.com/ndvanforeest/fit_ellipse/blob/master/fitEllipse.pdf
231|         #With additional adaptions from here: 
232|         #https://stackoverflow.com/a/48002645/12488760
233|         S=np.dot(D.T,D)
234|         C=np.zeros([6,6])
235|         C[0,2]=C[2,0]=2
236|         C[1,1]=-1
237|         E,V=np.linalg.eig(np.dot(np.linalg.inv(S),C))
238|         n=np.argmax(E)
239|         a=V[:,n]
240|         b,c,d,f,g,a=a[1]/2., a[2], a[3]/2., a[4]/2., a[5], a[0]
241|         num=b*b-a*c
242|         if num==0:
243|             raise np.linalg.LinAlgError("Insufficient data to fit an ellipse")
244|             
245|         #Now we know the general form coefficients, so we can convert them
246|         #to useful quantities
247|         self.centre_x = (c*d-b*f)/num
248|         self.centre_y = (a*f-b*d)/num
249|         self.centre = (self.centre_x,self.centre_y)
250|         self.angle = 0.5*np.arctan(2*b/(a-c))*180/np.pi #In degrees
251|         
252|         up = 2*(a*f*f+c*d*d+g*b*b-2*b*d*f-a*c*g)
253|         down1=(b*b-a*c)*( (c-a)*np.sqrt(1+4*b*b/((a-c)*(a-c)))-(c+a))
254|         down2=(b*b-a*c)*( (a-c)*np.sqrt(1+4*b*b/((a-c)*(a-c)))-(c+a))
255|         
256|         radius_horizontal=np.sqrt(abs(up/down1))
257|         radius_vertical  =np.sqrt(abs(up/down2))
258|         
259|         self.axes = (radius_horizontal,radius_vertical)
260|         self.area = np.pi*radius_horizontal*radius_vertical
261|         
262|         
263|         ell=Ellipse(self.centre,
264|                     radius_horizontal*2.,
265|                     radius_vertical*2.,
266|                     self.angle)
267|         self.points=ell.get_verts()
268|         self.sum_of_squares_error = self.get_mean_squared_error(xy)
269|     def plot(self, axis = None, color='blue'):
270|         '''
271|         Plot the ellipse , or add the ellipse to an existing plot
272|         
273|         Parameters
274|         ----------
275|         axis : TYPE, optional
276|             An existing axis to add this artist to or None. 
277|             The default is None.
278|         color : str, optional
279|             The matplotlib colorcode for the artist. The default is 'red'.
280|             
281|         Returns
282|         -------
283|         None.
284|         '''
285|         if axis==None:
286|             plt.plot(self.points[:,0],
287|                      self.points[:,1],
288|                      color= color,
289|                      label = "Fitted Ellipse")
290|         else:
291|             artist, = axis.plot(self.points[:,0],
292|                       self.points[:,1],
293|                       color= color,
294|                       label = "Fitted Ellipse")
295|             return artist
296|     def squared_error(self,pt):
297|         dist_2 = np.sum((pt - self.points)**2, axis=1)
298|         return np.min(dist_2)
299|     def get_mean_squared_error(self,pts):
300|         error = 0
301|         for point in pts:
302|             error += self.squared_error(point)
303|         return np.sqrt(error/pts.shape[0])
304|         
305|     def __repr__(self):
306|         return f"Fitted ellipse: area={self.area:.2f} angle={self.angle:.0f}"
307|     
308| 
309| def reject_outliers(group, stds=3):
310|     group = group.copy()
311|     group[np.abs(group - np.nanmean(group)) > stds * np.nanstd(group)] = np.nan
312|     return group
313| 
314| 
315| def unit_test_random():
316|     '''Randomly generate an ellipse, add points to it with
317|     some degree of error, and then attempt to fit an ellipse.
318|     '''
319|     N = 6
320|     DIM=2
321|     # Generate random points on the unit circle by sampling uniform angles
322|     theta = np.random.uniform(0, 2*np.pi, (N,1))
323|     eps_noise = 0.1 * np.random.normal(size=[N,1])
324|     circle = np.hstack([np.cos(theta), np.sin(theta)])
325| 
326|     # Stretch and rotate circle to an ellipse with random linear tranformation
327|     B = np.random.randint(-3, 3, (DIM, DIM))
328|     noisy_ellipse = circle.dot(B) + eps_noise
329|     X = noisy_ellipse[:,0:1]
330|     Y = noisy_ellipse[:,1:]
331|     plt.scatter(X, Y, label='Data Points')
332|     phi = np.linspace(0, 2*np.pi, 1000).reshape((1000,1))
333|     c = np.hstack([np.cos(phi), np.sin(phi)])
334|     ground_truth_ellipse = c.dot(B)
335|     plt.plot(ground_truth_ellipse[:,0], ground_truth_ellipse[:,1], 'k--', label='Generating Ellipse')
336|     fitted_ellipse = FittedEllipse(X,Y)
337|     try:
338|         fitted_ellipse.plot()
339|     except:
340|         pass
341|     plt.legend()
342|     print(fitted_ellipse)
343|  
344| def process_dataframe(df, csv= True):
345|     df.columns = [part+coord for part,coord in zip(df.loc[0],df.loc[1])]
346|     df = df.rename({"bodypartscoords":"path"},axis='columns')
347|     df = df.loc[2:]  
348|     return df
349|     
350| def labelcsv_as_dataframe(path):
351|     df = pd.read_csv(path)
352|     return process_dataframe(df)
353| 
354| def unit_test_data(path):
355|     '''
356|     Display a figure of a training frame, and attempt
357|     to fit an eye and pupil to the training data.
358| 
359|     Parameters
360|     ----------
361|     path : str
362|         Path to a DeepLabCut labeled_data csv.
363| 
364|     Returns
365|     -------
366|     None.
367| 
368|     '''
369|     folder,_ = os.path.split(path)
370|     df = labelcsv_as_dataframe(path)
371|     row = df.sample()
372|     row = row.dropna(axis=1)
373|     row = row.to_numpy().reshape(-1)
374|     path = row[0]
375|     _,file = os.path.split(path)
376|     abs_path = os.path.join(folder,file)
377|     xy = row[1:].reshape(-1,2).astype(float)
378|     pupil = xy[:-4]
379|     eye = xy[-4:]
380|     try:
381|         ellipse = FittedEllipse(pupil)
382|     except np.linalg.LinAlgError:
383|         ellipse = False
384|     fittedeye = FittedEyeShape(eye)
385|     img = mpimg.imread(abs_path)
386|     fig,ax = plt.subplots(ncols = 3, figsize = (10,2.3))
387|     #Plot of labelled points
388|     ax[0].imshow(img)
389|     ax[0].plot(eye[:,0], eye[:,1], 'o', color = "red")
390|     ax[0].plot(pupil[:,0], pupil[:,1], 'o', color = "blue")
391|     #Plot of ellipse on image
392|     ax[1].imshow(img)
393|     if ellipse:
394|         ellipse.plot(ax[1],color='blue')
395|     fittedeye.plot(ax[1])
396|     #plot of ellipse alone
397|     fittedeye.plot(ax[2])
398|     if ellipse:
399|         ellipse.plot(ax[2])
400|     ax[2].set_xlim(ax[1].get_xlim())
401|     ax[2].set_ylim(ax[1].get_ylim())
402|     fig.show()
403|  
404| def get_plot_of_extracted_eye(row, fig = None, ax = None, artists = None):
405|     '''
406|     Produce a nice plot of the eye position inferable from a list of points.
407| 
408|     Parameters
409|     ----------
410|     row : pandas.Series
411|         An iterable of interlaced x and y coordinates for the placement of 
412|         eye markers, as output by DeepLabCut.
413|     fig : Figure, optional
414|         A pre-existing figure. The default is None.
415|     ax : tuple, optional
416|         A prexisting (axes, axes) pair returned from a previous
417|         call to this function. The default is None.
418|     artists : iterable, optional
419|         A preexisting iterable of artists, returned from a previous
420|         call to this function. The default is None.
421| 
422|     Returns
423|     -------
424|     fig : Figure
425|     ax : (axes,axes) pair
426|     artists : iterable of artists
427| 
428|     '''
429|     row = row[np.arange(row.shape[0])%3!=2] #Drop likelihoods
430|     pupil = row[:-8]
431|     eye = row[-8:]
432|     fittedeye = FittedEyeShape(eye,allow_invalid=True)
433|     pupil_okay_idxs = fittedeye.contains(pupil)
434|     pupil_okay = pupil[pupil_okay_idxs]
435|     pupil_bad = pupil[~pupil_okay_idxs]
436|     try:
437|         ellipse = FittedEllipse(pupil_okay)
438|     except np.linalg.LinAlgError:
439|         ellipse = False
440|     if (not fig) and (not ax):
441|         fig,ax = plt.subplots(ncols = 2,figsize = (10,3.5))
442|         ax[0].set_title("Network prediction")
443|         ax[1].set_title("After quality control")
444|         ax[0].set_xlim(0,640)
445|         ax[0].set_ylim(480,0)
446|         ax[1].set_xlim(0,640)
447|         ax[1].set_ylim(480,0)
448|         eyeline1, eyeline2 = fittedeye.plot(ax[0])
449|         scatter1, = ax[0].plot(
450|             pupil_okay[::2],pupil_okay[1::2],'o',color='green')
451|         scatter2, = ax[0].plot(
452|             pupil_bad[::2],pupil_bad[1::2],'o',color='red')
453|         if fittedeye:
454|             eyeline3, eyeline4 = fittedeye.plot(ax[1])
455|         else:
456|             eyeline3, = ax[1].plot([],[])
457|             eyeline4, = ax[1].plot([],[])
458|         if fittedeye and ellipse:
459|             ellipseline = ellipse.plot(ax[1])
460|         else:
461|             ellipseline, = ax[1].plot([],[])
462|     else:
463|         (eyeline1,eyeline2,eyeline3,eyeline4,
464|                scatter1,scatter2, ellipseline) = artists
465|         x, upper, lower = fittedeye.curves()
466|         eyeline1.set_data(x, upper)
467|         eyeline2.set_data(x,lower)
468|         if fittedeye:
469|             eyeline3.set_data(x,upper)
470|             eyeline4.set_data(x,lower)
471|         else:
472|             eyeline3.set_data([],[])
473|             eyeline4.set_data([],[])
474|         scatter1.set_data(pupil_okay[::2],pupil_okay[1::2])
475|         scatter2.set_data(pupil_bad[::2],pupil_bad[1::2])
476|         if fittedeye and ellipse:
477|             ellipseline.set_data(ellipse.points[:,0],ellipse.points[:,1])
478|         else:
479|             ellipseline.set_data([],[])
480|         for artist in (eyeline1,eyeline2,scatter1,scatter2):
481|             ax[0].draw_artist(artist)
482|         for artist in (eyeline3, eyeline4,ellipseline):
483|             ax[1].draw_artist(artist)
484|         fig.canvas.update()
485|         fig.canvas.flush_events()
486|     artists = [eyeline1,eyeline2,eyeline3,eyeline4,
487|                scatter1,scatter2, ellipseline]
488|     return fig, ax, artists
489| 
490| def create_video(h5_path, file_path, n = None):
491|     '''
492|     Create a video of the extracted eye shapes from CNN output
493|     of DeepLabCut. Be aware that this could take quite a long time.
494| 
495|     Parameters
496|     ----------
497|     h5_path : str
498|         path to an h5 file created with deeplabcut.analyze_video.
499|     file_path : str
500|         path of the output video.
501|     n : int
502|         maximum number of frames to animate.
503| 
504|     Returns
505|     -------
506|     None.
507| 
508|     '''
509|     try: 
510|         os.mkdir(file_path)
511|     except:
512|         for file in os.listdir(file_path):
513|           os.remove(os.path.join(file_path,file))
514|           
515|     df = pd.read_hdf(h5_path)
516|     for idx, row in df.iterrows():
517|         if idx==0:
518|             fig,ax,artists = get_plot_of_extracted_eye(row)
519|         else:
520|             get_plot_of_extracted_eye(row,fig,ax,artists)
521|         target_path = os.path.join(file_path,f'{idx}.png')
522|         fig.savefig(target_path)
523|         if n!=None and n>0 and idx>=n:
524|             break
525|     os.system(
526|           "C:\\Users\\viviani\\ffmpeg\\bin\\.\\ffmpeg.exe -r 30 -f image2 -i"+
527|           f" {file_path}\\%d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p"+
528|           f" {file_path}.mp4")
529|     for file in os.listdir(file_path):
530|           os.remove(os.path.join(file_path,file))
531|     os.rmdir(file_path)
532|     plt.close('all')
533|         
534| def get_pupil_size_at_each_eyecam_frame(h5_path):
535|     df = pd.read_hdf(h5_path)
536|     results = np.empty(df.shape[0])
537|     for idx, row in df.iterrows():
538|         row = row[np.arange(row.shape[0])%3!=2] #Drop likelihoods
539|         pupil = row[:-8]
540|         eye = row[-8:]
541|         fittedeye = FittedEyeShape(eye,allow_invalid=True)
542|         pupil_okay_idxs = fittedeye.contains(pupil)
543|         pupil_okay = pupil[pupil_okay_idxs]
544|         try:
545|             ellipse = FittedEllipse(pupil_okay)
546|         except np.linalg.LinAlgError:
547|             ellipse = False
548|         if ellipse and fittedeye:
549|             results[idx] = min(ellipse.axes)
550|         else:
551|             results[idx] = np.nan
552|         if (idx%1000==0):
553|             print(f"{idx}/{df.shape[0]}")
554|     results = reject_outliers(results)
555|     return results
556|         
557| 
558| 
559| 
560| if __name__=="__main__":
561|     im = unit_test_data(
562|         "C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/labeled-data/2017-03-30_01_CFEB045_eye/CollectedData_viviani.csv")
563|     
564|     # res = get_pupil_size_over_time("C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/videos/"+
565|     #                                "2016-05-28_02_CFEB014_eyeDLC_resnet50_micepupilsJul9shuffle1_1030000.h5")
566|     # res_outlier_removed = reject_outliers(res)
567|     # plt.plot(np.array(list(range(len(res_outlier_removed))))/30,res_outlier_removed)
568|     # plt.show()


\ProcessPupil\size_pipeline_quality_control.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Mon Jul 20 11:27:18 2020
3  | 
4  | @author: viviani
5  | """
6  | import os
7  | from itertools import combinations
8  | 
9  | import numpy as np
10 | import pandas as pd
11 | import seaborn as sns
12 | import matplotlib.pyplot as plt
13 | import matplotlib.image as mpimg
14 | 
15 | 
16 | from accdatatools.ProcessPupil.size import FittedEyeShape, FittedEllipse
17 | from accdatatools.ProcessPupil.size import labelcsv_as_dataframe
18 | from scipy.spatial.distance import directed_hausdorff
19 | 
20 | sns.set_style("darkgrid")
21 | 
22 | class ModifiedFittedEllipse(FittedEllipse):
23 |     def __init__(self, n, pupil):
24 |         self.min_points = n
25 |         super().__init__(pupil)
26 | 
27 | 
28 | def hausdorff(ellipse1, ellipse2):
29 |     '''
30 |     Calculate the Hausdorff Distance between the two ellipses.
31 |     Parameters
32 |     ----------
33 |     ellipse1 : FittedEllipse Object
34 |  
35 |     ellipse2 : FittedEllipse Object
36 | 
37 |     Returns
38 |     -------
39 |     distance : int
40 | 
41 |     '''
42 |     distance = directed_hausdorff(ellipse1.points, ellipse2.points)
43 |     return distance
44 | 
45 | 
46 | 
47 | class SingleFrameFigure:
48 |     def __init__(self,path):
49 |         '''
50 |         Display a figure of a training frame, and attempt
51 |         to fit an eye and pupil to the training data.
52 |     
53 |         Parameters
54 |         ----------
55 |         path : str
56 |             Path to a DeepLabCut labeled_data csv.
57 |     
58 |         Returns
59 |         -------
60 |         None.
61 |     
62 |         '''
63 |         folder,_ = os.path.split(path)
64 |         df = labelcsv_as_dataframe(path)
65 |         row = df.sample()
66 |         row = row.dropna(axis=1)
67 |         row = row.to_numpy().reshape(-1)
68 |         path = row[0]
69 |         _,file = os.path.split(path)
70 |         abs_path = os.path.join(folder,file)
71 |         xy = row[1:].reshape(-1,2).astype(float)
72 |         pupil = xy[:-4]
73 |         eye = xy[-4:]
74 |         self.fig,ax_rows = plt.subplots(ncols = 3, nrows = 4, figsize = (8,7),
75 |                                    constrained_layout=True)
76 |         for n_points_to_consider, ax in zip(range(8,4,-1),ax_rows):
77 |             ellipses = []
78 |             for pupil_mod in combinations(pupil,n_points_to_consider):
79 |                 try:
80 |                     ellipses.append(ModifiedFittedEllipse(n_points_to_consider,pupil_mod))
81 |                 except np.linalg.LinAlgError:
82 |                     ellipses.append(False)
83 |             fittedeye = FittedEyeShape(eye)
84 |             img = mpimg.imread(abs_path)
85 |             ax[0].set_ylabel(f"N={n_points_to_consider}",
86 |                              rotation='horizontal',
87 |                              labelpad = 10)
88 |             #Plot of labelled points
89 |             ax[0].imshow(img)
90 |             ax[0].plot(eye[:,0], eye[:,1], 'o', color = "red")
91 |             ax[0].plot(pupil[:,0], pupil[:,1], 'o', color = "blue")
92 |             #Plot of ellipse on image
93 |             ax[1].imshow(img)
94 |             for ellipse in ellipses:
95 |                 if ellipse:
96 |                     ellipse.plot(ax[1],color='blue')
97 |             fittedeye.plot(ax[1])
98 |             #plot of ellipse alone
99 |             fittedeye.plot(ax[2])
100|             for ellipse in ellipses:
101|                 if ellipse:
102|                     ellipse.plot(ax[2])
103|             ax[2].set_xlim(ax[1].get_xlim())
104|             ax[2].set_ylim(ax[1].get_ylim())
105|             for axis in ax:
106|                 axis.set_xticks([])
107|                 axis.set_yticks([])
108|         ax_rows[0][0].set_title("DeepLabCut-labelled Points")
109|         ax_rows[0][1].set_title("Pupil extraction using N points")
110|         ax_rows[0][2].set_title("Algorithm's prediction")
111|     def show(self):
112|         self.fig.show()
113|     
114| 
115| # def least_squares_trend_for_one_frame(row, plotting = True):
116| #     if row.isnull().values.any():
117| #         raise ValueError("Row must have all 8 eye points placed!")
118| #     row = row.to_numpy().reshape(-1)
119| #     path = row[0]
120| #     _,file = os.path.split(path)
121| #     xy = row[1:].reshape(-1,2).astype(float)
122| #     pupil = xy[:-4]
123| #     eye = xy[-4:]
124| #     error_arrays = []
125| #     for n_points_to_consider in range(8,4,-1):
126| #         ellipses = []
127| #         for pupil_mod in combinations(pupil,n_points_to_consider):
128| #             try:
129| #                 ellipses.append(
130| #                     ModifiedFittedEllipse(n_points_to_consider,pupil_mod)
131| #                     )
132| #             except:
133| #                 pass
134| #         errors = np.array([(n_points_to_consider,x.get_mean_squared_error(pupil)) for x in ellipses])
135| #         error_arrays.append(errors)
136| #     errors = np.concatenate(error_arrays)
137| #     if plotting:
138| #         fig,ax = plt.subplots()
139| #         violin = ax.violinplot([i[:,1] for i in error_arrays],[8,7,6,5],showmeans=True)
140| #         violin["bodies"][0].set_label("Probability Density")
141| #         violin["cmeans"].set_color("black")
142| #         violin["cmeans"].set_label("Mean")
143| #         violin["cmins"].set_label("Minima and maxima")
144| #         ax.set_xticks([8,7,6,5])
145| #         ax.set_xlim((8.5,4.5));
146| #         ax.set_xlabel("Points considered in fitting ellipse")
147| #         ax.set_ylabel("Root Mean Squared Error from full set of 8 points (pixels)")
148| #         ax.legend(loc = 'upper left')
149| #         ax.set_ylim((0,ax.get_ylim()[1]))
150| #         fig.show()
151| #     return errors
152| 
153| # def least_squares_figure(path):
154| #     folder,_ = os.path.split(path)
155| #     df = labelcsv_as_dataframe(path)
156| #     errors = []
157| #     for idx,row in df.iterrows():
158| #         try:
159| #             errors.append(least_squares_trend_for_one_frame(row, False))
160| #         except: pass
161| #     errors = np.concatenate(errors) #get as a flat 2d np array
162| #     error_arrays = [errors[errors[:,0]==n][:,1] for n in range(8,4,-1)]
163| #     fig,ax = plt.subplots()
164| #     violin = ax.violinplot(error_arrays,[8,7,6,5],showmeans=True)
165| #     violin["bodies"][0].set_label("Probability Density")
166| #     violin["cmeans"].set_color("black")
167| #     violin["cmeans"].set_label("Mean")
168| #     violin["cmins"].set_label("Minima and maxima")
169| #     #ax.plot(distances[:,0],distances[:,1],'o', markersize=4, label = 'Ellipse from random permutation of points');
170| #     # ax.plot([8,7,6,5],[np.mean(i) for i in distance_arrays],
171| #     #          label='Mean',
172| #     #          color = 'black')
173| #     ax.set_xticks([8,7,6,5])
174| #     ax.set_xlim((8.5,4.5));
175| #     ax.set_xlabel("Points considered in fitting ellipse")
176| #     ax.set_ylabel("Root Mean Square Error from all 8 points (pixels)")
177| #     ax.legend(loc='upper left')
178| #     fig.show()
179| #     return errors
180| 
181| # def hausdorff_trend_for_one_frame(row, plotting = True):
182| #     if row.isnull().values.any():
183| #         raise ValueError("Row must have all 8 eye points placed!")
184| #     row = row.to_numpy().reshape(-1)
185| #     path = row[0]
186| #     _,file = os.path.split(path)
187| #     xy = row[1:].reshape(-1,2).astype(float)
188| #     pupil = xy[:-4]
189| #     eye = xy[-4:]
190| #     master_ellipse = FittedEllipse(pupil)
191| #     distance_arrays = []
192| #     for n_points_to_consider in range(8,4,-1):
193| #         ellipses = []
194| #         for pupil_mod in combinations(pupil,n_points_to_consider):
195| #             try:
196| #                 ellipses.append(
197| #                     ModifiedFittedEllipse(n_points_to_consider,pupil_mod)
198| #                     )
199| #             except:
200| #                 pass
201| #         distances = np.array([(n_points_to_consider,hausdorff(master_ellipse, x)[0]) for x in ellipses])
202| #         distance_arrays.append(distances)
203| #     distances = np.concatenate(distance_arrays)
204| #     if plotting:
205| #         plt.violinplot([i[:,1] for i in distance_arrays],[8,7,6,5],showmeans=True)
206| #         plt.plot(distances[:,0],distances[:,1],'o', markersize=4, label = 'Single ellipse from random combination of points');
207| #         plt.plot([8,7,6,5],[np.mean(i[:,1]) for i in distance_arrays],
208| #                  label='Mean',
209| #                  color = 'black')
210| #         plt.xticks([8,7,6,5])
211| #         plt.xlim((8.5,4.5));
212| #         plt.xlabel("Points considered in fitting ellipse")
213| #         plt.ylabel("Hausdorff Distance to 8-point fitted ellipse")
214| #         plt.legend()
215| #         plt.show()
216| #     return distances
217| 
218| # def hausdorff_trend_figure(path):
219| #     folder,_ = os.path.split(path)
220| #     df = labelcsv_as_dataframe(path)
221| #     distances = []
222| #     for idx,row in df.iterrows():
223| #         try:
224| #             distances.append(hausdorff_trend_for_one_frame(row, False))
225| #         except: pass
226| #     distances = np.concatenate(distances) #get as a flat 2d np array
227| #     distance_arrays = [distances[distances[:,0]==n][:,1] for n in range(8,4,-1)]
228| #     fig,ax = plt.subplots()
229| #     violin = ax.violinplot(distance_arrays,[8,7,6,5],showmeans=True)
230| #     violin["bodies"][0].set_label("Probability Density")
231| #     violin["cmeans"].set_color("black")
232| #     violin["cmeans"].set_label("Mean")
233| #     violin["cmins"].set_label("Minima and maxima")
234| #     #ax.plot(distances[:,0],distances[:,1],'o', markersize=4, label = 'Ellipse from random permutation of points');
235| #     # ax.plot([8,7,6,5],[np.mean(i) for i in distance_arrays],
236| #     #          label='Mean',
237| #     #          color = 'black')
238| #     ax.set_xticks([8,7,6,5])
239| #     ax.set_xlim((8.5,4.5));
240| #     ax.set_xlabel("Points considered in fitting ellipse")
241| #     ax.set_ylabel("Hausdorff Distance to 8-point fitted ellipse")
242| #     ax.legend(loc='upper left')
243| #     fig.show()
244| #     return distances
245|     
246| class LeastSquaresAndHausdorffEllipseFigure:
247|     def __init__(self,path):
248|         self.fig, ax = plt.subplots(figsize=(8,5),ncols = 2)
249|         self.draw_lst_squs_plot(ax[0],path)
250|         self.draw_hausdorff_plot(ax[1],path)
251|     def draw_lst_squs_plot(self,ax,path):
252|         folder,_ = os.path.split(path)
253|         df = labelcsv_as_dataframe(path)
254|         errors = []
255|         for idx,row in df.iterrows():
256|             try:
257|                 errors.append(least_squares_trend_for_one_frame(row, False))
258|             except: pass
259|         errors = np.concatenate(errors) #get as a flat 2d np array
260|         error_arrays = [errors[errors[:,0]==n][:,1] for n in range(8,4,-1)]
261|         violin = ax.violinplot(error_arrays,[8,7,6,5],showmeans=True)
262|         violin["bodies"][0].set_label("Probability Density")
263|         violin["cmeans"].set_color("black")
264|         violin["cmeans"].set_label("Mean")
265|         violin["cmins"].set_label("Minima and maxima")
266|         ax.set_xticks([8,7,6,5])
267|         ax.set_xlim((8.5,4.5));
268|         ax.set_xlabel("Points considered in fitting ellipse")
269|         ax.set_ylabel("Root Mean Square Error from all 8 points (pixels)")
270|         ax.legend(loc='upper left')
271|     def draw_hausdorff_plot(self,ax,path):
272|         folder,_ = os.path.split(path)
273|         df = labelcsv_as_dataframe(path)
274|         distances = []
275|         for idx,row in df.iterrows():
276|             try:
277|                 distances.append(hausdorff_trend_for_one_frame(row, False))
278|             except: pass
279|         distances = np.concatenate(distances) #get as a flat 2d np array
280|         distance_arrays = [distances[distances[:,0]==n][:,1] for n in range(8,4,-1)]
281|         violin = ax.violinplot(distance_arrays,[8,7,6,5],showmeans=True)
282|         violin["bodies"][0].set_label("Probability Density")
283|         violin["cmeans"].set_color("black")
284|         violin["cmeans"].set_label("Mean")
285|         violin["cmins"].set_label("Minima and maxima")
286|         ax.set_xticks([8,7,6,5])
287|         ax.set_xlim((8.5,4.5));
288|         ax.set_xlabel("Points considered in fitting ellipse")
289|         ax.set_ylabel("Hausdorff Distance to 8-point fitted ellipse")
290|         ax.legend(loc='upper left')
291|         
292|     @staticmethod
293|     def least_squares_trend_for_one_frame(row, plotting = True):
294|         if row.isnull().values.any():
295|             raise ValueError("Row must have all 8 eye points placed!")
296|         row = row.to_numpy().reshape(-1)
297|         path = row[0]
298|         _,file = os.path.split(path)
299|         xy = row[1:].reshape(-1,2).astype(float)
300|         pupil = xy[:-4]
301|         eye = xy[-4:]
302|         error_arrays = []
303|         for n_points_to_consider in range(8,4,-1):
304|             ellipses = []
305|             for pupil_mod in combinations(pupil,n_points_to_consider):
306|                 try:
307|                     ellipses.append(
308|                         ModifiedFittedEllipse(n_points_to_consider,pupil_mod)
309|                         )
310|                 except:
311|                     pass
312|             errors = np.array([(n_points_to_consider,x.get_mean_squared_error(pupil)) for x in ellipses])
313|             error_arrays.append(errors)
314|         errors = np.concatenate(error_arrays)
315|         if plotting:
316|             fig,ax = plt.subplots()
317|             violin = ax.violinplot([i[:,1] for i in error_arrays],[8,7,6,5],showmeans=True)
318|             violin["bodies"][0].set_label("Probability Density")
319|             violin["cmeans"].set_color("black")
320|             violin["cmeans"].set_label("Mean")
321|             violin["cmins"].set_label("Minima and maxima")
322|             ax.set_xticks([8,7,6,5])
323|             ax.set_xlim((8.5,4.5));
324|             ax.set_xlabel("Points considered in fitting ellipse")
325|             ax.set_ylabel("Root Mean Squared Error from full set of 8 points (pixels)")
326|             ax.legend(loc = 'upper left')
327|             ax.set_ylim((0,ax.get_ylim()[1]))
328|             fig.show()
329|         return errors
330|     @staticmethod
331|     def hausdorff_trend_for_one_frame(row, plotting = True):
332|         if row.isnull().values.any():
333|             raise ValueError("Row must have all 8 eye points placed!")
334|         row = row.to_numpy().reshape(-1)
335|         path = row[0]
336|         _,file = os.path.split(path)
337|         xy = row[1:].reshape(-1,2).astype(float)
338|         pupil = xy[:-4]
339|         eye = xy[-4:]
340|         master_ellipse = FittedEllipse(pupil)
341|         distance_arrays = []
342|         for n_points_to_consider in range(8,4,-1):
343|             ellipses = []
344|             for pupil_mod in combinations(pupil,n_points_to_consider):
345|                 try:
346|                     ellipses.append(
347|                         ModifiedFittedEllipse(n_points_to_consider,pupil_mod)
348|                         )
349|                 except:
350|                     pass
351|             distances = np.array([(n_points_to_consider,hausdorff(master_ellipse, x)[0]) for x in ellipses])
352|             distance_arrays.append(distances)
353|         distances = np.concatenate(distance_arrays)
354|         if plotting:
355|             plt.violinplot([i[:,1] for i in distance_arrays],[8,7,6,5],showmeans=True)
356|             plt.plot(distances[:,0],distances[:,1],'o', markersize=4, label = 'Single ellipse from random combination of points');
357|             plt.plot([8,7,6,5],[np.mean(i[:,1]) for i in distance_arrays],
358|                      label='Mean',
359|                      color = 'black')
360|             plt.xticks([8,7,6,5])
361|             plt.xlim((8.5,4.5));
362|             plt.xlabel("Points considered in fitting ellipse")
363|             plt.ylabel("Hausdorff Distance to 8-point fitted ellipse")
364|             plt.legend()
365|             plt.show()
366|         return distances
367|     def show(self):
368|         self.fig.show()
369| 
370|  
371| if __name__=="__main__":
372|     im = hausdorff_trend_figure(
373|         "C:/Users/viviani/Desktop/micepupils-viviani-2020-07-09/labeled-data/2017-03-30_01_CFEB045_eye/CollectedData_viviani.csv",
374|         )
375|     


\ProcessPupil\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\ResultsVisualisation\pie_chart.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Sep 15 11:10:20 2020
3  | 
4  | @author: viviani
5  | """
6  | from random import random
7  | import pandas as pd
8  | import numpy as np
9  | import matplotlib.pyplot as plt
10 | import matplotlib.patches as mpatches
11 | from matplotlib.gridspec import GridSpec
12 | from matplotlib.lines import Line2D
13 | from mpl_toolkits.mplot3d import Axes3D
14 | from sklearn.decomposition import PCA
15 | import seaborn as sns
16 | from matplotlib import animation
17 | sns.set_style("darkgrid")
18 | 
19 | 
20 | readable_titles = {
21 |     "ANOVA trial.segment fvalue" : "During Trial",
22 |     "ANOVA trial.segment:correct fvalue" : "Correct/Incorrect Response",
23 |     "ANOVA trial.segment:go fvalue" : "Go/No-Go Stimulus",
24 |     "ANOVA trial.segment:side fvalue" : "Left/Right Stimulus",
25 |     "ANOVA trial.segment:correct:contrast fvalue" : "High/Low Contrast",
26 |     "ANOVA trial.segment pvalue" : "During Trial",
27 |     "ANOVA trial.segment:correct pvalue" : "Correct/Incorrect Response",
28 |     "ANOVA trial.segment:go pvalue" : "Go/No-Go Stimulus",
29 |     "ANOVA trial.segment:side pvalue" : "Left/Right Stimulus",
30 |     "ANOVA trial.segment:correct:contrast pvalue" : "High/Low Contrast",
31 |     "ANOVA trial.segment partial_eta2" : "During Trial",
32 |     "ANOVA trial.segment:correct partial_eta2" : "Correct/Incorrect Response",
33 |     "ANOVA trial.segment:go partial_eta2" : "Go/No-Go Stimulus",
34 |     "ANOVA trial.segment:side partial_eta2" : "Left/Right Stimulus",
35 |     "ANOVA trial.segment:correct:contrast partial_eta2" : "High/Low Contrast"
36 |     }
37 | 
38 | def count_unique_index(df, by):                                                                                                                                                 
39 |     return df.groupby(by).size().reset_index().rename(columns={0:'count'})
40 | 
41 | 
42 | class CollapsedModelPieChartAnovaFigure:
43 |     colors = sns.color_palette()
44 |     def __init__(self,df,dataset='left_only',statistic='f'):
45 |         pvalue_cols = [c for c in df.columns if 'pvalue' in c and 'ANOVA' in c]
46 |         fvalue_cols = [c for c in df.columns if 'fvalue' in c]
47 |         eta_cols    = [c for c in df.columns if 'partial_eta2' in c]
48 |         pvals = df[pvalue_cols]
49 |         fvals = df[fvalue_cols]
50 |         evals = df[eta_cols]
51 |         if statistic.lower() in ('f','f value','fvalue'):
52 |             mode = 'f'
53 |         elif statistic.lower() in ('e','eta','eta2','eta squared'):
54 |             mode = 'eta2'
55 |         else:
56 |             raise ValueError(f"statistic muse be 'f' or 'eta2', not {statistic}")
57 |         
58 |         counter = count_unique_index((pvals<0.05),pvalue_cols)
59 |         counter['percent']=counter['count']/counter['count'].sum()
60 |         if dataset=='left_only':     names = np.array(('Trials','Correct','Go'))
61 |         elif dataset=='both_sides':  names = np.array(('Trials','Correct','Go',
62 |                                                     'Side'))
63 |         elif dataset=='low_contrast':names  = np.array(('Trials','Correct','Go',
64 |                                                     'Side','Contrast'))
65 |         
66 |         names = [names[boolrow[:-2].to_numpy().astype('bool')] for _,boolrow in counter.iterrows()]
67 |         names = np.array(['&'.join(ls) if list(ls) else "None" for ls in names])
68 |         self.fig = plt.figure(figsize = [12,5], tight_layout=True)
69 |         gs = GridSpec(len(fvals.columns),2,figure=self.fig)
70 |         right_ax = []
71 |         left_ax = self.fig.add_subplot(gs[:,0])
72 |         if mode=='f':
73 |             for i,c in enumerate(fvals.columns):
74 |                 right_ax.append(
75 |                     self.fig.add_subplot(gs[i,1])
76 |                     )
77 |                 right_ax[i].hist(fvals[c][pvals[c.replace('fvalue','pvalue')]<0.05],
78 |                              label = readable_titles[c],
79 |                              color = self.colors[i])
80 |                 right_ax[i].legend()
81 |         elif mode=='eta2':
82 |             for i,c in enumerate(evals.columns):
83 |                 right_ax.append(
84 |                     self.fig.add_subplot(gs[i,1])
85 |                     )
86 |                 right_ax[i].hist(evals[c][pvals[c.replace('partial_eta2','pvalue')]<0.05],
87 |                              label = readable_titles[c],
88 |                              color = self.colors[i])
89 |                 right_ax[i].legend()
90 |         
91 | 
92 |         right_ax[2].set_ylabel("Frequency")
93 |         right_ax[-1].set_xlabel("F value" if mode=='f' else "Partial $\eta^2$")
94 |         
95 |         wedges,text1,text2 = left_ax.pie(counter['count'],#labels=names,
96 |                                 autopct='%1.f%%',counterclock=True,
97 |                                 startangle=60, pctdistance=0.9)
98 |         
99 | 
100|         kw = dict(arrowprops=dict(arrowstyle="-",color='black'),
101|                   zorder=5, va="center")
102|         
103|         annotations = []
104|         for i, (p,c) in enumerate(zip(wedges,counter['count'])):
105|             ang = (p.theta2 - p.theta1)/2. + p.theta1
106|             y = np.sin(np.deg2rad(ang))
107|             x = np.cos(np.deg2rad(ang))
108|             text_xy = (x*1.2,y*1.2)
109|             if dataset=='left_only':
110|                 if names[i]=="Trials&Go":
111|                     text_xy = (text_xy[0]+0.1,text_xy[1]-0.1)
112|                 elif names[i]=="Trials&Correct":
113|                     text_xy = (text_xy[0]-0.2,text_xy[1]+0.05)
114|             horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
115|             annotations.append(
116|                 left_ax.annotate(names[i], xy=(x, y), 
117|                                  xytext= text_xy,
118|                                  annotation_clip=False,
119|                                  horizontalalignment=horizontalalignment, 
120|                                  **kw)
121|                 )
122|         left_ax.set_title("Significance of predictors on ROI Fluorescence")
123|         if mode=='f':
124|             right_ax[0].set_title("F values of predictors (when significant)")
125|         elif mode=='eta2':
126|             right_ax[0].set_title("Partial Eta Squared values of predictors (when significant)")
127|         new_xlim = (0,None)
128|         for a in right_ax: a.set_xlim(new_xlim); a.legend(loc='upper right')
129|     def show(self):
130|         self.fig.show()
131| 
132| class CollapsedModelCoefficientEstimatesFigure:
133|     def __init__(self,df):
134|         colnames = ("Tone Bin","Stimulus Bin","Response Bin")
135|         coefs = df[[c for c in df1.columns if ('coefficient' in c and 
136|                                                 'estimate' in c and
137|                                                 'lick' not in c)]]
138|         intercept = coefs["coefficient X.Intercept. estimate"].to_numpy()
139|         intercept = np.stack([intercept]*3).transpose()
140|         main_effect = coefs.iloc[:,0:3]
141|         main_effect.columns = colnames
142|         main_effect.loc[:,"Tone Bin"] = 0
143|         correct = coefs[[c for c in coefs.columns if 'correct1' in c]]
144|         go      =  coefs[[c for c in coefs.columns if 'go1' in c]]
145|         correct.columns = colnames
146|         go.columns      = colnames
147|         self.fig, ax = plt.subplots(ncols = 2, nrows = 2)
148|         ax[0][0].set_ylabel("Estimated effect ($\Delta$F/F0 units)")
149|         ax[0][0].set_title("Main Effect of Trial")
150|         ax[0][0].plot(main_effect.transpose(), color = 'k',
151|                       alpha=0.05)
152|         ax[0][1].set_ylabel("Estimated effect ($\Delta$F/F0 units)")
153|         ax[0][1].set_title("Effect of Go/No-go")
154|         ax[0][1].plot(go.transpose(),color='k',alpha=0.05)
155|         
156|         ax[1][0].set_ylabel("Estimated effect ($\Delta$F/F0 units)")
157|         ax[1][0].set_title("Effect of Correct/Incorrect")
158|         ax[1][0].plot(correct.transpose(),color='k',alpha=0.05)
159|         
160|         ax[1][1].set_title('Overall Mean Estimates')
161|         ax[1][1].set_ylabel("Prediction ($\Delta$F/F0 units)")
162|         hits = (intercept + main_effect + correct + go).mean()
163|         misses = (intercept + main_effect + go).mean()
164|         crs = (intercept + main_effect + correct).mean()
165|         fas = (intercept+main_effect).mean()
166|         ax[1][1].plot(hits,color='green',
167|                       alpha = 1, label = 'Hits')
168|         ax[1][1].plot(misses,color='palegreen',
169|                       alpha = 1, label = 'Misses')
170|         ax[1][1].plot(crs,color='red',
171|                       alpha = 1, label = 'Correct Rejections')
172|         ax[1][1].plot(fas,color='darksalmon',
173|                       alpha = 1, label = 'False Alarms')
174|         ax[1][1].legend()
175|     def show(self):
176|         self.fig.show()
177| 
178| a = None
179| class LickingModelFigure:
180|     def __init__(self,df):
181|         coefs = df[[c for c in df.columns if ('coefficient' in c and 
182|                                                 'pvalue' in c and
183|                                                 'lick' in c)]]
184|         
185|         intercept = coefs["lick.coefficient X.Intercept. pvalue"].to_numpy()
186|         kernels = coefs.iloc[:,1:]
187|         self.fig1, ax = plt.subplots(ncols = 2, figsize = [8,6],
188|                                     tight_layout=True)
189| 
190|         artist = ax[0].plot(kernels.transpose(),color='k',alpha = 0.05)
191|         artist[0].set_label('Value for a single ROI')
192|         ax[0].legend()
193|         ax[0].set_xlabel("$\Delta$t around a lick")
194|         ax[0].set_ylabel("Coefficient Value")
195|         ax[0].set_xticks([0,11,21])
196|         ax[0].set_xticklabels([-2,0,2])
197|         ax[1].set_ylabel('Coefficient Value')
198|         ax[1].set_xlabel("$\Delta$t around a lick")
199|         ax[1].set_xticks([0,11,21])
200|         ax[1].set_xticklabels([-2,0,2])
201|         ax[1].plot(kernels.mean().transpose(),color='k',
202|                    label = "mean across axons")
203|         ax[1].legend()
204|         # self.fig2, pca_ax = plt.subplots(ncols=3)
205|         pca = PCA(n_components=2)
206|         kernels_in_pca_coords = pca.fit_transform(kernels.to_numpy())
207|         # pc1, pc2 = pca.components_
208|         # pca_ax[2].plot(kernels_in_pca_coords[:,0],kernels_in_pca_coords[:,1],'o')
209|         # pca_ax[2].set_ylabel("Second Pricipal Component")
210|         # pca_ax[2].set_xlabel("First Principal Component")
211|         # pca_ax[0].plot(pc1)
212|         # pca_ax[0].set_title("First Principal Component")
213|         # pca_ax[1].plot(pc2)
214|         # pca_ax[1].set_title("Second Principal Component")
215|         pca = PCA(n_components = 3)
216|         kernels_in_pcs = pca.fit_transform(kernels.to_numpy())
217|         pc1,pc2,pc3 = pca.components_
218|         self.fig3, pca_ax = plt.subplots(ncols=3)
219|         pca_ax[0].plot(pc1)
220|         pca_ax[0].set_title("First Principal Component")
221|         pca_ax[1].plot(pc2)
222|         pca_ax[1].set_title("Second Principal Component")
223|         pca_ax[2].set_title("Third Principal Component")
224|         pca_ax[2].plot(pc3)
225|         self.fig4 = plt.figure()
226|         self.ax3d = self.fig4.add_subplot(111, projection='3d')
227|         self.ax3d.scatter(kernels_in_pcs[:,0],kernels_in_pcs[:,1],kernels_in_pcs[:,2],
228|                           s = 1)
229|         self.ax3d.set_xlabel("Component 1")
230|         self.ax3d.set_ylabel("Component 2")
231|         self.ax3d.set_zlabel("Component 3")
232|         
233| 
234|     def rotate(self,angle):
235|          self.ax3d.view_init(azim=angle)
236|     def save(self,name):
237|         angle = 3
238|         ani = animation.FuncAnimation(self.fig4, self.rotate, frames=np.arange(0, 360, angle), interval=50, repeat = True)
239|         ani.save(f'{name}.gif', writer=animation.PillowWriter(fps=20))
240|     def show(self):
241|         self.fig1.show()
242|         self.fig3.show()
243|         self.fig4.show()
244| 
245| def print_anova_stats(df):
246|     anova_pvals = df[[c for c in df.columns if 'ANOVA' in c and 'pvalue' in c]]
247|     print("NUMBER OF SIGNIFICANT ROIS")
248|     print(f"total = {len(df)}")
249|     print((anova_pvals<0.05).sum())
250|     print("\nPERCENTAGE SIGNIFICANT ROIS")
251|     print(100*(anova_pvals<0.05).sum()/len(anova_pvals))
252|     
253| def read_in_data():
254|     df1 = pd.read_csv("../RScripts/results_left_only.csv")
255|     df2 = pd.read_csv("../RScripts/results_binocular.csv")
256|     df3 = pd.read_csv("../RScripts/results_low_contrast.csv")
257|     return(df1,df2,df3)
258| 
259| 
260| 
261|         
262| def print_all_findings(df1,df2,df3):
263|     for i,n in zip((df1,df2,df3),('Monocular','Binocular','LowCon')):
264|         print(n)
265|         print_anova_stats(i)
266|         print("\n\n")
267| 
268| if __name__=="__main__":
269|     plt.close('all')
270|     df1,df2,df3 = read_in_data()
271|     # print_all_findings(df1,df2,df3)
272|     # CollapsedModelPieChartAnovaFigure(df1,'left_only','eta').show()
273|     # CollapsedModelPieChartAnovaFigure(df2,'both_sides','eta').show()
274|     # CollapsedModelPieChartAnovaFigure(df3,'low_contrast','eta').show()
275|     # CollapsedModelCoefficientEstimatesFigure(df1).show()
276|     # CollapsedModelCoefficientEstimatesFigure(df2).show()
277|     # CollapsedModelCoefficientEstimatesFigure(df3).show()  
278|     plt.ioff()
279|     # fig = LickingModelFigure(df1)
280|     # fig.save("high_contrast_licking_pca")
281|     while True:
282|         try:
283|             LickingModelFigure(df1).save("unilat_highcon_licking")
284|             LickingModelFigure(df2).save("bilat_highcon_licking_pca")
285|             LickingModelFigure(df3).save("lowcon_licking_pca")
286|             break
287|         except ValueError:
288|             pass
289|         
290|     


\RScripts\.Rhistory
0  | go:trial_factor + correct:trial_factor,
1  | data = roidat)
2  | dat$correct <- as.factor(dat$correct)
3  | dat$go <- as.factor(dat$go)
4  | rois <- unique(dat$ROI_ID)
5  | licking_model_pvalues = numeric(length(rois))
6  | licking_significant = 0
7  | licking_insignificant = 0
8  | summary_objects = vector(mode = "list", length= length(rois))
9  | roi <- rois[[6]]
10 | roidat <- dat[dat$ROI_ID==roi,]
11 | outside_trials  <- roidat[roidat$trial_factor== -999,]
12 | licking.model <- lm(dF_on_F ~ lick_factor,
13 | data = outside_trials)
14 | if(get_lm_pvalue(licking.model)>0.05){
15 | licking_insignificant = licking_insignificant + 1
16 | licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
17 | }
18 | licking.prediction <- predict(licking.model, newdata = roidat)
19 | roidat$residuals <- roidat$dF_on_F - licking.prediction
20 | residual.dat <- roidat
21 | residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
22 | collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
23 | lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
24 | +trial.segment:go + trial.segment:go:correct,
25 | data = collapsed.after.licking.subtraction)
26 | full.model <- lm(dF_on_F ~ lick_factor + trial_factor +
27 | go:trial_factor + correct:trial_factor,
28 | data = roidat)
29 | lmtest::dwtest(full.model)
30 | lmtest::dwtest(lm.with.licking.subtraction)
31 | full.mode..
32 | full.model
33 | summary(full.model)
34 | dat$correct <- as.factor(dat$correct)
35 | dat$go <- as.factor(dat$go)
36 | dat$trial_factor <- as.factor(dat$trial_factor)
37 | dat$lick_factor <- as.factor(dat$lick_factor)
38 | rois <- unique(dat$ROI_ID)
39 | licking_model_pvalues = numeric(length(rois))
40 | licking_significant = 0
41 | licking_insignificant = 0
42 | summary_objects = vector(mode = "list", length= length(rois))
43 | roi <- rois[[6]]
44 | roidat <- dat[dat$ROI_ID==roi,]
45 | outside_trials  <- roidat[roidat$trial_factor== -999,]
46 | licking.model <- lm(dF_on_F ~ lick_factor,
47 | data = outside_trials)
48 | if(get_lm_pvalue(licking.model)>0.05){
49 | licking_insignificant = licking_insignificant + 1
50 | licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
51 | }
52 | licking.prediction <- predict(licking.model, newdata = roidat)
53 | roidat$residuals <- roidat$dF_on_F - licking.prediction
54 | residual.dat <- roidat
55 | residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
56 | collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
57 | lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
58 | +trial.segment:go + trial.segment:go:correct,
59 | data = collapsed.after.licking.subtraction)
60 | full.model <- lm(dF_on_F ~ lick_factor + trial_factor +
61 | go:trial_factor + correct:trial_factor,
62 | data = roidat)
63 | lmtest::dwtest(full.model)
64 | lmtest::dwtest(lm.with.licking.subtraction)
65 | source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/compare_modelling_approaches.R')
66 | roi <- rois[[9]]
67 | roidat <- dat[dat$ROI_ID==roi,]
68 | outside_trials  <- roidat[roidat$trial_factor== -999,]
69 | licking.model <- lm(dF_on_F ~ lick_factor,
70 | data = outside_trials)
71 | if(get_lm_pvalue(licking.model)>0.05){
72 | licking_insignificant = licking_insignificant + 1
73 | licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
74 | }
75 | licking.prediction <- predict(licking.model, newdata = roidat)
76 | roidat$residuals <- roidat$dF_on_F - licking.prediction
77 | residual.dat <- roidat
78 | residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
79 | collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
80 | lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
81 | +trial.segment:go + trial.segment:go:correct,
82 | data = collapsed.after.licking.subtraction)
83 | full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
84 | go:as.factor(trial_factor) + correct:as.factor(trial_factor),
85 | data = roidat)
86 | lmtest::dwtest(full.model)
87 | lmtest::dwtest(lm.with.licking.subtraction)
88 | source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/compare_modelling_approaches.R')
89 | lmtest::dwtest(full.model,tol = 1e-10)
90 | lmtest::dwtest(full.model,tol = 1e-20)
91 | lmtest::dwtest(full.model,tol = 0)
92 | full.model
93 | summary(full.model)
94 | full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
95 | go:as.factor(trial_factor) + correct:as.factor(trial_factor),
96 | data = roidat[roidat$trial_factor!=-999])
97 | full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
98 | go:as.factor(trial_factor) + correct:as.factor(trial_factor),
99 | data = roidat[roidat$trial_factor!=-999,])
100| summary(full.model)
101| lmtest::dwtest(full.model,tol = 0)
102| lmtest::dwtest(lm.with.licking.subtraction)
103| a<- lmtest::dwtest(full.model,tol = 0)
104| b<- lmtest::dwtest(lm.with.licking.subtraction)
105| a$statistic
106| n <- length(rois)
107| full.model.pvals <- numeric(n)
108| full.model.stats <- numeric(n)
109| full.model.rsqds <- numeric(n)
110| collapsed.model.pvals <- numeric(n)
111| collapsed.model.stats <- numeric(n)
112| collapsed.model.rsqds <- numeric(n)
113| for (i in 1:n){
114| roi <- rois[[i]]
115| roidat <- dat[dat$ROI_ID==roi,]
116| outside_trials  <- roidat[roidat$trial_factor== -999,]
117| licking.model <- lm(dF_on_F ~ lick_factor,
118| data = outside_trials)
119| if(get_lm_pvalue(licking.model)>0.05){
120| licking_insignificant = licking_insignificant + 1
121| licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
122| }
123| licking.prediction <- predict(licking.model, newdata = roidat)
124| roidat$residuals <- roidat$dF_on_F - licking.prediction
125| residual.dat <- roidat
126| residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
127| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
128| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
129| +trial.segment:go + trial.segment:go:correct,
130| data = collapsed.after.licking.subtraction)
131| full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
132| go:as.factor(trial_factor) + correct:as.factor(trial_factor),
133| data = roidat[roidat$trial_factor!=-999,])
134| full.model.test <- lmtest::dwtest(full.model,tol = 0)
135| full.model.pvals[i] <- full.model.test$p.value
136| full.model.stats[i] <- full.model.test$statistic
137| full.model.rsqds[i] <- summary(full.model)$adj.r.squared
138| collapsed.model.test<- lmtest::dwtest(lm.with.licking.subtraction)
139| collapsed.model.pvals[i] <- full.model.test$p.value
140| collapsed.model.stats[i] <- full.model.test$statistic
141| collapsed.model.rsqds[i] <- summary(collapsed.model)$adj.r.squared
142| }
143| for (i in 1:n){
144| roi <- rois[[i]]
145| roidat <- dat[dat$ROI_ID==roi,]
146| outside_trials  <- roidat[roidat$trial_factor== -999,]
147| licking.model <- lm(dF_on_F ~ lick_factor,
148| data = outside_trials)
149| if(get_lm_pvalue(licking.model)>0.05){
150| licking_insignificant = licking_insignificant + 1
151| licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
152| }
153| licking.prediction <- predict(licking.model, newdata = roidat)
154| roidat$residuals <- roidat$dF_on_F - licking.prediction
155| residual.dat <- roidat
156| residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
157| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
158| collapsed.model<- lm(mean.dF ~ trial.segment + trial.segment:correct
159| +trial.segment:go + trial.segment:go:correct,
160| data = collapsed.after.licking.subtraction)
161| full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
162| go:as.factor(trial_factor) + correct:as.factor(trial_factor),
163| data = roidat[roidat$trial_factor!=-999,])
164| full.model.test <- lmtest::dwtest(full.model,tol = 0)
165| full.model.pvals[i] <- full.model.test$p.value
166| full.model.stats[i] <- full.model.test$statistic
167| full.model.rsqds[i] <- summary(full.model)$adj.r.squared
168| collapsed.model.test<- lmtest::dwtest(lm.with.licking.subtraction)
169| collapsed.model.pvals[i] <- full.model.test$p.value
170| collapsed.model.stats[i] <- full.model.test$statistic
171| collapsed.model.rsqds[i] <- summary(collapsed.model)$adj.r.squared
172| }
173| collapsed.model.tests
174| collapsed.model.pvals
175| summary(collapsed.model.pvals)
176| hist(collapsed.model.pvals, breaks = 20)
177| hist(full.model.pvals, breaks = 20)
178| for (i in 1:n){
179| roi <- rois[[i]]
180| roidat <- dat[dat$ROI_ID==roi,]
181| outside_trials  <- roidat[roidat$trial_factor== -999,]
182| licking.model <- lm(dF_on_F ~ lick_factor,
183| data = outside_trials)
184| if(get_lm_pvalue(licking.model)>0.05){
185| licking_insignificant = licking_insignificant + 1
186| licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
187| }
188| licking.prediction <- predict(licking.model, newdata = roidat)
189| roidat$residuals <- roidat$dF_on_F - licking.prediction
190| residual.dat <- roidat
191| residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
192| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
193| collapsed.model<- lm(mean.dF ~ trial.segment + trial.segment:correct
194| +trial.segment:go + trial.segment:go:correct,
195| data = collapsed.after.licking.subtraction)
196| full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
197| go:as.factor(trial_factor) + correct:as.factor(trial_factor),
198| data = roidat[roidat$trial_factor!=-999,])
199| full.model.test <- lmtest::dwtest(full.model,tol = 0)
200| full.model.pvals[i] <- full.model.test$p.value
201| full.model.stats[i] <- full.model.test$statistic
202| full.model.rsqds[i] <- summary(full.model)$adj.r.squared
203| collapsed.model.test<- lmtest::dwtest(lm.with.licking.subtraction)
204| collapsed.model.pvals[i] <- collapsed.model.test$p.value
205| collapsed.model.stats[i] <- collapsed.model.test$statistic
206| collapsed.model.rsqds[i] <- summary(collapsed.model)$adj.r.squared
207| }
208| hist(collapsed.model.pvals, breaks = 20)
209| hist(collapsed.model.pvals)
210| collapsed.model.pvals
211| for (i in 1:n){
212| roi <- rois[[i]]
213| roidat <- dat[dat$ROI_ID==roi,]
214| outside_trials  <- roidat[roidat$trial_factor== -999,]
215| licking.model <- lm(dF_on_F ~ lick_factor,
216| data = outside_trials)
217| if(get_lm_pvalue(licking.model)>0.05){
218| licking_insignificant = licking_insignificant + 1
219| licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
220| }
221| licking.prediction <- predict(licking.model, newdata = roidat)
222| roidat$residuals <- roidat$dF_on_F - licking.prediction
223| residual.dat <- roidat
224| residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
225| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
226| collapsed.model<- lm(mean.dF ~ trial.segment + trial.segment:correct
227| +trial.segment:go + trial.segment:go:correct,
228| data = collapsed.after.licking.subtraction)
229| full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) +
230| go:as.factor(trial_factor) + correct:as.factor(trial_factor),
231| data = roidat[roidat$trial_factor!=-999,])
232| full.model.test <- lmtest::dwtest(full.model,tol = 0)
233| full.model.pvals[i] <- full.model.test$p.value
234| full.model.stats[i] <- full.model.test$statistic
235| full.model.rsqds[i] <- summary(full.model)$adj.r.squared
236| collapsed.model.test<- lmtest::dwtest(collapsed.model)
237| collapsed.model.pvals[i] <- collapsed.model.test$p.value
238| collapsed.model.stats[i] <- collapsed.model.test$statistic
239| collapsed.model.rsqds[i] <- summary(collapsed.model)$adj.r.squared
240| }
241| hist(collapsed.model.pvals)
242| hist(collapsed.model.stats)
243| hist(full.model.stats)
244| summary(full.model)
245| plot(full.model$fitted.values)
246| summary(full.model$fitted.values)
247| summary(full.model)
248| anova(full.model)
249| collapsed.model.rsqds
250| hist(collapsed.model.rsqds)
251| hist(full.model.rsqds)
252| plot(collapsed.model)
253| plot(full.model)
254| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
255| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
256| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
257| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
258| licking_coefs[[1]]
259| class(licking_coefs[[1]])
260| rbind(licking_coefs)
261| data.frame(licking_coefs)
262| data.frame(licking_coefs[1:2])
263| data.frame(t(licking_coefs[1:2])
264| )
265| str(data.frame(t(licking_coefs[1:2]))
266| )
267| data.frame(as.list(licking_coeffs[1]))
268| data.frame(as.list(licking_coefs[1]))
269| ls.of.df <- lapply(licking_coefs[1:2],FUN=function(x) data.frame(as.list(x)))
270| do.call('rbind',ls.of.df)
271| str(do.call('rbind',ls.of.df))
272| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
273| #Now construct a dataframe of all the relevant statistics for each ROI
274| model_pvals     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
275| rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
276| coeffs          <- lapply(summary_objects, function(x) x$coefficients)
277| coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
278| coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
279| coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
280| licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
281| licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
282| licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
283| #Name each column something sensible
284| colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
285| colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
286| colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
287| colnames(licking_pvals)       <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
288| anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
289| anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
290| #Finally, partial eta-squareds as a measure of effect size on ANOVA:
291| anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
292| colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
293| colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
294| colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
295| FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
296| #Drop the residuals columns from the ANOVA output matrix
297| anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
298| anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
299| #Glue everything together and dump to CSV
300| output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates)
301| output_frame$`licking.model pvalue`       <- licking_model_pvalues
302| output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
303| anovas
304| summary_objects
305| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average_figuregen.R')
306| #Construct vectors to hold the results for each ROI
307| rois <- unique(dat$ROI_ID)
308| licking_model_pvalues <- numeric(length(rois))
309| summary_objects       <- vector(mode = "list", length= length(rois))
310| model_pvals           <- numeric(length(rois))
311| anovas                <- vector(mode = "list", length = length(rois))
312| licking_summaries     <- vector(mode = 'list', length = length(rois))
313| licking_pvals         <- numeric(length(rois))
314| #For each ROI/bouton in the dataset...
315| for(i in 1:length(rois)){
316| roi     <- rois[i]
317| subset  <- dat[dat$ROI_ID==roi,]
318| minimum <- min(subset$dF_on_F)
319| subset$logged_df <- log_transform(subset$dF_on_F)
320| #Get the timepoints when a trial is not occuring
321| outside_trials  <- subset[subset$trial_factor== -999,]
322| #Fit a licking kernel on those timepoints
323| licking.model <- lm(logged_df ~ as.factor(lick_factor),
324| data = outside_trials)
325| #If the kernel explains a significant amount of variance,
326| #subtract that kernel everytime there's a lick. Otherwise
327| #just subtract the mean value outside trials (ie the intercept).
328| licking_coefs[[i]] <- summary(licking.model)
329| licking_pvals[i]  <-get_lm_pvalue(licking.model)
330| if(licking_pvals[i]>0.05){
331| licking_model <- lm(logged_df ~ 1, data = outside_trials)
332| }
333| licking.prediction <- predict(licking.model, newdata = subset)
334| residual.dat <- subset
335| residual.dat$dF_on_F <- subset$dF_on_F - inverse_log_transform(licking.prediction,
336| minimum)
337| #Now that the effect of licking has been subtracted off if present,
338| #collapse each trial into 3 bins, averaging across time.
339| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
340| #Fit a linear model to predict the average fluorescence in each bin
341| if(contrast_varying && side_varying){
342| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
343| +trial.segment:go + trial.segment:side
344| +trial.segment:correct:contrast,
345| data = collapsed.after.licking.subtraction)
346| }else if(side_varying){
347| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
348| +trial.segment:go + trial.segment:side,
349| data = collapsed.after.licking.subtraction)
350| }else{
351| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
352| +trial.segment:go,
353| data = collapsed.after.licking.subtraction)
354| }
355| summary_objects[[i]] <- summary(lm.with.licking.subtraction)
356| anovas[[i]] <- anova(lm.with.licking.subtraction)
357| model_pvals[[i]] <- get_lm_pvalue(lm.with.licking.subtraction)
358| }
359| #Now construct a dataframe of all the relevant statistics for each ROI
360| model_pvals     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
361| rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
362| coeffs          <- lapply(summary_objects, function(x) x$coefficients)
363| coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
364| coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
365| coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
366| licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
367| licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
368| licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
369| #Name each column something sensible
370| colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
371| colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
372| colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
373| colnames(licking_pvals)       <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
374| anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
375| anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
376| #Finally, partial eta-squareds as a measure of effect size on ANOVA:
377| anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
378| colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
379| colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
380| colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
381| FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
382| #Drop the residuals columns from the ANOVA output matrix
383| anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
384| anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
385| #Glue everything together and dump to CSV
386| output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates)
387| output_frame$`licking.model pvalue`       <- licking_model_pvalues
388| output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
389| output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates, licking_estimates,licking_pvals)
390| output_frame$`licking.model pvalue`       <- licking_model_pvalues
391| output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
392| licking_estimates
393| model_pvals
394| licking.coefs
395| for(i in 1:length(rois)){
396| roi     <- rois[i]
397| subset  <- dat[dat$ROI_ID==roi,]
398| minimum <- min(subset$dF_on_F)
399| subset$logged_df <- log_transform(subset$dF_on_F)
400| #Get the timepoints when a trial is not occuring
401| outside_trials  <- subset[subset$trial_factor== -999,]
402| #Fit a licking kernel on those timepoints
403| licking.model <- lm(logged_df ~ as.factor(lick_factor),
404| data = outside_trials)
405| #If the kernel explains a significant amount of variance,
406| #subtract that kernel everytime there's a lick. Otherwise
407| #just subtract the mean value outside trials (ie the intercept).
408| licking_summaries[[i]] <- summary(licking.model)
409| licking_pvals[i]  <-get_lm_pvalue(licking.model)
410| if(licking_pvals[i]>0.05){
411| licking_model <- lm(logged_df ~ 1, data = outside_trials)
412| }
413| licking.prediction <- predict(licking.model, newdata = subset)
414| residual.dat <- subset
415| residual.dat$dF_on_F <- subset$dF_on_F - inverse_log_transform(licking.prediction,
416| minimum)
417| #Now that the effect of licking has been subtracted off if present,
418| #collapse each trial into 3 bins, averaging across time.
419| collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
420| #Fit a linear model to predict the average fluorescence in each bin
421| if(contrast_varying && side_varying){
422| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
423| +trial.segment:go + trial.segment:side
424| +trial.segment:correct:contrast,
425| data = collapsed.after.licking.subtraction)
426| }else if(side_varying){
427| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
428| +trial.segment:go + trial.segment:side,
429| data = collapsed.after.licking.subtraction)
430| }else{
431| lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
432| +trial.segment:go,
433| data = collapsed.after.licking.subtraction)
434| }
435| summary_objects[[i]] <- summary(lm.with.licking.subtraction)
436| anovas[[i]] <- anova(lm.with.licking.subtraction)
437| model_pvals[[i]] <- get_lm_pvalue(lm.with.licking.subtraction)
438| }
439| #Now construct a dataframe of all the relevant statistics for each ROI
440| model_pvals     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
441| rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
442| coeffs          <- lapply(summary_objects, function(x) x$coefficients)
443| coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
444| coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
445| coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
446| licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
447| licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
448| licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
449| #Name each column something sensible
450| colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
451| colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
452| colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
453| colnames(licking_pvals)       <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
454| anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
455| anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
456| #Finally, partial eta-squareds as a measure of effect size on ANOVA:
457| anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
458| colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
459| colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
460| colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
461| FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
462| #Drop the residuals columns from the ANOVA output matrix
463| anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
464| anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
465| #Glue everything together and dump to CSV
466| output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates, licking_estimates,licking_pvals)
467| output_frame$`licking.model pvalue`       <- licking_model_pvalues
468| output_frame$`collapsed.model pvalue`     <- model_pvals
469| output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
470| output_frame
471| licking_pvals
472| #Now construct a dataframe of all the relevant statistics for each ROI
473| model_pvals     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
474| rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
475| coeffs          <- lapply(summary_objects, function(x) x$coefficients)
476| coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
477| coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
478| coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
479| licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
480| licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
481| licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
482| #Name each column something sensible
483| colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
484| colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
485| colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
486| colnames(licking_pvals)       <- sapply(colnames(licking_pvals),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
487| anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
488| anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
489| #Finally, partial eta-squareds as a measure of effect size on ANOVA:
490| anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
491| colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
492| colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
493| colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
494| FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
495| #Drop the residuals columns from the ANOVA output matrix
496| anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
497| anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
498| #Glue everything together and dump to CSV
499| output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates, licking_estimates,licking_pvals)
500| output_frame$`licking.model pvalue`       <- licking_model_pvalues
501| output_frame$`collapsed.model pvalue`     <- model_pvals
502| output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
503| output_frame
504| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average.R')
505| left_only_results <- analyse_and_produce_csv_of_results(source_file_left_only,
506| "results_left_only.csv")
507| source('C:/Users/viviani/AppData/Roaming/Python/Python37/site-packages/accdatatools/RScripts/subtract_licking_then_average.R')
508| str(left_only_results)
509| left_only_results$`lick.coefficient as.factor.lick_factor.4 pvalue`
510| lm(left_only_results$`ANOVA trial.segment fvalue`~1)
511| ptukey


\RScripts\compare_modelling_approaches.R
0  | source_file <- "C:/Users/viviani/Desktop/single_experiments_for_testing/2016-11-05_03_CFEB029.csv"
1  | 
2  | set.seed(123456789) #Non
3  | 
4  | 
5  | require("lmtest")
6  | 
7  | get_lm_pvalue <- function (modelobject) {
8  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
9  |   f <- summary(modelobject)$fstatistic
10 |   p <- pf(f[1],f[2],f[3],lower.tail=F)
11 |   attributes(p) <- NULL
12 |   return(p)
13 | }
14 | 
15 | collapse.across.time <- function(dat){
16 |   
17 |   num_trials <- sum(dat$trial_factor==1)
18 |   trials.correct <- dat$correct[dat$trial_factor==1]
19 |   trials.go <- dat$go[dat$trial_factor==1]
20 |   trials.side <- dat$side[dat$trial_factor==1]
21 |   result = data.frame(mean.dF = numeric(num_trials*3),
22 |                       trial.segment = character(num_trials*3),
23 |                       stringsAsFactors = FALSE)
24 |   result$mean.dF <- NA
25 |   result$trial.segment<-NA
26 |   #We first need to select only the timepoints happening in the
27 |   #portion of each trial we care about, then we need to sum 
28 |   #together every K consecutive timepoints. In numpy i'd reshape
29 |   #and sum along an axis...
30 |   df.tone <- dat$dF_on_F[dat$trial_component == 'Tone']
31 |   df.tone <- df.tone[1:num_trials*5]
32 |   df.tone.matr <- matrix(df.tone,nrow=num_trials,ncol=5,byrow=TRUE)
33 |   mean.df.tone <- rowSums(df.tone.matr,na.rm=T)/5
34 |   
35 |   df.stim <- dat$dF_on_F[dat$trial_component=='Stim']
36 |   df.stim <- df.stim[1:num_trials*10]
37 |   df.stim.matr = matrix(df.stim,nrow=num_trials,ncol=10,byrow=TRUE)
38 |   mean.df.stim <- rowSums(df.stim.matr,na.rm=T)/10
39 |   
40 |   df.resp <- dat$dF_on_F[dat$trial_component=='Resp']
41 |   df.resp <- df.resp[1:num_trials*10]
42 |   df.resp.matr <- matrix(df.resp,nrow=num_trials,ncol=10,byrow=TRUE)
43 |   mean.df.resp <- rowSums(df.resp.matr,na.rm=T)/10
44 |   
45 |   result_idx = seq(1,3*num_trials,3)
46 | 
47 |   result$mean.dF[result_idx+0]       <- mean.df.tone
48 |   result$trial.segment[result_idx+0] <- "Tone"
49 |   result$mean.dF[result_idx+1]       <- mean.df.stim
50 |   result$trial.segment[result_idx+1] <- "Stimulus"
51 |   result$mean.dF[result_idx+2]       <- mean.df.resp
52 |   result$trial.segment[result_idx+2] <- "Response"
53 | 
54 |   result$correct <- rep(trials.correct,3)
55 |   result$go      <- rep(trials.go,3)
56 |   result$side    <- rep(trials.side,3)
57 |   
58 |   
59 |   result <- na.omit(result)
60 |   result$trial.segment <- as.factor(result$trial.segment)
61 |   return(result)
62 | }
63 | 
64 | 
65 | #Let's start out by getting a single ROI from a single recording.
66 | 
67 | dat <- read.csv(source_file)
68 | dat <- dat[!is.na(dat$dF_on_F),]
69 | dat$correct <- as.factor(dat$correct)
70 | dat$go <- as.factor(dat$go)
71 | 
72 | rois <- unique(dat$ROI_ID)
73 | n <- length(rois)
74 | 
75 | full.model.pvals <- numeric(n)
76 | full.model.stats <- numeric(n)
77 | full.model.rsqds <- numeric(n)
78 | full.model.shapiro.stat <- numeric(n)
79 | full.model.shapiro.pval <- numeric(n)
80 | 
81 | collapsed.model.pvals <- numeric(n)
82 | collapsed.model.stats <- numeric(n)
83 | collapsed.model.rsqds <- numeric(n)
84 | collapsed.model.shapiro.stat <- numeric(n)
85 | collapsed.model.shapiro.pval <- numeric(n)
86 | 
87 | 
88 | for (i in 1:n){
89 |   roi <- rois[[i]]
90 |   roidat <- dat[dat$ROI_ID==roi,]
91 |   outside_trials  <- roidat[roidat$trial_factor== -999,]
92 |   licking.model <- lm(dF_on_F ~ lick_factor, 
93 |                       data = outside_trials)
94 |   
95 |   if(get_lm_pvalue(licking.model)>0.05){
96 |     licking_model <- lm(dF_on_F ~ 1, data = outside_trials)
97 |   }
98 |   
99 |   licking.prediction <- predict(licking.model, newdata = roidat)
100|   roidat$residuals <- roidat$dF_on_F - licking.prediction
101|   residual.dat <- roidat
102|   residual.dat$dF_on_F <- roidat$dF_on_F - licking.prediction
103|   collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
104| 
105|   collapsed.model<- lm(mean.dF ~ trial.segment + trial.segment:correct
106|                                    +trial.segment:go + trial.segment:go:correct,
107|                                    data = collapsed.after.licking.subtraction)
108| 
109|   full.model <- lm(dF_on_F ~ as.factor(lick_factor) + as.factor(trial_factor) + 
110|                 go:as.factor(trial_factor) + correct:as.factor(trial_factor),
111|               data = roidat[roidat$trial_factor!=-999,])
112|   
113|   full.model.test <- lmtest::dwtest(full.model,tol = 0)
114|   full.model.pvals[i] <- full.model.test$p.value
115|   full.model.stats[i] <- full.model.test$statistic
116|   full.model.rsqds[i] <- summary(full.model)$adj.r.squared
117|   
118|   full.model.residuals <- full.model$fitted.values - roidat[roidat$trial_factor!=-999,]$dF_on_F
119|   full.model.shapiro.test <- shapiro.test(sample(full.model.residuals,
120|                                                  min(100,length(full.model.residuals))))
121|   full.model.shapiro.stat[i] <- full.model.shapiro.test$statistic
122|   full.model.shapiro.pval[i] <-full.model.shapiro.test$p.value
123|   
124|   collapsed.model.test<- lmtest::dwtest(collapsed.model)
125|   collapsed.model.pvals[i] <- collapsed.model.test$p.value
126|   collapsed.model.stats[i] <- collapsed.model.test$statistic
127|   collapsed.model.rsqds[i] <- summary(collapsed.model)$adj.r.squared
128| 
129|   collapsed.model.residuals <- (collapsed.model$fitted.values - 
130|                                     collapsed.after.licking.subtraction$mean.dF)
131|   collapsed.model.shapiro.test <- shapiro.test(sample(
132|                                       collapsed.model.residuals,
133|                                       min(100,length(collapsed.model.residuals))))
134|   collapsed.model.shapiro.stat[i] <- collapsed.model.shapiro.test$statistic
135|   collapsed.model.shapiro.pval[i] <- collapsed.model.shapiro.test$p.value
136|   
137| }
138| 
139| 


\RScripts\example_r_script.R
0  | cat("Successful execution!\n")


\RScripts\full_kernel_model.R
0  | require(effectsize)
1  | 
2  | source_file_left_only <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv"
3  | source_file_left_and_right <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/both_sides_high_contrast.csv"
4  | source_file_low_contrast <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/low_contrast.csv"
5  | 
6  | #Helper function to get the overall p-value from a summary(lm) object
7  | #(ie the result of the f-test that you see when you call summary(model))
8  | get_lm_pvalue <- function (modelobject) {
9  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
10 |   f <- summary(modelobject)$fstatistic
11 |   p <- pf(f[1],f[2],f[3],lower.tail=F)
12 |   attributes(p) <- NULL
13 |   return(p)
14 | }
15 | 
16 | #At times we will want to recoverably log-transform our data
17 | log_transform <- function(values){
18 |   minimum <- min(values)
19 |   log(values - minimum + 10e-16)
20 | }
21 | inverse_log_transform <- function(values, minimum){
22 |   exp(values) + minimum - 10e-16
23 | }
24 | 
25 | 
26 | #Function to perform fit the kernel-based model to each ROI and output a CSV
27 | analyse_and_produce_csv_of_results <- function(source_file,destination_file,
28 |                                                side_varying=FALSE,
29 |                                                contrast_varying=FALSE){
30 |   
31 |   num_of_free_variables <- (3 + side_varying + contrast_varying)
32 |   #Read in and clean the data
33 |   cat("Reading in data...")
34 |   dat <- read.csv(source_file)
35 |   dat <- dat[!is.na(dat$dF_on_F),]
36 |   dat$lick_factor  <- as.factor(dat$lick_factor)
37 |   dat$trial_factor <- as.factor(dat$trial_factor)
38 |   dat$correct      <- as.factor(dat$correct)
39 |   dat$go           <- as.factor(dat$go)
40 |   dat$side         <- as.factor(dat$side)
41 |   dat$contrast     <- as.factor(dat$contrast)
42 |   cat("done\nAnalysing...")
43 |   
44 |   #Construct vectors to hold the results for each ROI
45 |   rois <- unique(dat$ROI_ID)
46 |   summary_objects       <- vector(mode = "list", length= length(rois))
47 |   model_pvals           <- numeric(length(rois))
48 |   anovas                <- vector(mode = "list", length = length(rois))
49 |   
50 |   #For each ROI/bouton in the dataset...
51 |   for(i in 1:length(rois)){
52 |     roi     <- rois[i]
53 |     subset  <- dat[dat$ROI_ID==roi,]
54 |     minimum <- min(subset$dF_on_F)
55 |     subset$logged_df <- log_transform(subset$dF_on_F)
56 |     if(contrast_varying && side_varying){
57 |       model<- lm(logged.df ~ lick_factor + 
58 |                              trial_factor +
59 |                              trial_factor:correct +
60 |                              trial_factor:go+
61 |                              trial_factor:side +
62 |                              trial_factor:correct:contrast,
63 |                              data = subset)
64 |     }else if(side_varying){
65 |       model<- lm(logged.df ~ lick_factor + 
66 |                              trial_factor +
67 |                              trial_factor:correct +
68 |                              trial_factor:go+
69 |                              trial_factor:side,
70 |                              data = subset)
71 |     }else{
72 |       model<- lm(logged.df ~ lick_factor + 
73 |                              trial_factor +
74 |                              trial_factor:correct +
75 |                              trial_factor:go,
76 |                              data = subset)
77 |     }
78 |     summary_objects[[i]] <- summary(model)
79 |     model_pvals[i]  <-get_lm_pvalue(modelobject = )
80 |     anovas[[i]] <- anova(model)
81 |     summary_objects[[i]] <- summary(lm.with.licking.subtraction)
82 |     anovas[[i]] <- anova(lm.with.licking.subtraction)
83 |     model_pvals[[i]] <- get_lm_pvalue(lm.with.licking.subtraction)
84 |   }
85 |   
86 |   
87 |   #Now construct a dataframe of all the relevant statistics for each ROI
88 |   model_pvals_a   <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
89 |   rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
90 |   coeffs          <- lapply(summary_objects, function(x) x$coefficients)   
91 |   coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
92 |   coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
93 |   coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
94 |   
95 |   
96 |   #Name each column something sensible
97 |   colnames(coeff_pvals)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"p.unadjusted",sep=" "))
98 |   colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
99 |   colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
100| 
101|   anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
102|   anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
103|   #Finally, partial eta-squareds as a measure of effect size on ANOVA:
104|   anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
105|   
106|   colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
107|   colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
108|   colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
109|                                         FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
110|   #Drop the residuals columns from the ANOVA output matrix
111|   anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
112|   anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
113|   #Glue everything together and dump to CSV
114|   output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals, coeff_pvals_a,coeff_estimates)
115|   output_frame$`collapsed.model p.unadjsuted`     <- model_pvals
116|   output_frame$`collapsed.model pvalue`           <- model_pvals_a
117|   output_frame$`overall.model.adj.rsquared`       <- unlist(rsquareds)
118|   cat("done\nWriting CSV...")
119|   write.csv(output_frame,destination_file)
120|   cat("done\n")
121|   return(output_frame)
122| }
123| 
124| 
125| ######################################################
126| ##    ANALYSIS OF LEFT_ONLY (MONOCULAR) DATASET     ##
127| ######################################################
128| print("Beginning analysis of monocular data...")
129| left_only_results <- analyse_and_produce_csv_of_results(source_file_left_only,
130|                                                         "results_left_only_fullkernel.csv")
131| 
132| #######################################################
133| ## ANALYSIS OF BINOCULAR, HIGH-CONTRAST STIM DATASET ##
134| #######################################################
135| print("Beginning analysis of binocular high contrast data...")
136| binocular_high_con_results <- analyse_and_produce_csv_of_results(source_file_left_and_right,
137|                                                                  'results_binocular_fullkernel.csv',
138|                                                                  side_varying = TRUE)
139| 
140| #######################################################
141| ## ANALYSIS OF BINOCULAR, LOW-CONTRAST STIM DATASET  ##
142| #######################################################
143| print("Beginning analysis of low-contrast data...")
144| binocular_low_con_results <- analyse_and_produce_csv_of_results(source_file_low_contrast,
145|                                                                 'results_low_contrast_fullkernel.csv',
146|                                                                 side_varying = TRUE,
147|                                                                 contrast_varying = TRUE)
148| print("...done")


\RScripts\get_model_pval.R
0  | get_lm_pvalue <- function (modelobject) {
1  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
2  |   f <- summary(modelobject)$fstatistic
3  |   p <- pf(f[1],f[2],f[3],lower.tail=F)
4  |   attributes(p) <- NULL
5  |   return(p)
6  | }


\RScripts\is_there_elevated_activity_during_trials.R
0  | dat <- read.csv("C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv")
1  | 
2  | 
3  | get_lm_pvalue <- function (modelobject) {
4  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
5  |   f <- summary(modelobject)$fstatistic
6  |   p <- pf(f[1],f[2],f[3],lower.tail=F)
7  |   attributes(p) <- NULL
8  |   return(p)
9  | }
10 | 
11 | 
12 | 
13 | rois            <- unique(dat$ROI_ID)
14 | n               <- length(rois)
15 | model_pvals     <- numeric(n)
16 | model_estimates <- numeric(n)
17 | 
18 | for (i in 1:n){
19 |   roi <- rois[i]
20 |   roi.dat <- dat[dat$ROI_ID == roi,]
21 |   during_trial_model <- lm(dF_on_F ~ I(trial_factor>0),
22 |                            dat = roi.dat)
23 |   model_pvals[i]    <- get_lm_pvalue(during_trial_model)
24 |   model_estimates[i]<- during_trial_model$coefficients[[2]]
25 | }
26 | 
27 | model_pvals_a <- p.adjust(model_pvals,'fdr')
28 | sig_model_estimates <- model_estimates[model_pvals_a<0.05]
29 | sum(sig_model_estimates>0)/length(sig_model_estimates)


\RScripts\lick_kernel_subtraction_test.R
0  | dat <- read.csv("C:/Users/viviani/Desktop/test.csv")
1  | a <- dat[dat$ROI_ID == rois[3],]
2  | train <- a[1:10000,]
3  | train_no_trial = train[train$trial_factor== -999,]
4  | 
5  | # We might only want to train the model outside of trials as presumably more of the
6  | # variances is due to things other than licking in that phase. *might*
7  | 
8  | mdl.full <-  lm(dF_on_F ~ lick_factor, data = train) #Train on all training data
9  | mdl.no_trial <- lm(dF_on_F ~ lick_factor, data = train_no_trial) #Train on inter-trial data
10 | 
11 | # Only fair to test the model during licks, otherwise models will always look worse when tested
12 | # against data sets with less licks.
13 | 
14 | test <- a[-(1:10000),]
15 | test <- test[test$lick_factor != -999,]
16 | test_no_trial = test[test$trial_factor == -999,]
17 | 
18 | predict.full <- predict(mdl.full, newdata = test) #predict all test data with
19 | predict.full.no_trial <- predict(mdl.full, newdata = test_no_trial)
20 | predict.no_trial.no_trial <- predict(mdl.no_trial, newdata = test_no_trial)
21 | 
22 | #Calculate Mean Squared Prediction Error
23 | mspe.full = sqrt(mean((test$dF_on_F - predict.full)^2))
24 | mspe.full.no_trial = sqrt(mean( (test_no_trial$dF_on_F - predict.full.no_trial)^2 ))
25 | mspe.no_trial.no_trial = sqrt(mean( (test_no_trial$dF_on_F - predict.no_trial.no_trial)^2  ))
26 | 
27 | #Lets see this graphically
28 | #Real Against Predicted
29 | plot(test_no_trial$dF_on_F, predict.no_trial.no_trial, xlab = "Real Data", ylab = "Predicted Value")
30 | abline(lm(test_no_trial$dF_on_F ~predict.no_trial.no_trial)) #at least in my data, line is close to slop = 1.
31 | 
32 | #Plot them both against time
33 | plot(test_no_trial$dF_on_F, type='l')
34 | lines(1:length(predict.no_trial.no_trial), predict.no_trial.no_trial, col = "red")
35 | 
36 | # Finally, if our prediction was really good, then there should be no relationship
37 | # in the residual of the model against licking
38 | 
39 | resid.mdl <- lm(test_no_trial$dF_on_F - predict.no_trial.no_trial ~ test_no_trial$lick_factor)
40 | summary(resid.mdl)


\RScripts\pretrial_pupil_size_mixed_linear_model.R
0  | seed <- 123456789
1  | 
2  | #library(lme4)
3  | library(merTools)
4  | library(lmerTest)
5  | require(emmeans)
6  | dat <- read.csv("C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv")
7  | dat$licked <- dat$trial_type=="hit" | dat$trial_type == "fa"
8  | dat$correct <- dat$trial_type=="hit" | dat$trial_type=="cr"
9  | 
10 | 
11 | base_model       <- lmer(pupil_diameter ~ trial_frame + (1|recording), 
12 |                          data=dat, REML=FALSE)
13 | # base_model_parabola <- lmer(pupil_diameter ~ trial_frame + I(trial_frame^2) + (1|recording),
14 | #                             data = dat, REML = FALSE)
15 | # base_model_factor <- lmer(pupil_diameter ~ as.factor(trial_frame)+(1|recording),
16 | #                           data=dat,REML=FALSE)
17 | trial_type_model    <- lmer(pupil_diameter ~ trial_frame*licked*correct + (1|recording),
18 |                          data=dat, REML=FALSE)
19 | # go_correct_model <- lmer(pupil_diameter ~ go*correct*trial_type + (1|recording), 
20 | #                          data=dat, REML=FALSE)
21 | # trial_type_model_parabola<- lmer(pupil_diameter ~ I(trial_frame**2)*trial_type +trial_frame*trial_type + (1|recording), 
22 | #                              data=dat, REML=FALSE)
23 | # trial_type_model_factor<- lmer(pupil_diameter ~ as.factor(trial_frame)*trial_type - trial_type + (1|recording),
24 | #                                  data=dat, REML=FALSE)
25 | # go_correct_model_factor<- lmer(pupil_diameter ~ as.factor(trial_frame)*go*correct - go*correct + (1|recording), 
26 | #                                data=dat, REML=FALSE)
27 | # 
28 | # anova_of_models  <- anova(trial_type_model, base_model)
29 | 
30 | trial_frame    <- numeric(26)
31 | trial_frame[] <- 0:25
32 | trial_type     <- numeric(26)
33 | trial_type[]  <- "hit"
34 | recording      <- numeric(26)
35 | recording[]   <- dat$recording[1]
36 | mock_hit <- data.frame(trial_frame,trial_type,recording)
37 | mock_miss <- mock_hit
38 | mock_miss$trial_type <- "miss"
39 | mock_fa <- mock_hit
40 | mock_fa$trial_type <- "fa"
41 | mock_cr <- mock_hit
42 | mock_cr$trial_type <- "cr"
43 | 
44 | save_predictions_to_csv <- function(model,path){
45 |   intervals_hit <- predictInterval(model,
46 |                                    mock_hit,
47 |                                    which = "fixed",
48 |                                    level = 0.5,
49 |                                    n.sims=10000,
50 |                                    seed = seed)
51 |   intervals_hit$trial_type <- "hit"
52 |   
53 |   intervals_miss <- predictInterval(model,
54 |                                    mock_miss,
55 |                                    which = "fixed",
56 |                                    level = 0.5,
57 |                                    n.sims=10000,
58 |                                    seed = seed)
59 |   intervals_miss$trial_type <- "miss"
60 |   
61 |   intervals_cr <- predictInterval(model,
62 |                                    mock_cr,
63 |                                    which = "fixed",
64 |                                    level = 0.5,
65 |                                    n.sims=10000,
66 |                                    seed = seed)
67 |   intervals_cr$trial_type<-"cr"
68 |   
69 |   intervals_fa <- predictInterval(model,
70 |                                    mock_fa,
71 |                                    which = "fixed",
72 |                                    level = 0.5,
73 |                                    n.sims=10000,
74 |                                    seed = seed)
75 |   intervals_fa$trial_type <- "fa"
76 |   out <- rbind(intervals_hit,intervals_miss,intervals_cr,intervals_fa)
77 |   write.csv(out, 
78 |             path)
79 | }
80 | 
81 | # save_predictions_to_csv(trial_type_model_parabola,
82 | #                         "C:/Users/viviani/Desktop/parabolic_mixed_linear_model_pupil_prediction.csv"
83 | #                         )
84 | base_model       <- lmer(pupil_diameter ~ trial_frame + (1|recording), 
85 |                          data=dat, REML=FALSE)
86 | trial_type_model    <- lmer(pupil_diameter ~ trial_frame*licked*correct + (1|recording),
87 |                             data=dat, REML=FALSE)
88 | print(anova(base_model,trial_type_model))
89 | print(anova(trial_type_model))
90 | print(summary(trial_type_model))
91 | var <- emmeans::emtrends(trial_type_model, pairwise ~ licked*correct, var = "trial_frame")
92 | posthoc <- summary(var)$contrasts
93 | cat("\n\n")
94 | print(posthoc)
95 | # posthoc$p.value <- p.adjust(posthoc$p.value, method = "bonferroni")


\RScripts\pupils_vs_peritrial.R
0  | dat <- read.csv("C:/Users/viviani/Desktop/test.csv")
1  | 
2  | model_peritrial <- lm(pupil_diameter ~ as.factor(peritrial_factor) + number_of_trials_seen:as.factor(peritrial_factor),
3  |             data = dat)
4  | 
5  | model_intertrial <-lm(pupil_diameter ~ as.factor(trial_factor),
6  |                       data = dat)
7  | 
8  | 
9  | dat$trial_is_happening <- dat$trial_factor!=(-999)
10 | 
11 | rois <- unique(dat$ROI_ID)
12 | n <- length(rois)
13 | pvals = numeric(n)
14 | coeffs = numeric(n)
15 | 
16 | for (i in 1:n){
17 |   roi = rois[[i]]
18 |   cat(sprintf("%d of %d       \r",i,n))
19 |   subset = dat[dat$ROI_ID == roi,]
20 |   model_trial <- lm(dF_on_F~trial_is_happening,
21 |                                  data= subset)
22 |   pvals[i] <- anova(model_trial)$`Pr(>F)`[1] #The pvalue
23 |   coeffs[i]<- model_trial$coefficients[2]
24 | }
25 | pvals <- p.adjust(pvals, method = 'fdr')
26 | 
27 | 
28 |   
29 | 
30 | 


\RScripts\pupil_size_mixed_linear_model.R
0  | seed <- 123456789
1  | 
2  | #library(lme4)
3  | library(merTools)
4  | library(lmerTest)
5  | require(emmeans)
6  | dat <- read.csv("C:/Users/viviani/Desktop/full_datasets_for_analysis/pupil_analysis_data.csv")
7  | dat$licked <- dat$trial_type=="hit" | dat$trial_type == "fa"
8  | dat$correct <- dat$trial_type=="hit" | dat$trial_type=="cr"
9  | 
10 | 
11 | base_model       <- lmer(pupil_diameter ~ trial_frame + (1|recording), 
12 |                          data=dat, REML=FALSE)
13 | # base_model_parabola <- lmer(pupil_diameter ~ trial_frame + I(trial_frame^2) + (1|recording),
14 | #                             data = dat, REML = FALSE)
15 | # base_model_factor <- lmer(pupil_diameter ~ as.factor(trial_frame)+(1|recording),
16 | #                           data=dat,REML=FALSE)
17 | trial_type_model    <- lmer(pupil_diameter ~ trial_frame*licked*correct + (1|recording),
18 |                          data=dat, REML=FALSE)
19 | # go_correct_model <- lmer(pupil_diameter ~ go*correct*trial_type + (1|recording), 
20 | #                          data=dat, REML=FALSE)
21 | # trial_type_model_parabola<- lmer(pupil_diameter ~ I(trial_frame**2)*trial_type +trial_frame*trial_type + (1|recording), 
22 | #                              data=dat, REML=FALSE)
23 | # trial_type_model_factor<- lmer(pupil_diameter ~ as.factor(trial_frame)*trial_type - trial_type + (1|recording),
24 | #                                  data=dat, REML=FALSE)
25 | # go_correct_model_factor<- lmer(pupil_diameter ~ as.factor(trial_frame)*go*correct - go*correct + (1|recording), 
26 | #                                data=dat, REML=FALSE)
27 | # 
28 | # anova_of_models  <- anova(trial_type_model, base_model)
29 | 
30 | trial_frame    <- numeric(26)
31 | trial_frame[] <- 0:25
32 | trial_type     <- numeric(26)
33 | trial_type[]  <- "hit"
34 | recording      <- numeric(26)
35 | recording[]   <- dat$recording[1]
36 | mock_hit <- data.frame(trial_frame,trial_type,recording)
37 | mock_miss <- mock_hit
38 | mock_miss$trial_type <- "miss"
39 | mock_fa <- mock_hit
40 | mock_fa$trial_type <- "fa"
41 | mock_cr <- mock_hit
42 | mock_cr$trial_type <- "cr"
43 | 
44 | save_predictions_to_csv <- function(model,path){
45 |   intervals_hit <- predictInterval(model,
46 |                                    mock_hit,
47 |                                    which = "fixed",
48 |                                    level = 0.5,
49 |                                    n.sims=10000,
50 |                                    seed = seed)
51 |   intervals_hit$trial_type <- "hit"
52 |   
53 |   intervals_miss <- predictInterval(model,
54 |                                    mock_miss,
55 |                                    which = "fixed",
56 |                                    level = 0.5,
57 |                                    n.sims=10000,
58 |                                    seed = seed)
59 |   intervals_miss$trial_type <- "miss"
60 |   
61 |   intervals_cr <- predictInterval(model,
62 |                                    mock_cr,
63 |                                    which = "fixed",
64 |                                    level = 0.5,
65 |                                    n.sims=10000,
66 |                                    seed = seed)
67 |   intervals_cr$trial_type<-"cr"
68 |   
69 |   intervals_fa <- predictInterval(model,
70 |                                    mock_fa,
71 |                                    which = "fixed",
72 |                                    level = 0.5,
73 |                                    n.sims=10000,
74 |                                    seed = seed)
75 |   intervals_fa$trial_type <- "fa"
76 |   out <- rbind(intervals_hit,intervals_miss,intervals_cr,intervals_fa)
77 |   write.csv(out, 
78 |             path)
79 | }
80 | 
81 | # save_predictions_to_csv(trial_type_model_parabola,
82 | #                         "C:/Users/viviani/Desktop/parabolic_mixed_linear_model_pupil_prediction.csv"
83 | #                         )
84 | base_model       <- lmer(pupil_diameter ~ trial_frame + (1|recording), 
85 |                          data=dat, REML=FALSE)
86 | trial_type_model    <- lmer(pupil_diameter ~ trial_frame*licked*correct + (1|recording),
87 |                             data=dat, REML=FALSE)
88 | print(anova(base_model,trial_type_model))
89 | print(anova(trial_type_model))
90 | print(summary(trial_type_model))
91 | var <- emmeans::emtrends(trial_type_model, pairwise ~ licked*correct, var = "trial_frame")
92 | posthoc <- summary(var)$contrasts
93 | cat("\n\n")
94 | print(posthoc)
95 | # posthoc$p.value <- p.adjust(posthoc$p.value, method = "bonferroni")


\RScripts\r_interface.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Aug 20 12:50:01 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import subprocess
7  | from os import path
8  | from accdatatools.RScripts import __path__ as r_script_root
9  | 
10 | RSCRIPT_INTERPRETER = "c:\\Program Files\\R\\R-3.6.0\\bin\\Rscript.exe"
11 | 
12 | 
13 | def execute_r_script(r_script_name, *args):
14 |     try:
15 |         r_script_root_path = r_script_root._path[0]
16 |     except AttributeError:
17 |         r_script_root_path = r_script_root[0]
18 |     path_to_r_script = path.join(r_script_root_path,
19 |                                  r_script_name)
20 |     command = [RSCRIPT_INTERPRETER, path_to_r_script]
21 |     if args:
22 |         command.extend(args)
23 |     print(command)
24 |     result = subprocess.check_output(command)
25 |     return result.replace(b"\r\n",b"\n").decode("utf-8")
26 | 
27 | if __name__=="__main__":
28 |     a = execute_r_script(
29 |         "pupil_size_mixed_linear_model.R")
30 |     print(a)
31 |     


\RScripts\subtract_licking_then_average.R
0  | require(effectsize)
1  | 
2  | source_file_left_only <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv"
3  | source_file_left_and_right <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/both_sides_high_contrast.csv"
4  | source_file_low_contrast <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/low_contrast.csv"
5  | 
6  | #Helper function to get the overall p-value from a summary(lm) object
7  | #(ie the result of the f-test that you see when you call summary(model))
8  | get_lm_pvalue <- function (modelobject) {
9  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
10 |   f <- summary(modelobject)$fstatistic
11 |   p <- pf(f[1],f[2],f[3],lower.tail=F)
12 |   attributes(p) <- NULL
13 |   return(p)
14 | }
15 | 
16 | #At times we will want to recoverably log-transform our data
17 | log_transform <- function(values){
18 |   minimum <- min(values)
19 |   log(values - minimum + 10e-16)
20 | }
21 | inverse_log_transform <- function(values, minimum){
22 |   exp(values) + minimum - 10e-16
23 | }
24 | 
25 | 
26 | #Helper function to collapse a dataset across time
27 | collapse.across.time <- function(dat){
28 |   #Converts a dataset where datapoints are frames to a dataset where datapoints
29 |   #are segments of a trial - in particular, the first second, the second and 
30 |   #third second, and the fourth and fifth second. This is to reduce autocorrelation
31 |   #and increase normality of residuals.
32 |   num_trials <- sum(dat$trial_factor==1)
33 |   trials.correct <- dat$correct[dat$trial_factor==1]
34 |   trials.go      <- dat$go[dat$trial_factor==1]
35 |   trials.side    <- dat$side[dat$trial_factor==1]
36 |   trials.contrast<- dat$contrast[dat$trial_factor==1]
37 |   result = data.frame(mean.dF = numeric(num_trials*3),
38 |                       trial.segment = character(num_trials*3),
39 |                       stringsAsFactors = FALSE)
40 |   result$mean.dF <- NA
41 |   result$trial.segment<-NA
42 |   #We first need to select only the timepoints happening in the
43 |   #portion of each trial we care about, then we need to sum 
44 |   #together every K consecutive timepoints. In numpy i'd reshape
45 |   #and sum along an axis,, but in R oh no oh heck
46 |   df.tone <- dat$dF_on_F[dat$trial_component == 'Tone']
47 |   df.tone <- df.tone[1:num_trials*5]
48 |   df.tone.matr <- matrix(df.tone,nrow=num_trials,ncol=5,byrow=TRUE)
49 |   mean.df.tone <- rowSums(df.tone.matr,na.rm=T)/5
50 |   
51 |   df.stim <- dat$dF_on_F[dat$trial_component=='Stim']
52 |   df.stim <- df.stim[1:num_trials*10]
53 |   df.stim.matr = matrix(df.stim,nrow=num_trials,ncol=10,byrow=TRUE)
54 |   mean.df.stim <- rowSums(df.stim.matr,na.rm=T)/10
55 |   
56 |   df.resp <- dat$dF_on_F[dat$trial_component=='Resp']
57 |   df.resp <- df.resp[1:num_trials*10]
58 |   df.resp.matr <- matrix(df.resp,nrow=num_trials,ncol=10,byrow=TRUE)
59 |   mean.df.resp <- rowSums(df.resp.matr,na.rm=T)/10
60 |   
61 |   result_idx = seq(1,3*num_trials,3)
62 |   
63 |   result$mean.dF[result_idx+0]       <- mean.df.tone
64 |   result$trial.segment[result_idx+0] <- "Tone"
65 |   result$mean.dF[result_idx+1]       <- mean.df.stim
66 |   result$trial.segment[result_idx+1] <- "Stimulus"
67 |   result$mean.dF[result_idx+2]       <- mean.df.resp
68 |   result$trial.segment[result_idx+2] <- "Response"
69 |   
70 |   result$correct <- as.factor(rep(trials.correct,3))
71 |   result$go      <- as.factor(rep(trials.go,3))
72 |   result$side    <- as.factor(rep(trials.side,3))
73 |   result$contrast<- as.factor(rep(trials.contrast,3))
74 |   
75 |   result <- na.omit(result)
76 |   result$trial.segment <- as.factor(result$trial.segment)
77 |   return(result)
78 | }
79 | 
80 | #Function to perform our main mode of analysis
81 | analyse_and_produce_csv_of_results <- function(source_file,destination_file,
82 |                                                side_varying=FALSE,
83 |                                                contrast_varying=FALSE){
84 |   
85 |   num_of_free_variables <- (3 + side_varying + contrast_varying)
86 |   #Read in and clean the data
87 |   cat("Reading in data...")
88 |   dat <- read.csv(source_file)
89 |   dat <- dat[!is.na(dat$dF_on_F),]
90 |   cat("done\nAnalysing...")
91 |   
92 |   #Construct vectors to hold the results for each ROI
93 |   rois <- unique(dat$ROI_ID)
94 |   licking_model_pvalues <- numeric(length(rois))
95 |   summary_objects       <- vector(mode = "list", length= length(rois))
96 |   model_pvals           <- numeric(length(rois))
97 |   anovas                <- vector(mode = "list", length = length(rois))
98 |   
99 |   
100|   licking_summaries     <- vector(mode = 'list', length = length(rois))
101|   licking_pvals         <- numeric(length(rois))
102|   
103|   #For each ROI/bouton in the dataset...
104|   for(i in 1:length(rois)){
105|     roi     <- rois[i]
106|     subset  <- dat[dat$ROI_ID==roi,]
107|     minimum <- min(subset$dF_on_F)
108|     subset$logged_df <- log_transform(subset$dF_on_F)
109|     #Get the timepoints when a trial is not occuring
110|     outside_trials  <- subset[subset$trial_factor== -999,]
111|     #Fit a licking kernel on those timepoints
112|     licking.model <- lm(logged_df ~ as.factor(lick_factor), 
113|                         data = outside_trials)
114|     #If the kernel explains a significant amount of variance,
115|     #subtract that kernel everytime there's a lick. Otherwise
116|     #just subtract the mean value outside trials (ie the intercept).
117|     licking_summaries[[i]] <- summary(licking.model)
118|     licking_pvals[i]  <-get_lm_pvalue(licking.model)
119|     if(licking_pvals[i]>0.05){
120|       licking_model <- lm(logged_df ~ 1, data = outside_trials)
121|     }
122|     licking.prediction <- predict(licking.model, newdata = subset)
123|     residual.dat <- subset
124|     residual.dat$dF_on_F <- subset$dF_on_F - inverse_log_transform(licking.prediction,
125|                                                                    minimum)
126|     #Now that the effect of licking has been subtracted off if present,
127|     #collapse each trial into 3 bins, averaging across time.
128|     collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
129|     
130|     #Fit a linear model to predict the average fluorescence in each bin
131|     if(contrast_varying && side_varying){
132|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
133|                                        +trial.segment:go + trial.segment:side
134|                                        +trial.segment:correct:contrast,
135|                                        data = collapsed.after.licking.subtraction)
136|     }else if(side_varying){
137|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
138|                                        +trial.segment:go + trial.segment:side,
139|                                        data = collapsed.after.licking.subtraction)
140|     }else{
141|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
142|                                        +trial.segment:go,
143|                                        data = collapsed.after.licking.subtraction)
144|     }
145|     summary_objects[[i]] <- summary(lm.with.licking.subtraction)
146|     anovas[[i]] <- anova(lm.with.licking.subtraction)
147|     model_pvals[[i]] <- get_lm_pvalue(lm.with.licking.subtraction)
148|   }
149|   
150|   
151|   #Now construct a dataframe of all the relevant statistics for each ROI
152|   model_pvals_a     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
153|   rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
154|   coeffs          <- lapply(summary_objects, function(x) x$coefficients)   
155|   coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
156|   coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
157|   coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
158|   
159|   licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
160|   licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
161|   licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
162|   
163|   #Name each column something sensible
164|   colnames(coeff_pvals)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"p.unadjusted",sep=" "))
165|   colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
166|   colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
167|   colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('lick.coefficient',x,"estimate",sep=" "))
168|   colnames(licking_pvals)       <- sapply(colnames(licking_pvals),FUN=function(x) paste('lick.coefficient',x,"pvalue",sep=" "))
169|   
170|   anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
171|   anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
172|   #Finally, partial eta-squareds as a measure of effect size on ANOVA:
173|   anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
174|   
175|   colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
176|   colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
177|   colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
178|                                         FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
179|   #Drop the residuals columns from the ANOVA output matrix
180|   anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
181|   anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
182|   #Glue everything together and dump to CSV
183|   output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals, coeff_pvals_a,coeff_estimates, licking_estimates,licking_pvals)
184|   output_frame$`licking.model pvalue`       <- licking_model_pvalues
185|   output_frame$`collapsed.model pvalue`     <- model_pvals_a
186|   output_frame$`collapsed.model p.unadjusted`<-model_pvals
187|   output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
188|   cat("done\nWriting CSV...")
189|   write.csv(output_frame,destination_file)
190|   cat("done\n")
191|   return(output_frame)
192| }
193| 
194| 
195| ######################################################
196| ##    ANALYSIS OF LEFT_ONLY (MONOCULAR) DATASET     ##
197| ######################################################
198| print("Beginning analysis of monocular data...")
199| left_only_results <- analyse_and_produce_csv_of_results(source_file_left_only,
200|                                                         "results_left_only.csv")
201| 
202| #######################################################
203| ## ANALYSIS OF BINOCULAR, HIGH-CONTRAST STIM DATASET ##
204| #######################################################
205| print("Beginning analysis of binocular high contrast data...")
206| binocular_high_con_results <- analyse_and_produce_csv_of_results(source_file_left_and_right,
207|                                                                  'results_binocular.csv',
208|                                                                  side_varying = TRUE)
209| 
210| #######################################################
211| ## ANALYSIS OF BINOCULAR, LOW-CONTRAST STIM DATASET  ##
212| #######################################################
213| print("Beginning analysis of low-contrast data...")
214| binocular_low_con_results <- analyse_and_produce_csv_of_results(source_file_low_contrast,
215|                                                                 'results_low_contrast.csv',
216|                                                                  side_varying = TRUE,
217|                                                                  contrast_varying = TRUE)
218| print("...done")


\RScripts\subtract_licking_then_average_figuregen.R
0  | require(effectsize)
1  | 
2  | source_file_left_only <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv"
3  | source_file_left_and_right <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/both_sides_high_contrast.csv"
4  | source_file_low_contrast <- "C:/Users/viviani/Desktop/full_datasets_for_analysis/low_contrast.csv"
5  | 
6  | #Helper function to get the overall p-value from a summary(lm) object
7  | #(ie the result of the f-test that you see when you call summary(model))
8  | get_lm_pvalue <- function (modelobject) {
9  |   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
10 |   f <- summary(modelobject)$fstatistic
11 |   p <- pf(f[1],f[2],f[3],lower.tail=F)
12 |   attributes(p) <- NULL
13 |   return(p)
14 | }
15 | 
16 | #At times we will want to recoverably log-transform our data
17 | log_transform <- function(values){
18 |   m <- min(values)
19 |   log(values - m + 10e-16)
20 | }
21 | inverse_log_transform <- function(values, minimum){
22 |   exp(values) + minimum - 10e-16
23 | }
24 | 
25 | 
26 | #Helper function to collapse a dataset across time
27 | collapse.across.time <- function(dat){
28 |   #Converts a dataset where datapoints are frames to a dataset where datapoints
29 |   #are segments of a trial - in particular, the first second, the second and 
30 |   #third second, and the fourth and fifth second. This is to reduce autocorrelation
31 |   #and increase normality of residuals.
32 |   num_trials <- sum(dat$trial_factor==1)
33 |   trials.correct <- dat$correct[dat$trial_factor==1]
34 |   trials.go      <- dat$go[dat$trial_factor==1]
35 |   trials.side    <- dat$side[dat$trial_factor==1]
36 |   trials.contrast<- dat$contrast[dat$trial_factor==1]
37 |   result = data.frame(mean.dF = numeric(num_trials*3),
38 |                       trial.segment = character(num_trials*3),
39 |                       stringsAsFactors = FALSE)
40 |   result$mean.dF <- NA
41 |   result$trial.segment<-NA
42 |   #We first need to select only the timepoints happening in the
43 |   #portion of each trial we care about, then we need to sum 
44 |   #together every K consecutive timepoints. In numpy i'd reshape
45 |   #and sum along an axis, but in R I know of no elegant way to
46 |   #do this
47 |   df.tone <- dat$dF_on_F[dat$trial_component == 'Tone']
48 |   df.tone <- df.tone[1:num_trials*5]
49 |   df.tone.matr <- matrix(df.tone,nrow=num_trials,ncol=5,byrow=TRUE)
50 |   mean.df.tone <- rowSums(df.tone.matr,na.rm=T)/5
51 |   
52 |   df.stim <- dat$dF_on_F[dat$trial_component=='Stim']
53 |   df.stim <- df.stim[1:num_trials*10]
54 |   df.stim.matr = matrix(df.stim,nrow=num_trials,ncol=10,byrow=TRUE)
55 |   mean.df.stim <- rowSums(df.stim.matr,na.rm=T)/10
56 |   
57 |   df.resp <- dat$dF_on_F[dat$trial_component=='Resp']
58 |   df.resp <- df.resp[1:num_trials*10]
59 |   df.resp.matr <- matrix(df.resp,nrow=num_trials,ncol=10,byrow=TRUE)
60 |   mean.df.resp <- rowSums(df.resp.matr,na.rm=T)/10
61 |   
62 |   result_idx = seq(1,3*num_trials,3)
63 |   
64 |   result$mean.dF[result_idx+0]       <- mean.df.tone
65 |   result$trial.segment[result_idx+0] <- "Tone"
66 |   result$mean.dF[result_idx+1]       <- mean.df.stim
67 |   result$trial.segment[result_idx+1] <- "Stimulus"
68 |   result$mean.dF[result_idx+2]       <- mean.df.resp
69 |   result$trial.segment[result_idx+2] <- "Response"
70 |   
71 |   result$correct <- as.factor(rep(trials.correct,3))
72 |   result$go      <- as.factor(rep(trials.go,3))
73 |   result$side    <- as.factor(rep(trials.side,3))
74 |   result$contrast<- as.factor(rep(trials.contrast,3))
75 |   
76 |   result <- na.omit(result)
77 |   result$trial.segment <- as.factor(result$trial.segment)
78 |   return(result)
79 | }
80 | 
81 | source_file <- "C:/Users/viviani/Desktop/single_experiments_for_testing/2016-11-01_03_CFEB027.csv"
82 | side_varying=FALSE
83 | contrast_varying=FALSE
84 |   
85 |   num_of_free_variables <- (3 + side_varying + contrast_varying)
86 |   #Read in and clean the data
87 |   cat("Reading in data...")
88 |   dat <- read.csv(source_file)
89 |   dat <- dat[!is.na(dat$dF_on_F),]
90 |   cat("done\n")
91 |   
92 |   #Construct vectors to hold the results for each ROI
93 |   rois <- unique(dat$ROI_ID)
94 |   licking_model_pvalues <- numeric(length(rois))
95 |   summary_objects       <- vector(mode = "list", length= length(rois))
96 |   model_pvals           <- numeric(length(rois))
97 |   anovas                <- vector(mode = "list", length = length(rois))
98 |   
99 |   
100|   licking_summaries     <- vector(mode = 'list', length = length(rois))
101|   licking_pvals         <- numeric(length(rois))
102|   
103|   #For each ROI/bouton in the dataset...
104|   for(i in 1:length(rois)){
105|     roi     <- rois[i]
106|     subset  <- dat[dat$ROI_ID==roi,]
107|     minimum <- min(subset$dF_on_F)
108|     subset$logged_df <- log_transform(subset$dF_on_F)
109|     #Get the timepoints when a trial is not occuring
110|     outside_trials  <- subset[subset$trial_factor== -999,]
111|     #Fit a licking kernel on those timepoints
112|     licking.model <- lm(logged_df ~ as.factor(lick_factor), 
113|                         data = outside_trials)
114|     #If the kernel explains a significant amount of variance,
115|     #subtract that kernel everytime there's a lick. Otherwise
116|     #just subtract the mean value outside trials (ie the intercept).
117|     licking_summaries[[i]] <- summary(licking.model)
118|     licking_pvals[i]  <-get_lm_pvalue(licking.model)
119|     if(licking_pvals[i]>0.05){
120|       licking_model <- lm(logged_df ~ 1, data = outside_trials)
121|     }
122|     licking.prediction <- predict(licking.model, newdata = subset)
123|     residual.dat <- subset
124|     residual.dat$dF_on_F <- subset$dF_on_F - inverse_log_transform(licking.prediction,
125|                                                                    minimum)
126|     #Now that the effect of licking has been subtracted off if present,
127|     #collapse each trial into 3 bins, averaging across time.
128|     collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
129|     
130|     #Fit a linear model to predict the average fluorescence in each bin
131|     if(contrast_varying && side_varying){
132|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
133|                                        +trial.segment:go + trial.segment:side
134|                                        +trial.segment:correct:contrast,
135|                                        data = collapsed.after.licking.subtraction)
136|     }else if(side_varying){
137|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
138|                                        +trial.segment:go + trial.segment:side,
139|                                        data = collapsed.after.licking.subtraction)
140|     }else{
141|       lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
142|                                        +trial.segment:go,
143|                                        data = collapsed.after.licking.subtraction)
144|     }
145|     summary_objects[[i]] <- summary(lm.with.licking.subtraction)
146|     anovas[[i]] <- anova(lm.with.licking.subtraction)
147|     model_pvals[[i]] <- get_lm_pvalue(lm.with.licking.subtraction)
148|   }
149| 
150|   
151|   #Now construct a dataframe of all the relevant statistics for each ROI
152|   model_pvals     <- p.adjust(model_pvals, method = "fdr")                 #Overall model significance
153|   rsquareds       <- lapply(summary_objects, function(x) x$adj.r.squared)  #Overall adjusted R squared
154|   coeffs          <- lapply(summary_objects, function(x) x$coefficients)   
155|   coeff_estimates <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Estimate"])))     #Coefficient Estimates
156|   coeff_pvals     <- data.frame(do.call(rbind, lapply(coeffs,function(x) x[,"Pr(>|t|)"])))
157|   coeff_pvals_a   <- data.frame(lapply(coeff_pvals, FUN=function(x) p.adjust(x,method='fdr'))) #Coefficient pvalues
158| 
159|   licking.coefs   <- lapply(licking_summaries, function(x) x$coefficients)
160|   licking_estimates <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Estimate"])))     #Coefficient Estimates
161|   licking_pvals     <- data.frame(do.call(rbind, lapply(licking.coefs,function(x) x[,"Pr(>|t|)"])))
162|   
163|   #Name each column something sensible
164|   colnames(coeff_pvals_a)   <- sapply(colnames(coeff_pvals_a),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
165|   colnames(coeff_estimates) <- sapply(colnames(coeff_estimates),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
166|   colnames(licking_estimates)   <- sapply(colnames(licking_estimates),FUN=function(x) paste('coefficient',x,"pvalue",sep=" "))
167|   colnames(licking_pvals)       <- sapply(colnames(licking_pvals),FUN=function(x) paste('coefficient',x,"estimate",sep=" "))
168|   
169|   anova_frame_pvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) p.adjust(x$`Pr(>F)`,method='fdr')))))   #ANOVA p-values for each var
170|   anova_frame_fvals <- data.frame(t(rbind(sapply(anovas,FUN=function(x) x$`F value`))))  #ANOVA f-values
171|   #Finally, partial eta-squareds as a measure of effect size on ANOVA:
172|   anova_frame_etas  <- data.frame(t(rbind(sapply(anovas,FUN=function(x) effectsize::eta_squared(x)$Eta_Sq_partial))))
173|   
174|   colnames(anova_frame_pvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"pvalue",sep=" "))
175|   colnames(anova_frame_fvals) <- sapply(row.names(anovas[[1]]),FUN=function(x) paste('ANOVA',x,"fvalue",sep=" "))
176|   colnames(anova_frame_etas)  <- sapply(effectsize::eta_squared(anovas[[1]])$Parameter,
177|                                         FUN=function(x) paste('ANOVA',x,"partial_eta2",sep=" "))
178|   #Drop the residuals columns from the ANOVA output matrix
179|   anova_frame_pvals <- anova_frame_pvals[,1:num_of_free_variables]
180|   anova_frame_fvals <- anova_frame_fvals[,1:num_of_free_variables]
181|   #Glue everything together and dump to CSV
182|   output_frame <- cbind(anova_frame_pvals,anova_frame_fvals, anova_frame_etas, coeff_pvals_a,coeff_estimates, licking_estimates,licking_pvals)
183|   output_frame$`licking.model pvalue`       <- licking_model_pvalues
184|   output_frame$`collapsed.model pvalue`     <- model_pvals
185|   output_frame$`overall.model.adj.rsquared` <- unlist(rsquareds)
186| 
187|   
188| 
189| 
190| 


\RScripts\subtract_licking_then_average_validation.R
0  | 
1  | collapse.across.time <- function(dat){
2  |   num_trials <- sum(dat$trial_factor==1)
3  |   trials.correct <- dat$correct[dat$trial_factor==1]
4  |   trials.go <- dat$go[dat$trial_factor==1]
5  |   trials.side <- dat$side[dat$trial_factor==1]
6  |   result = data.frame(mean.dF = numeric(num_trials*3),
7  |                       trial.segment = character(num_trials*3),
8  |                       stringsAsFactors = FALSE)
9  |   result$mean.dF <- NA
10 |   result$trial.segment<-NA
11 |   #We first need to select only the timepoints happening in the
12 |   #portion of each trial we care about, then we need to sum 
13 |   #together every K consecutive timepoints. In numpy i'd reshape
14 |   #and sum along an axis...
15 |   df.tone <- dat$dF_on_F[(dat$trial_factor!=(-999) & dat$trial_factor<6)]
16 |   df.tone <- df.tone[1:(5*(length(df.tone) %/% 5))]
17 |   mean.df.tone <- colSums(matrix(df.tone,5))/5
18 |   
19 |   df.stim <- dat$dF_on_F[(dat$trial_factor>=6 & dat$trial_factor)<16]
20 |   df.stim <- df.stim[1:(10*(length(df.stim) %/% 10))]
21 |   mean.df.stim <- colSums(matrix(df.stim,10))/10
22 |   
23 |   df.resp <- dat$dF_on_F[dat$trial_factor>15]
24 |   df.resp <- df.resp[1:(11*(length(df.resp) %/% 11))]/11
25 |   mean.df.resp <- colSums(matrix(df.resp,11))/11
26 |   result_idx = seq(1,3*length(mean.df.tone),3)
27 |   result$mean.dF[result_idx+0]       <- mean.df.tone
28 |   result$trial.segment[result_idx+0] <- "Tone"
29 |   result$mean.dF[result_idx+1]       <- mean.df.stim
30 |   result$trial.segment[result_idx+1] <- "Stimulus"
31 |   result$mean.dF[result_idx+2]       <- mean.df.resp
32 |   result$trial.segment[result_idx+2] <- "Response"
33 |   
34 |   result$correct <- rep(trials.correct,3)
35 |   result$go      <- rep(trials.go,3)
36 |   result$side    <- rep(trials.side,3)
37 |   
38 |   
39 |   result <- na.omit(result)
40 |   result$trial.segment <- as.factor(result$trial.segment)
41 |   return(result)
42 | }
43 | 
44 | 
45 | #Let's start out by getting a single ROI from a single recording.
46 | 
47 | dat <- read.csv("C:/Users/viviani/Desktop/test.csv")
48 | rois <- unique(dat$ROI_ID)
49 | dat <- dat[dat$ROI_ID == rois[2],]
50 | 
51 | 
52 | # Let's fit a naive model first, which doesn't do the licking 
53 | collapsed.before.licking.subtraction <- collapse.across.time(dat)
54 | lm.no.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
55 |                                 +trial.segment:go,
56 |                                 data = collapsed.before.licking.subtraction)
57 | 
58 | #Okay, now to fit a licking model so we can subtract off its predictions.
59 | #We'll fit this model only on intertrial periods so we don't attribute
60 | #eg Hit-trial-assosiated fluorescence to licking
61 | outside_trials        <- dat[dat$trial_factor== -999,]
62 | #Reserve some data for testing
63 | training_cuttoff      <- floor(0.8*nrow(outside_trials))
64 | licking.training.data <- outside_trials[1:training_cuttoff,] 
65 | licking.testing.data  <- outside_trials[-(1:training_cuttoff),]
66 | 
67 | licking.model <- lm(dF_on_F ~ lick_factor, 
68 |                     data = licking.training.data) 
69 | 
70 | # Before we go any further, was the licking model actually any good?
71 | # We can check by making sure that (1) the model was significant, and 
72 | # (2) we can no longer get a significant model of the residuals
73 | # with licking as the independant variable (this idea courtesy of Bill).
74 | licking.prediction.licktest   <- predict(licking.model, 
75 |                                          newdata = licking.testing.data)
76 | licking.testing.data$residuals <- (licking.testing.data$dF_on_F - 
77 |                                      licking.prediction.licktest)
78 | snd.order.licking.model<- lm(residuals ~ lick_factor,
79 |                              data = licking.testing.data)
80 | 
81 | print("LICKING MODEL (SHOULD BE SIGNIFICANT):")
82 | print(summary(licking.model))
83 | cat("\n\nSECOND ORDER LICKING MODEL (SHOULD BE INSIGNIFICANT):")
84 | print(summary(snd.order.licking.model))
85 | 
86 | # Okay, now let's get out model that fits residuals after licking
87 | # subtraction based on trial features
88 | licking.prediction.everywhere <- predict(licking.model, newdata = dat)
89 | dat$residuals <- dat$dF_on_F - licking.prediction.everywhere
90 | residual.dat <- dat
91 | residual.dat$dF_on_F <- dat$dF_on_F - licking.prediction
92 | collapsed.after.licking.subtraction <- collapse.across.time(residual.dat)
93 | 
94 | lm.with.licking.subtraction<- lm(mean.dF ~ trial.segment + trial.segment:correct
95 |                                   +trial.segment:go,
96 |                                   data = collapsed.after.licking.subtraction)
97 | 
98 | #Was our second model better?
99 | cat("\n\nModel WITHOUT Licking Subtraction")
100| print(summary(lm.no.licking.subtraction))
101| cat("\n\nModel WITH Licking Subtraction")
102| print(summary(lm.with.licking.subtraction))
103| 
104| #We can't get an F-test off an anova because out models
105| #have the same number of degrees of freedom, but we
106| #can at least get the RSS.
107| cat("\n\nANOVA")
108| print(anova(lm.no.licking.subtraction,
109|             lm.with.licking.subtraction))


\RScripts\__init__.py


\Suite2p\automate_s2p.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun May 10 14:45:31 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | from statistics import mode, StatisticsError
8  | 
9  | import numpy as np
10 | import os
11 | from suite2p.run_s2p import run_s2p, default_ops
12 | from accdatatools.Utils.map_across_dataset import apply_to_all_recordings
13 | from accdatatools.Utils.path import exp_id
14 | from accdatatools.Summaries.produce_trial_summary_document import get_classes_of_recording
15 | 
16 | 
17 | def run_s2p_on(path, ops_file=None, reprocess=False, 
18 |                infer_from_recording_classes=False,
19 |                inferrer=None):
20 |     db = {'data_path':[path]}
21 |     if ops_file==None:
22 |         ops_file = os.path.join(path,"suite2p","plane0","ops.npy")
23 |     try:
24 |         ops = np.load(ops_file,allow_pickle=True).item()
25 |         ops["keep_movie_raw"] #This is so we get a KeyError
26 |         ops["keep_movie_raw"] = True                
27 |         ops["connected"]
28 |         ops["connected"] = False
29 |         ops["max_overlap"]
30 |         ops["max_overlap"] = 0.2
31 |         ops["do_registration"]
32 |         ops["do_registration"] = 2 #ALWAYS redo registration
33 |         ops["look_one_level_down"]
34 |         ops["look_one_level_down"] = True
35 |     except FileNotFoundError:
36 |         if all((any(["tif" in file for file in os.listdir(path)]),
37 |                 infer_from_recording_classes,
38 |                 inferrer!=None)):
39 |             #How many planes?
40 |             no_of_planes = inferrer(exp_id(path))
41 |             if no_of_planes==1:
42 |                 ops = default_ops()
43 |                 ops["nchannels"] = 2
44 |                 ops["look_one_level_down"] = False
45 |                 ops["do_registration"] = 2
46 |                 ops["keep_movie_raw"] = True
47 |                 ops["align_by_chan"] = 2
48 |                 ops["nonrigid"] = False
49 |                 ops["connected"] = False
50 |                 ops["max_overlap"] = 0.2
51 |                 ops["bidi_corrected"] =  True
52 |                 ops["two_step_reigstration"] = True
53 |                 ops["sparse_mode"] = True
54 |                 try:
55 |                     run_s2p(ops=ops,db=db)
56 |                 except Exception as e:
57 |                     print(f"exception {e} raised at path {path} in response to run s2p call")
58 |                     print(f"db file:")
59 |                     for key,value in db.items():
60 |                         print(f"{key}: {value}")
61 |                     print(f"ops file:")
62 |                     for key,value in ops.items():
63 |                         print(f"{key}: {value}")
64 |         else: 
65 |             print(f"No TIFFS at {path}" if infer_from_recording_classes else f"{path} not yet processed.")
66 |             return
67 |     else:
68 |         if not reprocess:
69 |              print(f"{path} has already been processed by suite2p")
70 |              return
71 |         run_s2p(ops=ops,db=db)
72 | 
73 | 
74 | 
75 | def no_of_planes(experiment_path, infer_from_recording_class=False,
76 |                  inferrer = None):
77 |     '''
78 |     Returns the number of planes in a preprocessed suite2p folder.
79 | 
80 |     Parameters
81 |     ----------
82 |     experiment_path : string
83 |         The path to a recording directory.
84 | 
85 |     Returns
86 |     -------
87 |     count : int
88 |         Returns the number of directories named plane* contained in 
89 |         experiment_path. If experiment_path does not exist, returns 0.
90 | 
91 |     '''
92 |     #I'm sure there's an elegant way to do this with regex
93 |     #or something
94 |     count = 0
95 |     try:
96 |         for file in os.listdir(os.path.join(experiment_path,"suite2p")):
97 |             if file[:-1] == "plane":
98 |                 count +=1
99 |     except NotADirectoryError:
100|         if infer_from_recording_class and inferrer != None:
101|             raise NotImplementedError
102|             exp_id(experiment_path)
103|             count = inferrer(exp_id)
104|         else:
105|             count = 0
106|     
107| 
108| class PlaneNumberInferrer:
109|     def __init__(self):
110|         recording_classes, df, planes = get_classes_of_recording(root = "H:\\")
111|         what_class_is_this_recording = {}
112|         plane_number_idxed_by_rec_class = []
113|         for idx,(attribute_values,ls_of_recordings) in enumerate(recording_classes.items()):
114|             plane_values = []
115|             for r in ls_of_recordings:
116|                 what_class_is_this_recording[r] = idx
117|                 if planes[r]!=0:
118|                     plane_values.append(planes[r])
119|                 else:
120|                     planes.pop(r)
121|             try:
122|                 plane_number_idxed_by_rec_class.append(mode(plane_values))
123|             except StatisticsError:
124|                 #guess 1
125|                 plane_number_idxed_by_rec_class.append(0)
126|         self.explicit_values = planes
127|         self.recording_classifier = what_class_is_this_recording
128|         self.plane_number = plane_number_idxed_by_rec_class
129|             
130|     def __call__(self,recording):
131|         try:
132|             return self.explicit_values[recording]
133|         except KeyError:
134|             try:
135|                 cls = self.recording_classifier[recording]
136|                 plane_num = self.plane_number[cls]
137|                 return plane_num
138|             except KeyError:
139|                 return 0
140|             
141| 
142| 
143| if __name__ == "__main__":
144|     drive = "H:\\"
145|     inferrer = PlaneNumberInferrer()
146|     run_s2p_on_no_reprocessing = lambda path:run_s2p_on(path,reprocess=False,
147|                                                         infer_from_recording_classes=True,
148|                                                         inferrer = inferrer
149|                                                         )
150|     #Do this to every one-plane experiment:
151|     apply_to_all_recordings(drive, run_s2p_on_no_reprocessing)
152| 
153|     


\Suite2p\get_tiff_from_binary.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Wed Mar  4 14:02:15 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | import skimage.external.tifffile as tif
8  | import numpy as np
9  | import os
10 | 
11 | 
12 | _res = 256*256 #Assumed to always be the resolution of these tiff files
13 | 
14 | def extract_interleaved_frames(fds, save_path,n_frames,offset=0,verbose=False):
15 |     '''
16 |     MUST TEST OFFSET FUNCTIONALITY
17 | 
18 |     Parameters
19 |     ----------
20 |     fds : list
21 |         list of numpy.memmap instances.
22 |     save_path : TYPE
23 |         DESCRIPTION.
24 |     n_frames : TYPE
25 |         DESCRIPTION.
26 |     offset : TYPE, optional
27 |         DESCRIPTION. The default is 0.
28 |     verbose : TYPE, optional
29 |         DESCRIPTION. The default is False.
30 | 
31 |     Returns
32 |     -------
33 |     None.
34 | 
35 |     '''
36 |     stack = []
37 |     if verbose:
38 |         print("reading...",end='', flush=True)
39 |     for frame in range(n_frames):
40 |         source = fds[frame%len(fds)]
41 |         start_loc = ((offset+frame)//len(fds))*_res
42 |         stack.append(source[start_loc:start_loc+_res].reshape(1,256,256))
43 |     res = np.stack(stack)
44 |     if verbose:
45 |         print("writing...", end='', flush=True)
46 |     tif.imsave(save_path,
47 |                res)
48 |     if verbose:
49 |         print('done.')
50 | 
51 | def extract_all_frames(read_paths,target_directory, verbose=False):
52 |     '''
53 |     Extract all frames from a set of suite2p binary files and dumps them as 1000-frame
54 |     (maximum) tiff files in a target directory. The files are named 'data0.tif' 
55 |     through to 'dataN.tif'. Frames from multiple read_paths are interleaved in
56 |     the ordering of paths in read_paths. If files contain different numbers of
57 |     frames, only the frames up to the minimum length of the binary files are
58 |     processed and included.
59 | 
60 |     Parameters
61 |     ----------
62 |     read_path : list of str
63 |         The paths to the suite2p binary files to read.
64 |     target_directory : str
65 |         The path to the directory to contain the tif files.
66 |     verbose : bool, optional
67 |         Whether to print operations to stdout. The default is False.
68 | 
69 |     Returns
70 |     -------
71 |     None.
72 | 
73 |     '''
74 |     orig_wd = os.getcwd()
75 |     try:
76 |         os.chdir(target_directory)
77 |     except FileNotFoundError:
78 |         os.makedirs(target_directory)
79 |         os.chdir(target_directory)
80 |     frame_size = 2*_res
81 |     frames_in_each_file = list(os.path.getsize(x)/frame_size for x in read_paths)
82 |     frames_in_file = min(frames_in_each_file)
83 |     del frame_size, frames_in_each_file
84 |     frames_read = 0
85 |     i = 0
86 |     fds = []
87 |     for idx,read_path in enumerate(read_paths):
88 |         fds.append(np.memmap(read_path,
89 |                              dtype = np.int16,
90 |                              mode = 'r'))
91 |     while frames_read < (frames_in_file - 1000):
92 |         #Extract a 1000-frame tiff
93 |         if verbose:
94 |             print("Extracting a 1000 frame tiff...", end="", flush=True)
95 |         extract_interleaved_frames(fds,
96 |                          save_path = f"data{i}.tif",
97 |                          n_frames = 1000,
98 |                          offset = frames_read,
99 |                          verbose = verbose)
100|         frames_read+=1000
101|         i+=1
102|     if frames_read!=frames_in_file:
103|         #Extract all remaining frames
104|         if verbose:
105|             print(
106|                 f"Exctracting all remaining {frames_in_file-frames_read} frames...",
107|                 end='',
108|                 flush=True
109|                 )
110|         extract_interleaved_frames(fds,
111|                          save_path = f"data{i}.tif",
112|                          n_frames = int(frames_in_file-frames_read),
113|                          offset = frames_read,
114|                          verbose = verbose)
115|     os.chdir(orig_wd)
116| 
117| 
118| def extract_n_frames_from_fd(fd, save_path,n_frames,offset=0, verbose = False):
119|     '''
120|     Extracts frame data from a numpy binary file and dumps it as a tiff file.
121| 
122|     Parameters
123|     ----------
124|     fd : numpy.memmap instance
125|         A memory mapping of  suite2p binary file from which to read.
126|     save_path : str
127|         Path to the tiff file.
128|     n_frames : int
129|         Number of frames to include in the tiff file.
130|     offset : int, optional
131|         The frame from which to begin extraction. The default is 0.
132|     verbose : bool, optional
133|         Whether to print operations to stdout. The default is False.
134| 
135|     Returns
136|     -------
137|     None.
138| 
139|     '''
140|     if verbose:
141|         print("reading...",end='', flush=True)
142|     ims = fd[(offset*_res):((offset + n_frames)*_res)]
143|     length = ims.shape[0]
144|     frames = length//_res
145|     ims = ims.reshape((frames,256,256))
146|     if verbose:
147|         print("writing...", end='', flush=True)
148|     tif.imsave(save_path,
149|            ims)
150|     if verbose:
151|         print('done.')
152| 
153| 
154| 
155| # if __name__=='__main__':
156| #     save_path = "C:\\Users\\uic\\Desktop\\tiff_files"
157| #     PATH = "D:\\Local_Repository\\CFEB026\\2016-09-21_05_CFeb026\\suite2p\\plane0\\data_raw.bin"
158| #     extract_all_frames(PATH,save_path, verbose=True)


\Suite2p\overwrite_iscells.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | from get_metrics import StatisticExtractor
3  | from automate_s2p import apply_to_all_one_plane_recordings
4  | import os
5  | 
6  | def overwrite_iscell(exp_path):
7  |     path = os.path.join(exp_path,'suite2p','plane0')
8  |     se = StatisticExtractor(path)
9  |     se._overwrite_iscell()
10 | 
11 | if __name__=="__main__":
12 |     apply_to_all_one_plane_recordings("E:\\", overwrite_iscell)


\Suite2p\to_interleaved_tiffs.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Fri Mar  6 11:11:05 2020
3  | 
4  | @author: uic
5  | """
6  | from os import listdir
7  | from os.path import join, isfile
8  | from acc_path_tools import get_all_files_with_name
9  | from get_tiff_from_binary import extract_all_frames
10 | 
11 | def collect_planes_in(path):
12 |     res = []
13 |     val = join(path,'suite2p')
14 |     for folder in listdir(val):
15 |         if folder[:-1] == 'plane': 
16 |             target = join(val,folder)
17 |             raw = join(target,'raw_data.bin')
18 |             nonraw = join(target,'data.bin')
19 |             if isfile(raw):
20 |                 res.append(raw)
21 |             elif isfile(nonraw):
22 |                 res.append(nonraw)
23 |             else:
24 |                 raise FileNotFoundError(
25 |                     f'No data found in {target}')
26 |     return res
27 |                 
28 | 
29 | def collect_channels_in(path):
30 |     res = []
31 |     res += collect_planes_in(path)
32 |     for folder in listdir(path):
33 |         if folder[:-1]=='cd':
34 |             res += collect_planes_in(folder)
35 |     return res
36 | 
37 | 
38 | def main(read_path,write_path,verbose=True):
39 |     res = get_all_files_with_name(read_path, name='data.bin')
40 |     exps = set(list(zip(*res))[1])
41 |     for experiment in exps:
42 |         print(experiment)
43 |         paths = collect_channels_in(experiment)
44 |         print(paths)
45 |         extract_all_frames(paths, write_path, verbose=verbose)
46 | 
47 | 
48 | 
49 | if __name__=='__main__':
50 |     read_path = 'G:\\Local_Repository'
51 |     write_path = 'D:\\TIFF_FILES'
52 |     main(read_path,write_path)
53 |     


\Suite2p\to_tiffs.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Mar  5 13:57:51 2020
3  | 
4  | @author: uic
5  | """
6  | from acc_path_tools import get_all_files_with_name
7  | from get_tiff_from_binary import extract_all_frames
8  | from os.path import split,join
9  | 
10 | def get_path_after_str(path,strng):
11 |     head, tail = split(path)
12 |     if head == strng:
13 |         return tail
14 |     if head == "":
15 |         raise AttributeError()
16 |     else:
17 |         try:
18 |             return join(get_path_after_str(head,strng),tail)
19 |         except AttributeError:
20 |             raise AttributeError(f"Failed to split {strng} off {path}")
21 | 
22 | def main(read_dir, write_dir):
23 |     all_files = get_all_files_with_name(read_dir, name='data.bin')
24 |     _,_,files=zip(*all_files)
25 | 
26 |     for idx, file in enumerate(files):
27 |         directory,justfile = split(file)
28 |         relpath = get_path_after_str(directory,read_dir)
29 |         new_path = join(write_dir,relpath)
30 |         print(f"Extracting file {idx+1} of {len(files)}")
31 |         extract_all_frames(
32 |             read_path = file,
33 |             target_directory = new_path,
34 |             verbose = True)
35 | 
36 | 
37 | if __name__=="__main__":
38 |     READ_PATH = 'G:\\Local_Repository'
39 |     WRITE_PATH = 'D:\\TIFF_FILES'
40 |     main(READ_PATH,WRITE_PATH)
41 |     


\Suite2p\untitled0.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Aug 11 11:36:41 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 


\Suite2p\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\Summaries\classwise_summary.txt
0  | CLASS 0 with pooled stats (Count=12292.0, Acc=76%, Sen=71%, Spe=85%, D'=1.6, test=5%, left=50%, go=52%)
1  | Recordings in this class have trials that vary across the following parameters:
2  |     contrast=1.0
3  |     go
4  |           0.0 (Count=5853.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=50%, go=0%)
5  |           1.0 (Count=6439.0, Acc=90%, Sen=100%, Spe=0%, D'=NA, test=9%, left=50%, go=100%)
6  |     side
7  |         left  (Count=6147.0, Acc=77%, Sen=72%, Spe=86%, D'=1.7, test=5%, left=100%, go=52%)
8  |         right (Count=6145.0, Acc=75%, Sen=70%, Spe=84%, D'=1.5, test=5%, left=0%, go=52%)
9  |     task=bGoNoGoLickAdapt
10 |     test
11 |             0 (Count=11699.0, Acc=76%, Sen=70%, Spe=87%, D'=1.6, test=0%, left=50%, go=50%)
12 |             1 (Count=593.0, Acc=83%, Sen=100%, Spe=0%, D'=NA, test=100%, left=50%, go=100%)
13 | This class contained the following 45 recordings:
14 |     2016-05-31_02_CFEB013 (1 plane)    2016-06-03_02_CFEB013 (? plane)    2016-05-27_02_CFEB014 (? plane)
15 |     2016-05-28_02_CFEB014 (1 plane)    2016-09-21_05_CFeb026 (1 plane)    2016-09-23_02_CFEB026 (1 plane)
16 |     2016-09-25_01_CFEB026 (1 plane)    2016-09-30_01_CFEB027 (1 plane)    2016-09-30_02_CFEB027 (? plane)
17 |     2016-10-05_01_CFEB027 (? plane)    2016-10-06_05_CFEB027 (? plane)    2016-10-07_03_CFEB027 (1 plane)
18 |     2016-10-10_04_CFEB027 (1 plane)    2016-10-11_05_CFEB027 (1 plane)    2016-10-13_03_CFEB027 (1 plane)
19 |     2016-10-14_03_CFEB027 (1 plane)    2016-10-17_03_CFEB027 (1 plane)    2016-10-18_01_CFEB027 (1 plane)
20 |     2016-10-21_03_CFEB027 (1 plane)    2016-10-22_03_CFEB027 (1 plane)    2016-10-24_03_CFEB027 (1 plane)
21 |     2016-10-25_04_CFEB027 (1 plane)    2016-10-05_01_CFEB029 (1 plane)    2016-10-06_01_CFEB029 (1 plane)
22 |     2016-10-09_03_CFEB029 (1 plane)    2016-10-10_03_CFEB029 (1 plane)    2016-10-13_02_CFEB029 (1 plane)
23 |     2016-10-14_01_CFEB029 (1 plane)    2016-10-15_01_CFEB029 (1 plane)    2016-10-17_03_CFEB029 (1 plane)
24 |     2016-10-19_03_CFEB029 (1 plane)    2016-10-21_03_CFEB029 (1 plane)    2016-10-24_03_CFEB029 (1 plane)
25 |     2016-10-25_03_CFEB029 (1 plane)    2016-10-26_03_CFEB029 (1 plane)    2016-11-26_01_CFEB033 (1 plane)
26 |     2016-12-09_03_CFEB033 (1 plane)    2016-12-15_01_CFEB033 (1 plane)    2016-12-16_01_CFEB033 (1 plane)
27 |     2016-12-17_01_CFEB033 (1 plane)    2016-12-18_01_CFEB033 (1 plane)    2016-12-19_01_CFEB033 (1 plane)
28 |     2016-12-09_01_CFEB035 (1 plane)    2016-12-16_01_CFEB035 (1 plane)    2016-12-17_01_CFEB035 (1 plane)
29 | 
30 | 
31 | 
32 | CLASS 1 with pooled stats (Count=4107.0, Acc=71%, Sen=67%, Spe=78%, D'=1.2, test=25%, left=50%, go=50%)
33 | Recordings in this class have trials that vary across the following parameters:
34 |     contrast
35 |           0.1 (Count=2053.0, Acc=66%, Sen=65%, Spe=66%, D'=0.8, test=0%, left=50%, go=50%)
36 |           0.5 (Count=2054.0, Acc=77%, Sen=69%, Spe=96%, D'=2.2, test=50%, left=50%, go=50%)
37 |     go
38 |           0.0 (Count=2055.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=25%, left=50%, go=0%)
39 |           1.0 (Count=2052.0, Acc=83%, Sen=100%, Spe=0%, D'=NA, test=25%, left=50%, go=100%)
40 |     side
41 |         left  (Count=2056.0, Acc=71%, Sen=68%, Spe=78%, D'=1.2, test=50%, left=100%, go=50%)
42 |         right (Count=2051.0, Acc=71%, Sen=67%, Spe=78%, D'=1.2, test=0%, left=0%, go=50%)
43 |     task=bGoNoGoLickFull
44 |     test
45 |             0 (Count=3079.0, Acc=69%, Sen=66%, Spe=73%, D'=1.0, test=0%, left=33%, go=50%)
46 |             1 (Count=1028.0, Acc=79%, Sen=70%, Spe=97%, D'=2.5, test=100%, left=100%, go=50%)
47 | This class contained the following 15 recordings:
48 |     2016-06-29_02_CFEB013 (1 plane)    2016-07-05_03_CFEB014 (1 plane)    2016-09-29_06_CFEB026 (1 plane)
49 |     2016-10-11_03_CFEB026 (1 plane)    2016-10-22_01_CFEB026 (1 plane)    2016-10-27_01_CFEB027 (1 plane)
50 |     2016-11-01_03_CFEB027 (1 plane)    2016-11-05_03_CFEB027 (1 plane)    2016-11-07_03_CFEB027 (1 plane)
51 |     2016-11-03_03_CFEB029 (1 plane)    2016-11-04_05_CFEB029 (1 plane)    2016-11-05_03_CFEB029 (1 plane)
52 |     2016-11-07_03_CFEB029 (1 plane)    2016-11-08_03_CFEB029 (1 plane)    2016-11-09_04_CFEB029 (1 plane)
53 | 
54 | 
55 | 
56 | CLASS 2 with pooled stats (Count=897.0, Acc=77%, Sen=71%, Spe=88%, D'=1.7, test=37%, left=50%, go=50%)
57 | Recordings in this class have trials that vary across the following parameters:
58 |     contrast=1.0
59 |     go
60 |           0.0 (Count=449.0, Acc=62%, Sen=0%, Spe=100%, D'=NA, test=37%, left=50%, go=0%)
61 |           1.0 (Count=448.0, Acc=91%, Sen=100%, Spe=0%, D'=NA, test=37%, left=50%, go=100%)
62 |     side
63 |         left  (Count=447.0, Acc=84%, Sen=79%, Spe=89%, D'=2.1, test=75%, left=100%, go=50%)
64 |         right (Count=450.0, Acc=70%, Sen=64%, Spe=85%, D'=1.4, test=0%, left=0%, go=50%)
65 |     task=bGoNoGoLickFull
66 |     test
67 |             0 (Count=562.0, Acc=72%, Sen=66%, Spe=86%, D'=1.5, test=0%, left=20%, go=50%)
68 |             1 (Count=335.0, Acc=85%, Sen=81%, Spe=89%, D'=2.1, test=100%, left=100%, go=50%)
69 | This class contained the following 3 recordings:
70 |     2016-11-13_03_CFEB027 (? plane)    2016-11-12_03_CFEB029 (? plane)    2016-11-13_03_CFEB029 (? plane)
71 | 
72 | 
73 | 
74 | CLASS 3 with pooled stats (Count=1038.0, Acc=84%, Sen=100%, Spe=0%, D'=NA, test=27%, left=37%, go=100%)
75 | Recordings in this class have trials that vary across the following parameters:
76 |     contrast
77 |           0.0 (Count=657.0, Acc=98%, Sen=100%, Spe=0%, D'=NA, test=0%, left=0%, go=100%)
78 |           1.0 (Count=381.0, Acc=59%, Sen=100%, Spe=0%, D'=NA, test=74%, left=100%, go=100%)
79 |     go=1.0
80 |     side
81 |         left  (Count=381.0, Acc=59%, Sen=100%, Spe=0%, D'=NA, test=74%, left=100%, go=100%)
82 |         right (Count=657.0, Acc=98%, Sen=100%, Spe=0%, D'=NA, test=0%, left=0%, go=100%)
83 |     task=bGoNoGoLickAdaptOne
84 |     test
85 |             0 (Count=756.0, Acc=93%, Sen=100%, Spe=0%, D'=NA, test=0%, left=13%, go=100%)
86 |             1 (Count=282.0, Acc=58%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
87 | This class contained the following 5 recordings:
88 |     2017-01-15_01_CFEB037 (1 plane)    2017-01-15_01_CFEB040 (1 plane)    2017-03-09_02_CFEB042 (1 plane)
89 |     2017-03-09_02_CFEB044 (1 plane)    2017-03-23_04_CFEB045 (1 plane)
90 | 
91 | 
92 | CLASS 4 with pooled stats (Count=7494.0, Acc=80%, Sen=76%, Spe=85%, D'=1.7, test=3%, left=100%, go=52%)
93 | Recordings in this class have trials that vary across the following parameters:
94 |     contrast=1.0
95 |     go
96 |           0.0 (Count=3600.0, Acc=70%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
97 |           1.0 (Count=3894.0, Acc=88%, Sen=100%, Spe=0%, D'=NA, test=6%, left=100%, go=100%)
98 |     side=left
99 |     task=bGoNoGoLickAdaptOne
100|     test
101|             0 (Count=7275.0, Acc=80%, Sen=75%, Spe=86%, D'=1.8, test=0%, left=100%, go=51%)
102|             1 (Count=219.0, Acc=76%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
103| This class contained the following 31 recordings:
104|     2017-01-17_01_CFEB037 (1 plane)    2017-01-27_01_CFEB037 (1 plane)    2017-01-28_01_CFEB037 (1 plane)
105|     2017-01-30_01_CFEB037 (1 plane)    2017-02-08_02_CFEB037 (1 plane)    2017-02-09_01_CFEB037 (1 plane)
106|     2017-02-13_05_CFEB037 (1 plane)    2017-02-14_01_CFEB037 (1 plane)    2017-01-23_01_CFEB040 (1 plane)
107|     2017-01-27_01_CFEB040 (1 plane)    2017-02-08_01_CFEB040 (1 plane)    2017-02-09_01_CFEB040 (1 plane)
108|     2017-02-13_01_CFEB040 (1 plane)    2017-01-15_01_CFEB041 (1 plane)    2017-01-17_01_CFEB041 (1 plane)
109|     2017-01-28_01_CFEB041 (1 plane)    2017-01-30_01_CFEB041 (1 plane)    2017-01-31_01_CFEB041 (1 plane)
110|     2017-02-08_01_CFEB041 (1 plane)    2017-02-09_01_CFEB041 (1 plane)    2017-02-14_01_CFEB041 (1 plane)
111|     2017-03-17_01_CFEB042 (1 plane)    2017-03-20_01_CFEB042 (1 plane)    2017-03-28_01_CFEB042 (1 plane)
112|     2017-03-29_01_CFEB042 (1 plane)    2017-03-30_01_CFEB042 (1 plane)    2017-03-23_01_CFEB044 (1 plane)
113|     2017-03-28_01_CFEB045 (1 plane)    2017-03-29_01_CFEB045 (1 plane)    2017-03-30_01_CFEB045 (1 plane)
114|     2017-04-03_01_CFEB045 (1 plane)
115| 
116| 
117| CLASS 5 with pooled stats (Count=575.0, Acc=NA, Sen=NA, Spe=NA, D'=NA, test=100%, left=NA, go=0%)
118| Recordings in this class have trials that vary across the following parameters:
119|     contrast=Unspecified
120|     go=Unspecified
121|     side=unknown
122|     task=DriftGrating1
123|     test=True
124| This class contained the following 21 recordings:
125|     2017-02-14_02_CFEB037 (? plane)    2017-02-04_02_CFEB040 (? plane)    2017-02-13_02_CFEB040 (? plane)
126|     2017-02-14_02_CFEB041 (? plane)    2017-03-23_02_CFEB042 (? plane)    2017-03-28_04_CFEB042 (? plane)
127|     2017-03-29_02_CFEB042 (? plane)    2017-03-30_02_CFEB042 (? plane)    2017-03-23_02_CFEB044 (? plane)
128|     2017-03-30_02_CFEB045 (? plane)    2018-04-26_03_CFEB105 (? plane)    2018-04-26_02_CFEB106 (? plane)
129|     2018-06-01_02_CFEB107 (? plane)    2018-06-01_02_CFEB108 (? plane)    2018-10-03_03_CFEB127 (? plane)
130|     2018-10-04_03_CFEB127 (5 plane)    2018-10-04_05_CFEB128 (5 plane)    2018-10-03_02_CFEB130 (? plane)
131|     2018-10-04_03_CFEB130 (5 plane)    2018-11-03_03_CFEB133 (5 plane)    2018-11-03_03_CFEB134 (5 plane)
132| 
133| 
134| 
135| CLASS 6 with pooled stats (Count=142422.0, Acc=NA, Sen=NA, Spe=NA, D'=NA, test=1%, left=NA, go=0%)
136| Recordings in this class have trials that vary across the following parameters:
137|     contrast=Unspecified
138|     go=Unspecified
139|     side=unknown
140|     task=DriftGrating1
141|     test
142|             0 (Count=141084.0, Acc=NA, Sen=NA, Spe=NA, D'=NA, test=0%, left=NA, go=0%)
143|             1 (Count=1338.0, Acc=NA, Sen=NA, Spe=NA, D'=NA, test=100%, left=NA, go=0%)
144| This class contained the following 50 recordings:
145|     2017-03-07_01_CFEB042 (? plane)    2018-04-17_01_CFEB105 (5 plane)    2018-04-19_01_CFEB105 (5 plane)
146|     2018-04-24_03_CFEB105 (5 plane)    2018-04-16_01_CFEB106 (? plane)    2018-04-16_02_CFEB106 (? plane)
147|     2018-04-16_03_CFEB106 (? plane)    2018-04-17_03_CFEB106 (5 plane)    2018-04-19_01_CFEB106 (5 plane)
148|     2018-04-23_01_CFEB106 (5 plane)    2018-04-24_02_CFEB106 (5 plane)    2018-05-23_03_CFEB107 (5 plane)
149|     2018-05-24_01_CFEB107 (5 plane)    2018-05-29_01_CFEB107 (5 plane)    2018-05-23_05_CFEB108 (5 plane)
150|     2018-05-24_01_CFEB108 (5 plane)    2018-05-29_01_CFEB108 (5 plane)    2018-06-16_03_CFEB125 (? plane)
151|     2018-06-16_04_CFEB125 (5 plane)    2018-06-17_01_CFEB126 (5 plane)    2018-06-17_02_CFEB126 (5 plane)
152|     2018-09-18_01_CFEB127 (? plane)    2018-09-18_02_CFEB127 (? plane)    2018-09-19_02_CFEB127 (? plane)
153|     2018-09-19_03_CFEB127 (? plane)    2018-09-26_01_CFEB127 (5 plane)    2018-09-26_02_CFEB127 (5 plane)
154|     2018-09-27_01_CFEB127 (? plane)    2018-09-27_02_CFEB127 (? plane)    2018-10-01_01_CFEB127 (? plane)
155|     2018-10-15_01_CFEB127 (? plane)    2018-09-27_01_CFEB128 (5 plane)    2018-09-27_02_CFEB128 (5 plane)
156|     2018-09-17_01_CFEB130 (? plane)    2018-09-17_02_CFEB130 (? plane)    2018-09-18_01_CFEB130 (? plane)
157|     2018-09-18_02_CFEB130 (? plane)    2018-09-18_03_CFEB130 (? plane)    2018-09-18_04_CFEB130 (? plane)
158|     2018-09-18_05_CFEB130 (? plane)    2018-09-18_06_CFEB130 (? plane)    2018-09-18_07_CFEB130 (? plane)
159|     2018-09-25_01_CFEB130 (5 plane)    2018-09-25_02_CFEB130 (5 plane)    2018-10-01_01_CFEB130 (? plane)
160|     2018-10-01_02_CFEB130 (? plane)    2018-10-31_07_CFEB133 (5 plane)    2018-10-31_08_CFEB133 (5 plane)
161|     2018-11-01_03_CFEB134 (5 plane)    2018-11-01_05_CFEB134 (5 plane)
162| 
163| 
164| CLASS 7 with pooled stats (Count=808.0, Acc=89%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
165| Recordings in this class have trials that vary across the following parameters:
166|     contrast=0.0
167|     go=1.0
168|     side=left
169|     task=bDetect
170|     test=False
171| This class contained the following 15 recordings:
172|     2018-04-26_01_CFEB105 (? plane)    2018-04-26_01_CFEB106 (? plane)    2018-06-01_01_CFEB107 (? plane)
173|     2018-06-01_01_CFEB108 (? plane)    2018-10-03_02_CFEB127 (? plane)    2018-10-04_01_CFEB127 (? plane)
174|     2018-10-04_02_CFEB127 (5 plane)    2018-10-05_01_CFEB127 (? plane)    2018-10-04_04_CFEB128 (5 plane)
175|     2018-10-03_01_CFEB130 (? plane)    2018-10-04_01_CFEB130 (? plane)    2018-10-04_02_CFEB130 (5 plane)
176|     2018-10-05_01_CFEB130 (? plane)    2018-11-03_02_CFEB133 (5 plane)    2018-11-03_02_CFEB134 (5 plane)
177| 
178| 
179| 
180| CLASS 8 with pooled stats (Count=1807.0, Acc=89%, Sen=86%, Spe=94%, D'=2.7, test=54%, left=100%, go=54%)
181| Recordings in this class have trials that vary across the following parameters:
182|     contrast
183|           0.0 (Count=838.0, Acc=81%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
184|           1.0 (Count=969.0, Acc=96%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
185|     go
186|           0.0 (Count=838.0, Acc=81%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
187|           1.0 (Count=969.0, Acc=96%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
188|     side=left
189|     task=bDetect
190|     test
191|             0 (Count=838.0, Acc=81%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
192|             1 (Count=969.0, Acc=96%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
193| This class contained the following 18 recordings:
194|     2018-05-15_01_CFEB106 (? plane)    2018-10-08_02_CFEB127 (? plane)    2018-10-09_01_CFEB127 (? plane)
195|     2018-10-10_01_CFEB127 (? plane)    2018-10-11_01_CFEB127 (5 plane)    2018-10-12_01_CFEB127 (? plane)
196|     2018-10-15_04_CFEB127 (5 plane)    2018-10-09_02_CFEB128 (5 plane)    2018-10-10_01_CFEB128 (5 plane)
197|     2018-10-09_01_CFEB130 (? plane)    2018-10-10_01_CFEB130 (? plane)    2018-10-11_01_CFEB130 (5 plane)
198|     2018-10-12_01_CFEB130 (? plane)    2018-10-15_01_CFEB130 (5 plane)    2018-11-14_01_CFEB133 (5 plane)
199|     2018-11-16_01_CFEB133 (5 plane)    2018-11-12_01_CFEB134 (5 plane)    2018-11-13_01_CFEB134 (5 plane)
200| 
201| 
202| 
203| CLASS 9 with pooled stats (Count=8.0, Acc=0%, Sen=NA, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
204| Recordings in this class have trials that vary across the following parameters:
205|     contrast=0.0
206|     go=1.0
207|     side=left
208|     task=bDetect
209|     test=False
210| This class contained the following 1 recordings:
211|     2018-10-03_01_CFEB127 (? plane)
212| 
213| 
214| CLASS 10 with pooled stats (Count=228.0, Acc=57%, Sen=100%, Spe=0%, D'=NA, test=100%, left=100%, go=100%)
215| Recordings in this class have trials that vary across the following parameters:
216|     contrast=1.0
217|     go=1.0
218|     side=left
219|     task=bDetect
220|     test=True
221| This class contained the following 6 recordings:
222|     2018-10-05_02_CFEB127 (5 plane)    2018-10-05_03_CFEB128 (5 plane)    2018-10-05_02_CFEB130 (5 plane)
223|     2018-10-07_02_CFEB130 (? plane)    2018-10-08_01_CFEB130 (? plane)    2018-11-05_02_CFEB134 (5 plane)
224| 
225| 
226| 
227| CLASS 11 with pooled stats (Count=4.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
228| Recordings in this class have trials that vary across the following parameters:
229|     contrast=1.0
230|     go=1.0
231|     side=left
232|     task=bDetect
233|     test=True
234| This class contained the following 1 recordings:
235|     2018-10-15_02_CFEB127 (? plane)
236| 
237| 
238| CLASS 12 with pooled stats (Count=16.0, Acc=100%, Sen=100%, Spe=100%, D'=inf, test=81%, left=100%, go=81%)
239| Recordings in this class have trials that vary across the following parameters:
240|     contrast
241|           0.0 (Count=3.0, Acc=100%, Sen=NA, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
242|           0.5 (Count=3.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
243|           1.0 (Count=10.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
244|     go
245|           0.0 (Count=3.0, Acc=100%, Sen=NA, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
246|           1.0 (Count=13.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
247|     side=left
248|     task=bDetect
249|     test
250|             0 (Count=3.0, Acc=100%, Sen=NA, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
251|             1 (Count=13.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
252| This class contained the following 2 recordings:
253|     2018-10-16_01_CFEB127 (? plane)    2018-10-16_02_CFEB127 (? plane)
254| 
255| 
256| CLASS 13 with pooled stats (Count=84.0, Acc=76%, Sen=83%, Spe=60%, D'=1.2, test=19%, left=100%, go=70%)
257| Recordings in this class have trials that vary across the following parameters:
258|     contrast
259|           0.0 (Count=25.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
260|         0.007 (Count=6.0, Acc=0%, Sen=NA, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
261|         0.015 (Count=6.0, Acc=33%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
262|          0.03 (Count=6.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=0%, left=100%, go=100%)
263|          0.06 (Count=6.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=0%, left=100%, go=100%)
264|         0.125 (Count=6.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=0%, left=100%, go=100%)
265|          0.25 (Count=6.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=0%, left=100%, go=100%)
266|           0.5 (Count=13.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=46%, left=100%, go=100%)
267|           1.0 (Count=10.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
268|     go
269|           0.0 (Count=25.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
270|           1.0 (Count=59.0, Acc=83%, Sen=100%, Spe=0%, D'=NA, test=27%, left=100%, go=100%)
271|     side=left
272|     task=bDetect
273|     test
274|             0 (Count=68.0, Acc=71%, Sen=77%, Spe=60%, D'=1.0, test=0%, left=100%, go=63%)
275|             1 (Count=16.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
276| This class contained the following 1 recordings:
277|     2018-10-16_03_CFEB127 (? plane)
278| 
279| 
280| CLASS 14 with pooled stats (Count=421.0, Acc=67%, Sen=82%, Spe=47%, D'=0.8, test=17%, left=100%, go=69%)
281| Recordings in this class have trials that vary across the following parameters:
282|     contrast
283|           0.0 (Count=130.0, Acc=66%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
284|         0.005 (Count=31.0, Acc=32%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
285|          0.01 (Count=31.0, Acc=42%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
286|         0.015 (Count=31.0, Acc=32%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
287|          0.02 (Count=30.0, Acc=63%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
288|         0.025 (Count=32.0, Acc=62%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
289|          0.03 (Count=30.0, Acc=73%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
290|         0.035 (Count=33.0, Acc=82%, Sen=100%, Spe=0%, D'=NA, test=0%, left=100%, go=100%)
291|           0.5 (Count=33.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
292|           1.0 (Count=40.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
293|     go
294|           0.0 (Count=130.0, Acc=66%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
295|           1.0 (Count=291.0, Acc=67%, Sen=100%, Spe=0%, D'=NA, test=25%, left=100%, go=100%)
296|     side=left
297|     task=bDetect
298|     test
299|             0 (Count=348.0, Acc=59%, Sen=73%, Spe=47%, D'=0.5, test=0%, left=100%, go=63%)
300|             1 (Count=73.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
301| This class contained the following 4 recordings:
302|     2018-10-21_04_CFEB127 (5 plane)    2018-10-21_03_CFEB128 (5 plane)    2018-10-17_02_CFEB130 (? plane)
303|     2018-11-14_01_CFEB134 (5 plane)
304| 
305| 
306| CLASS 15 with pooled stats (Count=44.0, Acc=82%, Sen=75%, Spe=100%, D'=38.2, test=55%, left=100%, go=55%)
307| Recordings in this class have trials that vary across the following parameters:
308|     contrast
309|           0.0 (Count=20.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
310|           0.5 (Count=19.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
311|           1.0 (Count=5.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
312|     go
313|           0.0 (Count=20.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
314|           1.0 (Count=24.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
315|     side=left
316|     task=bDetect
317|     test
318|             0 (Count=20.0, Acc=60%, Sen=0%, Spe=100%, D'=NA, test=0%, left=100%, go=0%)
319|             1 (Count=24.0, Acc=100%, Sen=100%, Spe=NA, D'=NA, test=100%, left=100%, go=100%)
320| This class contained the following 1 recordings:
321|     2018-10-17_01_CFEB130 (? plane)
322| 
323| 


\Summaries\produce_trial_summary_document.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Jul 21 11:43:16 2020
3  | 
4  | @author: viviani
5  | 
6  | Create a summary dataframe and summary document detailing the composition
7  | of each recording session in terms of the trial attributes - eg how often
8  | mice were correct, their d-prime statistics, whether the side varied in the 
9  | trial, whether contrast varied in the trial, and so on...
10 | 
11 | Alternatively, create a summary document detailing the different KINDS of
12 | recording session and their POOLED statistics.
13 | 
14 | """
15 | import os
16 | from collections import namedtuple
17 | from contextlib import redirect_stdout
18 | 
19 | import pandas as pd
20 | 
21 | from accdatatools.Observations.trials import _get_trial_structs, SparseTrial
22 | from accdatatools.Utils.map_across_dataset import apply_to_all_recordings
23 | from accdatatools.Utils.convienience import item
24 | from accdatatools.Utils.path import get_exp_id, get_timeline_path, get_psychstim_path
25 | from accdatatools.Utils.map_across_dataset import no_of_planes
26 | from accdatatools.Utils.deeploadmat import loadmat
27 | from accdatatools.DataCleaning.determine_dprime import d_prime
28 | 
29 | class StatisticRepresenter:
30 |     def __init__(self,count,acc,sen,spe,d,test,leftside,go):
31 |         self.strings = []
32 |         for var in (count,acc,sen,spe,d,test,leftside,go):
33 |             if var in (count,d):
34 |                 self.strings.append(f"{var:.1f}" if var!=None else "NA")
35 |             else:
36 |                 self.strings.append(f"{100*var:.0f}%" if var!=None else "NA")
37 |     def __repr__(self):
38 |         return(f"(Count={self.strings[0]}, "+
39 |                f"Acc={self.strings[1]}, "+
40 |                f"Sen={self.strings[2]}, "+
41 |                f"Spe={self.strings[3]}, "+
42 |                f"D'={self.strings[4]}, "+
43 |                f"test={self.strings[5]}, "+
44 |                f"left={self.strings[6]}, "+
45 |                f"go={self.strings[7]})"
46 |                )
47 | 
48 | 
49 | class ParentedSparseTrial(SparseTrial):
50 |     '''
51 |     A SparseTrial Object that also knows in which recording it was collected,
52 |     and the type of that recording
53 |     '''
54 |     def __init__(self,struct,trial_type,exp_path,tolerant=False):
55 |         self.recording_id = get_exp_id(exp_path)
56 |         self.type=trial_type
57 |         super().__init__(struct,tolerant)
58 |     def to_dict(self):
59 |         result = super().to_dict()
60 |         result["recording_id"] = self.recording_id
61 |         result["task"] = self.type
62 |         return result
63 | 
64 | 
65 | def get_count_and_stats(dataframe,attribute='not_provided',value='not_provided', subsetting=True):
66 |     if subsetting:
67 |         if attribute=="not_provided" or value=="not_provided":
68 |             raise ValueError("Subset requested but no attribute/value pair provided")
69 |         else:
70 |             subset = dataframe[dataframe[attribute]==value]
71 |     else:
72 |         subset = dataframe
73 |     
74 |     count             = len(subset.index)
75 |     try: tests        = len(subset[subset.test==True].index)/count
76 |     except ZeroDivisionError: tests = None
77 |     try:gos           = len(subset[subset.go==True].index)/count
78 |     except ZeroDivisionError: gos = None
79 |     left              = len(subset[subset.side=='left'])
80 |     right             = len(subset[subset.side=='right'])
81 |     try:left          = left / (left+right)
82 |     except ZeroDivisionError: left = None
83 |     corrects          = subset[subset.correct==True]
84 |     incorrects        = subset[subset.correct==False]
85 |     
86 |     hit               = len(corrects[corrects.go==True].index)
87 |     miss              = len(incorrects[incorrects.go==True].index)
88 |     correct_rejection = len(corrects[corrects.go==False].index)
89 |     false_alarm       = len(incorrects[incorrects.go==False].index)
90 |     try: accurracy   = len(corrects.index) / (len(corrects.index) + len(incorrects.index))
91 |     except ZeroDivisionError: accurracy = None
92 |     try: sensitivity = hit / (hit + false_alarm)
93 |     except ZeroDivisionError: sensitivity = None
94 |     try: specificity = correct_rejection / (correct_rejection + miss)
95 |     except ZeroDivisionError: specificity = None
96 |     d = d_prime(sensitivity, (1-specificity)) if sensitivity and specificity else None
97 |     return StatisticRepresenter(count,accurracy,sensitivity,specificity,d,
98 |                                 tests,left,gos)
99 | 
100| 
101| 
102| def get_unique_attribute_values(df):
103|     result = {}
104|     for column in df.columns:
105|         unique_values = df[column].unique().astype("object")
106|         #nan values aren't equal to themselves, so we need to turn them into
107|         #something nice
108|         unique_values[pd.isna(unique_values)] = "Unspecified"
109|         result[column] = unique_values
110|     return result
111| 
112| 
113| 
114| 
115| def get_every_trial(root="D:\\"):
116|     ls = []
117|     planes = {}
118|     def func(path):
119|         path2 = get_psychstim_path(path)
120|         structs = _get_trial_structs(path2)
121|         psychstim = loadmat(path2)
122|         trial_type =  psychstim["expData"]["stim"]["stimType"]
123|         try:
124|             trial_objects = [ParentedSparseTrial(struct,trial_type,path,tolerant=True).to_dict() for struct in structs]
125|             ls.extend(trial_objects)
126|             plane = no_of_planes(path)
127|             planes[trial_objects[-1]["recording_id"]] = plane
128|         except AttributeError as e:
129|             print(f"{e} occured at {path}")
130|     apply_to_all_recordings(root, func)
131|     return ls, planes
132| 
133| def get_unique_trial_attrs_by_recording(root = "D:\\", return_df=False):
134|     trials, planes = get_every_trial(root = root)
135|     df = pd.DataFrame(trials)
136|     recordings = {}
137|     for recording in df.recording_id.unique():
138|         subset = df[df.recording_id==recording]
139|         subset = subset.loc[:,
140|                             ['recording_id', 'test', 'go', 'side', 
141|                              'correct', 'affirmative', 'contrast',
142|                              'task']]
143|         recordings[recording] = get_unique_attribute_values(subset)
144|     return (df,recordings,planes) if return_df else (recordings, planes)
145|         
146| 
147| def simple_summary():
148|     recordings, planes = get_unique_trial_attrs_by_recording()
149|     with open("C:/Users/viviani/Desktop/recording_descriptions.txt","w") as f:
150|         for (recording,value) in zip(planes,recordings.items()):
151|             f.write(f"{recording}\n")
152|             f.write(f"    {planes[recording]} plane{'s' if planes[recording]>1 else ''}\n")
153|             for attr, array in value.items():
154|                 if array.shape[0]>1:
155|                     f.write(f"    {attr:13}{list(array)}\n")
156|             f.write("\n\n")
157| 
158| 
159| def get_classes_of_recording(root="D:\\"):
160|     df, recordings, planes = get_unique_trial_attrs_by_recording(root,True)
161|     # recordings is a dict with keys of recording_ids and values of
162|     # the unique values of trial attribute (side, contrast, etc) that
163|     # occured in that trial.
164|     # We want the opposite: a mapping from the KIND of recording (ie the unique
165|     # trial attributes) to a LIST of recording_IDs. To do this we invert the
166|     # dictionary...
167|     recording_classes = {} 
168|       
169|     for recording, varying_attributes in recordings.items():
170|         #Drop the recording_id, else each recording class would be 1 recording!
171|         varying_attributes.pop("recording_id",None)
172|         #The problem is, tuples and arrays can't be keys in python, so we
173|         #convert varying_attributes, which is a dictionary with typing
174|         #dict{attribute::str -> occuring_values::ndarray of str}, to nested tuples,
175|         #ie tuple(attribute::str, occurring_values::tuple of str)
176|         varying_attributes = tuple(
177|                                 sorted(
178|                                     (k,
179|                                      tuple(sorted(v))
180|                                      ) for k,v in sorted(varying_attributes.items()))
181|                                     )
182|         if varying_attributes not in recording_classes: 
183|             recording_classes[varying_attributes] = [recording] 
184|         else: 
185|             recording_classes[varying_attributes].append(recording) 
186|     return recording_classes, df, planes
187| 
188| 
189| def classwise_summary(root = "D:\\"):
190|     recording_classes, df, planes = get_classes_of_recording(root=root)
191|     with open("classwise_summary.txt","w") as file:
192|         for idx,(attribute_values,ls_of_recordings) in enumerate(recording_classes.items()):
193|             file.write(f"CLASS {idx} ")
194|             subset = df[df.recording_id.isin(ls_of_recordings)]
195|             stats = get_count_and_stats(subset,subsetting=False)
196|             file.write(f"with pooled stats {stats}\n")
197|             file.write("Recordings in this class have trials that vary across the following parameters:\n")
198|             for attribute,unique_vals in attribute_values:
199|                 if attribute not in ("recording_id","correct","affirmative"):
200|                     if len(unique_vals)>1:
201|                         file.write(f"    {attribute}\n")
202|                         for value in unique_vals:
203|                             stats = get_count_and_stats(subset,attribute,value)
204|                             file.write(f"        {value:5} {stats}\n")
205|                     elif len(unique_vals)==1:
206|                         file.write(f"    {attribute}={unique_vals[0]}\n")
207|             file.write(f"This class contained the following {len(ls_of_recordings)} recordings:\n")
208|             for idx,r in enumerate(ls_of_recordings):
209|                 file.write(f"    {r} ({planes[r] if planes[r]!=0 else '?'} plane)")
210|                 if idx%3==2:
211|                     file.write("\n")
212|             file.write("\n\n\n")
213|                     
214| 
215| 
216| def full_summary():
217|     df, uniques,planes = get_unique_trial_attrs_by_recording(True)
218|     with open("fullsummary.txt","w") as file:
219|         for recording, attr_dict in uniques.items():
220|             file.write(f"{recording}\n")
221|             subset = df[df.recording_id==recording]
222|             for attribute,unique_vals in attr_dict.items():
223|                 if attribute not in ("recording_id","correct","affirmative"):
224|                     file.write(f"    {attribute}\n")
225|                     for value in unique_vals:
226|                         res = get_count_and_stats(subset,attribute,value)
227|                         file.write(f"        {value:5} {res}\n")
228|             file.write("\n\n")
229| 
230| 
231|         
232|     # df = pd.DataFrame(recordings)
233|     # #Drop all the timing information
234|     # df = df.loc[:,['recording_id', 'test', 'go', 'side', 'correct', 'affirmative', 'contrast']]
235|     
236| if __name__=="__main__":
237|     classwise_summary("H:\\")
238|     


\Summaries\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\TheoreticalVisualisation\figures_for_thesis_intro.py
0  | import numpy as np
1  | import seaborn
2  | import matplotlib.pyplot as plt
3  | import matplotlib.patches as patches
4  | import matplotlib.lines as lines
5  | import matplotlib.image as mpimg
6  | 
7  | seaborn.set_style("dark")
8  | 
9  | plt.rcParams["font.family"] = 'Times New Roman'
10 | plt.rcParams["font.size"] = 11
11 | 
12 | class Line:
13 |     '''
14 |     Encapsulates a linear function
15 |     and its inverse
16 |     '''
17 |     def __init__(self,m,c):
18 |         self.m = m
19 |         self.c = c
20 |     def __call__(self,x):
21 |         return self.m*x+self.c
22 |     def inverse(self):
23 |         return Line(1/self.m, -self.c/self.m)
24 | 
25 | 
26 | 
27 | 
28 | class RetinalReceptiveField:
29 |     gauss = staticmethod(
30 |         lambda r2,a,b:a*np.exp(-b*(r2))
31 |         )
32 |     def __init__(self,xy = (30,30),sigma = 10):
33 |         field = np.empty((100,100))
34 |         self.xy = xy
35 |         self.sigma = sigma
36 |         for (x,y),_ in np.ndenumerate(field):
37 |             field[x][y] = self.field_fn(x,y)
38 |         self.field = field
39 |     def field_fn(self,x,y):
40 |         _x = x - self.xy[0]
41 |         _y = y - self.xy[1]
42 |         r2 = (_x**2+_y**2) / ((self.sigma)**2)
43 |         return (10/3)*(self.gauss(r2,1,4)-self.gauss(r2,0.7,1))
44 |     
45 |     
46 |     def apply_to_axis(self, axis):
47 | 
48 |         axis.imshow(self.field,
49 |                     cmap = "gray",
50 |                     interpolation = "bicubic")
51 |         axis.set_yticks([])
52 |         axis.set_xticks([])
53 |         return axis
54 | 
55 | class Grating:
56 |     sine = staticmethod(
57 |         lambda x,y,a,theta:a*np.sin((np.cos(theta)*x + np.sin(theta)*y)/a)
58 |         )
59 |     def __init__(self,a = 10,theta = 1):
60 |         field = np.empty((100,100))
61 |         for (x,y),_ in np.ndenumerate(field):
62 |             field[x][y] = self.sine(x,y,a,theta)
63 |         self.field = field
64 |     def apply_to_axis(self, axis):
65 | 
66 |         axis.imshow(self.field,
67 |                     cmap = "gray",
68 |                     interpolation = "bicubic")
69 |         axis.set_yticks([])
70 |         axis.set_xticks([])
71 |         return axis
72 | 
73 | 
74 | class HubelWeiselFigure:
75 |     caption = ("Figure 1: A bipolar retinal cell's receptive field. "
76 |                "On-centre lumiance or off-centre darkness increases "
77 |                "neuron firing; on-centre\n darkness or off-centre luminance "
78 |                "decreases it. Mathematically, this is equivalent to "
79 |                "convolving the receptive field (a)\nwith the stimulus (b) "
80 |                "and responding to the average brightness of the output (c).")
81 |     def __init__(self):
82 |         field = RetinalReceptiveField()
83 |         grat = Grating()
84 |         self.fig, (field_ax, stim_ax, conv_ax) = plt.subplots(ncols=3,figsize = (8,4))
85 |         field_ax.set_title("(a) Bipolar cell receptive field")
86 |         field.apply_to_axis(field_ax)
87 |         stim_ax.set_title("(b) Visual stimulus")
88 |         grat.apply_to_axis(stim_ax)
89 |         conv_ax.set_title("(c) Convolved stimulus")
90 |         conv_ax.imshow(field.field*grat.field,
91 |                        cmap = 'gray',
92 |                        interpolation = 'bicubic')
93 |         conv_ax.set_yticks([])
94 |         conv_ax.set_xticks([])
95 |     def show(self):
96 |         self.fig.show()
97 |         
98 | 
99 | class TopDownIllusionFigure:
100|     caption = (
101|         "Figure 3: many optical illusions are thought to be caused\n"
102|         "by top-down signalling in the visual system. In the Ponzo\n"
103|         "illusion, two horizontal lines of equal length are perceived\n"
104|         "to be of different lengths because the other lines in the\n"
105|         "figure mimic a perspective situation with which we are familiar.\n"
106|         "Past knowledge is influencing the perception of simple elements\n"
107|         "of the image."
108|         )
109| 
110|     def __init__(self):
111|         self.fig, (self.ax1,self.ax2) = plt.subplots(ncols=2, figsize = (8,6))
112|         for axis in (self.ax1, self.ax2):
113|             axis.set_xticks([])
114|             axis.set_yticks([])
115|             axis.set_xlim(0,10)
116|             axis.set_ylim(0,10)
117|             rect1 = patches.Rectangle((3.75,1.5),2.5,0.25,
118|                                color = "blue")
119|             rect2 = patches.Rectangle((3.75,7),2.5,0.25,
120|                                color = "blue")
121|             axis.add_patch(rect1)
122|             axis.add_patch(rect2)
123|         for line in self.generate_railroad_lines(3.7):
124|             self.ax1.add_line(line)
125|             
126|         dotted_line1 = lines.Line2D((3.75,3.75),(0,10),
127|                                     color = 'orange',
128|                                     linestyle = ":")
129|         dotted_line2 = lines.Line2D((6.25,6.25),(0,10),
130|                                     color = 'orange',
131|                                     linestyle = ":")
132|         self.ax2.add_line(dotted_line1)
133|         self.ax2.add_line(dotted_line2)
134|         self.ax1.set_title("(a)")
135|         self.ax2.set_title("(b)")
136|         # self.fig.text(0.1, 0.01, self.caption,
137|         #               horizontalalignment = 'left')
138|     def show(self):
139|         self.fig.show()
140|     def generate_railroad_lines(self,slope): #() -> [lines.Line2D]
141|         line1 = Line(10/slope,-5/slope)
142|         line2 = Line(-10/slope,9.5*10/slope)
143|         xs1 = (0.5, 4.2)
144|         xs2 = (9.5,5.8)
145|         rail1 = lines.Line2D(xs1,tuple(map(line1,xs1)),
146|                              color = 'black')
147|         rail2 = lines.Line2D(xs2,tuple(map(line2,xs2)),
148|                              color = 'black')
149|         lst = [rail1,rail2]
150|         for y in range(2,10,2):
151|             lst.append(lines.Line2D(
152|                 (line1.inverse()(y),line2.inverse()(y)),(y,y),
153|                 color = 'blue')
154|             )
155|         return lst
156|                              
157| 
158| 
159| 
160| class BayesPlot:
161|     xs = np.linspace(-5, 5, 200)
162|     @staticmethod
163|     def format_axis(axis, legend=True):
164|         axis.set_xticks([])
165|         axis.set_yticks([])
166|         axis.set_ylim((0,1.5))
167|         if legend: axis.legend()
168|         return axis
169|     def format_axis_prior(self, axis, prior, likelihood, legend = True):
170|         prior_line, = axis.plot(self.xs, prior.predict(self.xs))
171|         prior_line.set_label("Prior")
172|         likelihood_line, = axis.plot(self.xs,likelihood.predict(self.xs))
173|         likelihood_line.set_label("Likelihood")
174|         self.format_axis(axis, legend)
175|         return axis
176|     def format_axis_post(self,axis,posterior, legend = True):
177|         post_line, = axis.plot(self.xs,posterior.predict(self.xs),
178|                                color = 'green')
179|         post_line.set_label("Posterior")
180|         self.format_axis(axis, legend)
181|         return axis
182| 
183| class GaussianDistribution:
184|     def __init__(self, mu, sig):
185|         self.mu = mu
186|         self.sig = sig
187|     def predict(self, X):
188|         return self.gaussian(X,self.mu,self.sig)
189|     @staticmethod
190|     def gaussian(x, mu, sig):
191|         coeff = 1/(sig * np.sqrt(2*np.pi))
192|         return coeff * np.exp(-(x-mu)*(x-mu) / (2 * sig * sig))
193| 
194| 
195| class PosteriorGaussianDistribution(GaussianDistribution):
196|     def __init__(self,prior,likelihood):
197|         p = prior
198|         l = likelihood
199|         self.sig = np.sqrt(p.sig**2*l.sig**2)/(p.sig**2 + l.sig**2)
200|         self.mu = (p.sig**(-2)*p.mu + l.sig**(-2)*l.mu)/(p.sig**(-2)+l.sig**(-2))
201|         
202| 
203|         
204| 
205| def test():
206|     xs = np.linspace(-5, 5, 200)
207|     dist = GaussianDistribution(0,0.5)
208|     plt.plot(xs,dist.predict(xs))
209|     plt.show()
210|         
211| 
212| class PredictiveCodingPlot(BayesPlot):
213|     labels = ("(a)","(b)","(c)","(d)")
214|     caption = ("Figure 4: The bayesian formaulation of predictive coding.(a) A\n"
215|                "prior belief measures the probability of various environmental\n"
216|                "states based on prior knowlege. (b) When new information is received,\n"
217|                "the likelihood of seeing it for each environmental state comprises\n"
218|                "a likelihood function. (c) Together these can produce a new prediction\n"
219|                "of the probabilities of external states, a posterior distribution.\n"
220|                "(d) How much this differs from the prior is the prediction error.\n"
221|                "\n"
222|                )
223|     def __init__(self, prior_mu = 0, prior_sig = 1,
224|                  likeli_mu = 3, likeli_sig = 1.4):
225|         prior      = GaussianDistribution(prior_mu, prior_sig)
226|         likelihood = GaussianDistribution(likeli_mu, likeli_sig)
227|         posterior  = PosteriorGaussianDistribution(prior,likelihood)
228|         self.fig, self.axes = plt.subplots(nrows = 4, figsize = (8,6))
229|         self.axes = self.axes.flatten()
230|         self.fig.text(0.5, 0.05, 'Environmental State', ha='center', va='center')
231|         self.fig.text(0.03, 0.5, 'Probability Density', ha='center', va='center',
232|                  rotation='vertical')
233|         # self.fig.text(0.03, 0.01, self.caption, ha = 'left')
234|         for idx,axis in enumerate(self.axes):
235| 
236|             self.format_axis(axis, legend = False)
237|             prior_line, = axis.plot(self.xs, prior.predict(self.xs))
238|             if idx in (0,): prior_line.set_label("Prior")
239|             if idx in (1,2):
240|                 likelihood_line, = axis.plot(self.xs,likelihood.predict(self.xs))
241|                 if idx==1: likelihood_line.set_label("Likelihood")
242|             if idx>1:
243|                 post_line, = axis.plot(self.xs,posterior.predict(self.xs),
244|                                        color = 'green')
245|                 if idx==2: post_line.set_label("Posterior")
246|             if idx==3:
247|                 axis.axvline(prior.mu,
248|                              linestyle = ':',
249|                              color = prior_line.get_color())
250|                 axis.axvline(posterior.mu,
251|                              linestyle = ':',
252|                              color = post_line.get_color())
253|                 arrow = axis.arrow(prior.mu, 1,
254|                                 (posterior.mu - prior.mu),
255|                                 0,
256|                                 color = 'red',
257|                                 length_includes_head = True,
258|                                 head_width = 0.2,
259|                                 head_length = 0.2
260|                             )
261|                 axis.legend([arrow],['Prediction Error'])
262|             else:
263|                 axis.legend()
264|             axis.set_ylabel(self.labels[idx],
265|                             rotation = "horizontal",
266|                             labelpad = 9)
267|     def show(self):
268|         self.fig.show()
269| 
270| 
271| 
272| 
273| 
274| 
275| 
276| class ContrastResponsePlot:
277|     caption = ("Figure 5: The effects of attention on the contrast response "
278|                "function of a V1 neuron.\n"
279|                "(a) Contrast gain, where the sensitivity of the neuron to its "
280|                "stimulus increases for all\n"
281|                "contrast levels, as though the stimulus had higher contrast. "
282|                "(b) Response gain, where \n"
283|                "the maximum response of the neuron is increased. (c) A "
284|                "combination of both contrast\n"
285|                "and response gain "
286|                "is sometimes observed. (adapted from Carrasco 2011)")
287|     xs = np.linspace(-5,5,50)
288|     log_fn = staticmethod(
289|         lambda limit,midpoint,X:limit/(1+np.exp(midpoint-X))
290|         )
291|     def __init__(self):
292|         self.fig,self.axes= plt.subplots(ncols=3, figsize = [8,2.5])
293|         for axis in self.axes:
294|             axis.set_yticks([])
295|             axis.set_xticks([])
296|             axis.set_xlabel("Stimulus Contrast")
297|             axis.set_ylabel("Neuron Response")
298|             axis.plot(self.log_fn(1,0,self.xs))
299|             axis.set_ylim((0,1.2))
300|         #First Axis
301|         self.axes[0].set_title("(a) Contrast Gain")
302|         self.axes[0].plot(self.log_fn(1,-1,self.xs),
303|                           linestyle = ":",
304|                           color = "blue")
305|         self.axes[1].set_title("(b) Response Gain")
306|         self.axes[1].plot(self.log_fn(1.15,0,self.xs),
307|                           linestyle = ":",
308|                           color = "blue")
309|         self.axes[2].set_title("(c) Contrast and Response Gain")
310|         self.axes[2].plot(self.log_fn(1.15,-1,self.xs),
311|                           linestyle = ":",
312|                           color = "blue")
313|         # self.fig.text(0.0625,0.1,self.caption)
314|     def show(self):
315|         self.fig.show()
316|     def __call__(self):
317|         self.fig.show()
318| 
319| 
320| 
321| 
322| class PsychosisPlot(BayesPlot):
323|     # caption = ("Figure 6: The predictive coding account of psychosis. Compare "
324|     #            "the healthy response to an unexpected \n"
325|     #            "stimulus (a) to the psychotic response characterised "
326|     #            "by a less certain prior and overestimation of the\n"
327|     #            "signal's precision (b). This leads to a larger shift in, "
328|     #            "and higher estimated precision of, the posterior. \n"
329|     #            "This results in a radically altered experience from noise in "
330|     #            "the environment.")
331|     def __init__(self):
332|         healthy_prior = GaussianDistribution(0, 0.3)
333|         psych_prior = GaussianDistribution(0,1.65)
334|         healthy_likelihood = GaussianDistribution(3, 0.7)
335|         psych_likelihood = GaussianDistribution(3, 0.5)
336|         healthy_posterior  = PosteriorGaussianDistribution(healthy_prior,
337|                                                            healthy_likelihood)
338|         
339|         psych_posterior  = PosteriorGaussianDistribution(psych_prior,
340|                                                            psych_likelihood)
341| 
342|         self.fig,ax =plt.subplots(nrows = 2, ncols = 2, figsize = (8,6))
343|         healthy,psych = ax.transpose()
344|         # fig.text(0.03, 0.1, self.caption, ha = 'left')
345| 
346|         self.format_axis_prior(healthy[0],healthy_prior,healthy_likelihood)
347|         healthy[0].set_title("(a) Healthy Response")
348|         healthy[0].set_ylabel("Probability Density")
349|         self.format_axis_prior(psych[0], psych_prior, psych_likelihood,
350|                                legend = False)
351|         
352|         psych[0].set_title("(b) Psychosis")
353| 
354|         
355|         self.format_axis_post(healthy[1], healthy_posterior)
356|         healthy[1].set_xlabel("Environmental State")
357|         healthy[1].set_ylabel("Probability Density")
358|         
359|         self.format_axis_post(psych[1], psych_posterior,
360|                               legend = False)
361|         psych[1].set_xlabel("Environmental State")
362| 
363|     def __call__(self):
364|         self.show()
365|     def show(self):
366|         self.fig.show()
367| 
368| 
369| if __name__=="__main__":
370|     plt.close("all")
371|     HubelWeiselFigure().show()
372|     TopDownIllusionFigure().show()
373|     PredictiveCodingPlot().show()
374|     ContrastResponsePlot().show()
375|     PsychosisPlot().show()


\TheoreticalVisualisation\kernel_figures.py
0  | 
1  | import matplotlib.pyplot as plt
2  | import numpy as np
3  | import seaborn
4  | 
5  | from accdatatools.ProcessLicking.kernel import lick_transform
6  | from accdatatools.Timing.synchronisation import get_lick_state_by_frame
7  | 
8  | class KernelExampleFigure:
9  |     seaborn.set_style("dark")
10 |     lick_kernel_fn = lambda x:100*x**2*(1-x)**7
11 |     lick_kernel_series = lick_kernel_fn(np.linspace(0,1,33))
12 |     reward_kernel_fn = lambda x:-200*x**4*(1-x)**5
13 |     reward_kernel_series = reward_kernel_fn(np.linspace(0,1,33))
14 |     licks = (1,5.7,8,8.2,8.4)
15 |     rewards = (3,5,6)
16 |     def __init__(self):
17 |         fig = plt.figure()
18 |         lick_kernel   = fig.add_axes((0.1,0.7,0.35,0.2))
19 |         reward_kernel = fig.add_axes((0.55,0.7,0.35,0.2))
20 |         event_train   = fig.add_axes((0.1,0.45,0.8,0.1))
21 |         prediction    = fig.add_axes((0.1,0.1, 0.8,0.2))
22 |         for ax in (lick_kernel,reward_kernel):
23 |             ax.set_ylim((-0.5,1.2))
24 |         for ax in (event_train,prediction):
25 |             ax.set_xlim((0,12))
26 |         lick_kernel.set_title("(A) Licking kernel")
27 |         lick_kernel.set_xlabel("t around a lick")
28 |         lick_kernel.plot(np.linspace(-0.2,2,33),self.lick_kernel_series,
29 |                          color = 'blue')
30 |         reward_kernel.set_title("(B) Reward kernel")
31 |         reward_kernel.plot(np.linspace(-0.2,2,33),self.reward_kernel_series,
32 |                            color = 'orange')
33 |         reward_kernel.set_xlabel("t around a reward")
34 |         event_train.set_title("(C) Event Occurances")
35 |         event_train.set_yticks([])
36 |         event_train.vlines(self.licks,0,1,colors = 'blue',label='lick')
37 |         event_train.vlines(self.rewards,0,1,colors='orange',label='rewards')
38 |         event_train.legend()
39 |         prediction.set_title("(D) Predicted Fluoresence Response")
40 |         prediction.set_xlabel("time")
41 |         prediction.set_ylabel("f/f")
42 |         prediction.plot(*self.predict(),color='k')
43 |         for lick in self.licks:
44 |             prediction.plot(np.linspace(lick-0.2,lick+2,33),
45 |                             self.lick_kernel_series,
46 |                             color = 'blue',
47 |                             linestyle = '-',
48 |                             alpha = 0.5)
49 |         for reward in self.rewards:
50 |             prediction.plot(np.linspace(reward-0.2,reward+2,33),
51 |                             self.reward_kernel_series,
52 |                             color = 'orange',
53 |                             linestyle = '-',
54 |                             alpha = 0.5)
55 |         self.fig = fig
56 |     def predict(self):
57 |         time = np.linspace(0,12,180)
58 |         prediction = np.zeros(time.shape)
59 |         for lick in self.licks:
60 |             start = np.searchsorted(time,lick-0.2)+1
61 |             prediction[start:start+len(
62 |                 self.lick_kernel_series)]+=self.lick_kernel_series
63 |         for reward in self.rewards:
64 |             start = np.searchsorted(time,reward-0.2)+1
65 |             prediction[start:start+len(
66 |                 self.reward_kernel_series)]+=self.reward_kernel_series
67 |         prediction = np.concatenate((prediction[1:],np.zeros(1)))
68 |         return (time,prediction)
69 | 
70 |     def show(self):
71 |         self.fig.show()
72 |         
73 |         
74 | class LinearApproximationFigure(KernelExampleFigure):
75 |     licks = (1.4,6,6.5,9)
76 |     rewards = (3,8)
77 |     
78 |     def lin_approx_predict(self):
79 |         time = np.linspace(0,12,180)
80 |         lick_vector = get_lick_state_by_frame(frame_times = time,lick_times = self.licks)
81 |         print(lick_vector.reshape(-1,10))
82 |         reward_vector = get_lick_state_by_frame(frame_times = time,lick_times = self.rewards)
83 |         reward_prediction = np.zeros(time.shape)
84 |         lick_prediction = np.zeros(time.shape)
85 |         
86 |         lick_kernels = lick_transform(lick_vector,15).astype(int)
87 |         lick_kernels = lick_kernels + 15 - 6
88 |         lick_kernels[lick_kernels < 0] = -1
89 |         lick_prediction = np.array([self.lick_kernel_series[i] if i>-1 else 0 for i in lick_kernels])
90 | 
91 |         reward_kernels = lick_transform(reward_vector,15).astype(int)
92 |         reward_kernels = reward_kernels + 15 - 6
93 |         reward_kernels[reward_kernels < 0] = -1
94 |         reward_prediction = np.array([self.reward_kernel_series[i] if i>-1 else 0 for i in reward_kernels])
95 |         
96 |         prediction = lick_prediction + reward_prediction
97 |         prediction = np.concatenate((np.zeros(7),prediction[:-7]))
98 |         return (time,prediction)
99 |     
100|     def __init__(self):
101|         fig = plt.figure(figsize = (8,9))
102|         lick_kernel   = fig.add_axes((0.1,0.75,0.35,0.13))
103|         reward_kernel = fig.add_axes((0.55,0.75,0.35,0.13))
104|         event_train   = fig.add_axes((0.1,0.55,0.8,0.07))
105|         prediction    = fig.add_axes((0.1,0.32, 0.8,0.13))
106|         approximation = fig.add_axes((0.1,0.05, 0.8,0.13))
107|         for ax in (lick_kernel,reward_kernel):
108|             ax.set_ylim((-0.5,1.2))
109|         for ax in (event_train,prediction, approximation):
110|             ax.set_xlim((0,12))
111|         lick_kernel.set_title("(A) Licking kernel")
112|         lick_kernel.set_xlabel("t around a lick")
113|         lick_kernel.plot(np.linspace(-0.2,2,33),self.lick_kernel_series,
114|                          color = 'blue')
115|         reward_kernel.set_title("(B) Reward kernel")
116|         reward_kernel.plot(np.linspace(-0.2,2,33),self.reward_kernel_series,
117|                            color = 'orange')
118|         reward_kernel.set_xlabel("t around a reward")
119|         event_train.set_title("(C) Event Occurances")
120|         event_train.set_yticks([])
121|         event_train.vlines(self.licks,0,1,colors = 'blue',label='lick')
122|         event_train.vlines(self.rewards,0,1,colors='orange',label='rewards')
123|         event_train.legend()
124|         prediction.set_title("(D) Kernel Approach")
125|         prediction.set_xlabel("time")
126|         prediction.set_ylabel("f/f")
127|         prediction.plot(*self.predict(),color='k', linestyle = "--",
128|                         label = "Predicted Fluorescence")
129|         prediction.legend()
130|         ylim = prediction.get_ylim()
131|         approximation.set_title("(E) Kernel-like Linear Regression")
132|         approximation.set_xlabel("time")
133|         approximation.set_ylabel("f/f")
134|         approximation.plot(*self.lin_approx_predict(),color='k',linestyle = "--")
135|         approximation.set_ylim(ylim)
136|         for lick in self.licks:
137|             for axis in (prediction, approximation):
138|                 axis.plot(np.linspace(lick-0.2,lick+2,33),
139|                                 self.lick_kernel_series,
140|                                 color = 'blue',
141|                                 linestyle = '-',
142|                                 alpha = 0.5)
143|         for reward in self.rewards:
144|             for axis in (prediction, approximation):
145|                 axis.plot(np.linspace(reward-0.2,reward+2,33),
146|                                 self.reward_kernel_series,
147|                                 color = 'orange',
148|                                 linestyle = '-',
149|                                 alpha = 0.5)
150|         self.fig = fig
151| if __name__=="__main__":
152|     fig = LinearApproximationFigure()
153|     fig.show()


\TheoreticalVisualisation\time_series_merging_algorithm_figure.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Wed Aug  5 18:23:19 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | 
8  | import pandas as pd
9  | import numpy as np
10 | import networkx as nx
11 | import matplotlib.pyplot as plt
12 | from random import randint
13 | 
14 | CUTOFF = 0.9
15 | N = 300
16 | N_CLUSTERS = 10
17 | MAX_COPIES = 10
18 | MIN_COPIES = 2
19 | NOISE = 0.2
20 | 
21 | for i in range(100):
22 |     df = pd.DataFrame()
23 |     name = 0
24 |     for i in range(N_CLUSTERS):
25 |         base = np.random.uniform(0,1,size=N)
26 |         for j in range(randint(MIN_COPIES,MAX_COPIES)):
27 |             df[str(name)]   = base + np.random.uniform(-NOISE/2,NOISE/2,size=N)
28 |             name+=1
29 |     
30 |     
31 |     corrs    = df.corr()
32 |     adj_matr = (corrs - np.eye(corrs.shape[0])) > CUTOFF
33 |     edges    = adj_matr[adj_matr > 0].stack().index.tolist()
34 |     
35 |     
36 |     graph = nx.Graph()
37 |     graph.add_nodes_from(adj_matr.columns)
38 |     graph.add_edges_from(edges)
39 |     
40 |     new_df = pd.DataFrame()
41 |     
42 |     #nx.draw(graph, with_labels = True)
43 |     for component in nx.connected_components(graph):
44 |         name = " ".join(component)
45 |         new_df[name] = df[component].mean(axis='columns')
46 |         
47 |     
48 |     #Merging algorithm
49 |     
50 |     
51 |     new_corrs    = new_df.corr()
52 |     new_adj_matr = (new_corrs - np.eye(new_corrs.shape[0])) > CUTOFF
53 |     new_edges    = new_adj_matr[new_adj_matr > 0].stack().index.tolist()
54 |     
55 |     new_graph = nx.Graph()
56 |     new_graph.add_nodes_from(new_adj_matr.columns)
57 |     new_graph.add_edges_from(new_edges)
58 |     
59 |     #Evaluate Results
60 |     initially_unconnected = not adj_matr.any(axis=None)
61 |     finally_unconnected   = not new_adj_matr.any(axis=None)
62 |     
63 |     
64 |     if not initially_unconnected:
65 |         try: assert finally_unconnected
66 |         except AssertionError as e:
67 |             fig,ax = plt.subplots(ncols = 2, figsize = (12,6))
68 |             ax[0].set_title("Initial Graph")
69 |             ax[1].set_title("Final Graph")
70 |             nx.draw_networkx(graph, with_labels = True, ax=ax[0])        
71 |             nx.draw_networkx(new_graph, with_labels = True, ax=ax[1])
72 |             fig.show()
73 |             raise e
74 | 
75 | fig,ax = plt.subplots(ncols = 2, figsize = (12,6))
76 | ax[0].set_title("Initial Graph")
77 | ax[1].set_title("Final Graph")
78 | nx.draw_networkx(graph, with_labels = True, ax=ax[0])        
79 | nx.draw_networkx(new_graph, with_labels = True, ax=ax[1])
80 | fig.show()


\TheoreticalVisualisation\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\Timing\synchronisation.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Jul 16 14:50:32 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | import numpy as np
8  | 
9  | 
10 | from accdatatools.ProcessPupil.size import get_pupil_size_at_each_eyecam_frame
11 | from accdatatools.Utils.deeploadmat import loadmat
12 | from accdatatools.Utils.signal_processing import (rising_edges, 
13 |                                                   rising_or_falling_edges)
14 | from accdatatools.ProcessPupil.size import get_pupil_size_at_each_eyecam_frame
15 | 
16 | 
17 | def get_neural_frame_times(timeline_path, number_of_frames):
18 |     '''
19 |     Helper function to get the mean time at which each frame in a recording
20 |     was captured.
21 | 
22 |     Parameters
23 |     ----------
24 |     timeline_path : str
25 |         path to Timeline.mat file in experiment directory.
26 |     number_of_frames : int
27 |         Total number of frames in the recording.
28 | 
29 |     Raises
30 |     ------
31 |     IOError
32 |         The Timeline.mat object has a frame counter which describes the number
33 |         of frames captured before the current tick. This is raised if the 
34 |         counter is either not monotonically increasing, or if it increases
35 |         by more than 1 in a single millisecond clock tick.
36 | 
37 |     Returns
38 |     -------
39 |     frame_times : list of floats
40 |         The times at which each frame occured. Because every 6 frames are
41 |         averaged together, this is the mean of the times of those six frames.
42 | 
43 |     '''
44 |     timeline = loadmat(timeline_path)
45 |     timeline = timeline['Timeline']
46 |     frame_counter = timeline['rawDAQData'][:,2]
47 |     timestamps = timeline['rawDAQTimestamps']
48 |     frame_times = np.zeros(int(frame_counter[-1]))
49 |     current_frame = 0
50 |     for timestamp,no_frames in zip(timestamps,frame_counter):
51 |         if no_frames == (current_frame + 1):
52 |             frame_times[current_frame] = timestamp
53 |             current_frame+=1
54 |         elif no_frames != current_frame:
55 |             raise IOError('Frame counter in timeline.mat not monotonic')
56 |     #Each suite2p frame is the average of 6 raw frames
57 |     #extra ticks are extraneous
58 |     frame_times = frame_times[:number_of_frames*6]
59 |     try:
60 |         frame_times = frame_times.reshape(number_of_frames, 6)
61 |     except ValueError:
62 |             #in the case where number_of_frames*6 > len(frame_times)
63 |             missing_frames = number_of_frames - frame_times.shape[0]//6
64 |             frame_times = frame_times[:(number_of_frames-missing_frames)*6]
65 |             frame_times = frame_times.reshape(number_of_frames-missing_frames, 6)
66 |             frame_times = np.mean(frame_times, axis = -1)
67 |             #In this case, we don't know when the last frame happens, but
68 |             #we can just linearly extrapolate
69 |             last_frames = np.empty(missing_frames)
70 |             delta_T = np.mean(np.diff(frame_times))
71 |             for idx, _ in enumerate(last_frames):
72 |                 last_frames[idx] = frame_times[-1] + (idx+1)*delta_T
73 |             frame_times = np.append(frame_times,last_frames)
74 |             return frame_times
75 |     frame_times = np.mean(frame_times, axis = -1)
76 |     return frame_times
77 | 
78 | 
79 | def get_lick_times(timeline_path):
80 |     timeline = loadmat(timeline_path)
81 |     timeline = timeline['Timeline']
82 |     lick_voltages = timeline['rawDAQData'][:,5]
83 |     edges = rising_edges(lick_voltages)
84 |     #When did these rising edges happen?
85 |     lick_times = timeline['rawDAQTimestamps'][edges]
86 |     return lick_times
87 | 
88 | def get_lick_state_by_frame(timeline_path = None, frame_times=None, lick_times = None):
89 |     '''
90 |     For each frame, whether one or more licks occurred during the
91 |     time over which that frame was captured.
92 |     
93 |     Parameters
94 |     ----------
95 |     timeline_path : string
96 |         Path to Timeline.mat file
97 |     frame_times : array of floats
98 |         List of times of averaged frames; output of get_neural_frame_times()
99 |         or of get_eyecam_frame_times()
100| 
101|     Returns
102|     -------
103|     licks : array of ints; same shape as frame_times.shape
104|         Number of licks was initiated during the corresponding frame.
105| 
106|     '''
107|     if not lick_times:
108|         lick_times = get_lick_times(timeline_path)
109|     #For each frame, how many licks occured in the time between that frame
110|     #and the next?
111|     #To find this we can just find the cumulative number of licks:
112|     cumulative_licks = np.zeros(frame_times.shape)
113|     #(surely there's a vectorised way to do this)
114|     for idx, frame_time in enumerate(frame_times):
115|         cumulative_licks[idx] = np.count_nonzero(lick_times<frame_time)
116|     #and then get the elementwise differences:
117|     return np.append(np.diff(cumulative_licks),0)
118| 
119| 
120| def get_eye_diameter_at_timepoints(hdf_path,timeline_path,timepoints):
121|     pupil_sizes = get_pupil_size_at_each_eyecam_frame(hdf_path)
122|     eyecam_frame_times = get_eyecam_frame_times(timeline_path)
123|     eyecam_frame_times = eyecam_frame_times[:len(pupil_sizes)]
124|     #What was the nearest eyecam frame to each timepoints?
125|     nearest_eyecam_frames = get_nearest_frame_to_each_timepoint(
126|                                 eyecam_frame_times,
127|                                 timepoints)
128|     # try:
129|     #     assert len(pupil_sizes) == len(eyecam_frame_times)
130|     # except AssertionError as e:
131|     #     print(f'len(pupil_sizes) = {len(pupil_sizes)}')
132|     #     print(f'len(eyecam_frame_times) = {len(eyecam_frame_times)}')
133|     #     raise e
134|     return pupil_sizes[nearest_eyecam_frames]
135|     
136| 
137| def get_eyecam_frame_times(matlab_timeline_file):
138|     obj = loadmat(matlab_timeline_file)
139|     timeline = obj["Timeline"]
140|     columns = np.array([i.name for i in timeline["hw"]["inputs"]])
141|     eye_camera_strobe = timeline["rawDAQData"][:,np.where(columns=="eyeCameraStrobe")]
142|     eye_camera_strobe = eye_camera_strobe.reshape(-1) #Remove excess axes
143|     edges = rising_or_falling_edges(eye_camera_strobe, 0.5)
144|     eye_frames = intersperse_events(edges,n=20)
145|     timestamps = timeline['rawDAQTimestamps']
146|     eye_frame_times = timestamps[eye_frames]
147|     return eye_frame_times
148| 
149| 
150| def intersperse_events(input_array, n):
151|     '''
152|     Replaces each single True value in the input array with N evenly 
153|     distributed True values.
154|     
155|     >>> intersperse_events(np.array([1,0,0,0,1,0,1]),2).astype('int')
156|     Out: array([1, 0, 1, 0, 1, 1, 0])
157| 
158|     Parameters
159|     ----------
160|     input_array : Array of Bool or Array of Bool-like
161|     n : int
162| 
163|     Returns
164|     -------
165|     output_array : Array of Bool
166|     '''
167|     #get an array of true indexes
168|     events, = np.where(input_array)
169|     output_array = np.zeros(input_array.shape,dtype=bool)
170|     #in terms of indexes, where should all the new events go?
171|     #Well, just construct a linearly spaced vector between
172|     #each event, including the starting but not the stopping
173|     #event. This will get (input_array-1)*N events!
174|     for (this_event,next_event) in zip(events[:-1],events[1:]):
175|         interspersed_events = np.linspace(this_event,next_event,n,endpoint=False)
176|         interspersed_events = np.round(interspersed_events).astype('int')
177|         output_array[interspersed_events] = True
178|     return output_array
179| 
180| 
181| def get_nearest_frame_to_each_timepoint(frame_times, ls_of_timepoints):
182|     '''
183|     Get an array of the indexes of the frames captured closest to a list
184|     of times. 
185|     
186|     Example usage:
187|         You have an array of frame times of a camera viewing the mouses's head
188|         and an array of times the mouse licked an electrode.
189|         To get the frame captured closest to each lick, call
190|         >>>get_nearest_frame_to_each_timepoint(mouse_head_frame_times, 
191|                                                licking_times)
192| 
193|     Parameters
194|     ----------
195|     frame_times : array of float
196|         The times each frame occurred.
197|     ls_of_timepoints : array of float
198|         The event times of interest.
199| 
200|     Returns
201|     -------
202|     frameidx_list : array of int
203|         The frame indexes corresponding to ls_of_timepoints.
204| 
205|     '''
206|     #find all the smallest idxs such that all frame_times[idxs]>ls_of_timepoints
207|     idxs = np.searchsorted(frame_times, ls_of_timepoints, side="left")
208|     idxs[idxs==len(frame_times)] -= 1
209|     #Now there are only two options:
210|     #Either frame_times[idxs][n] is closest to ls_of_timepoints[n],
211|     #  or frame_times[idxs][n-1] is. Set up a condition list:
212|     suprema = frame_times[idxs]
213|     infima  = frame_times[idxs-1]
214|     condlist = [np.abs(suprema - ls_of_timepoints) < np.abs(infima - ls_of_timepoints),
215|                 True]
216|     choicelist = [idxs, idxs-1]
217|     frame_idx_list = np.select(condlist, choicelist)
218| 
219|     return frame_idx_list
220| 
221| 
222| def get_pupil_size_by_neural_frame(timeline_path, h5_path, frame_times):
223|     pass


\Timing\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\ToCSV\without_collapsing.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Wed Jun 24 14:07:54 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import os
7  | 
8  | import numpy  as np
9  | import pandas as pd
10 | 
11 | 
12 | from accdatatools.ProcessLicking.kernel import lick_transform
13 | from accdatatools.Utils.map_across_dataset import (apply_to_all_one_plane_recordings,
14 |                                                    apply_to_all_recordings_of_class)
15 | from accdatatools.Observations.trials import get_trials_in_recording
16 | from accdatatools.Observations.recordings import Recording
17 | from accdatatools.Utils.convienience import item
18 | from accdatatools.Utils.path import get_exp_id, DLC_ANALYSED_VIDEOS_DIRECTORY
19 | from accdatatools.Timing.synchronisation import (get_neural_frame_times, 
20 |                                                 get_lick_state_by_frame,
21 |                                                 get_eye_diameter_at_timepoints)
22 | 
23 | 
24 | 
25 | 
26 | class RecordingUnroller(Recording):
27 |     def __init__(self,exp_path,ignore_dprime=False, tolerate_lack_of_eye_video = False,
28 |                  ignore_eye_video=False):
29 |         self.exp_path = exp_path
30 |         super().__init__(exp_path)
31 |         self.trials = get_trials_in_recording(exp_path, 
32 |                                               return_se=False,
33 |                                               se = self,
34 |                                               ignore_dprime = ignore_dprime,
35 |                                               suppress_dprime_error=False)
36 |         timeline_path  = os.path.join(exp_path,
37 |                                   item(
38 |                                       [s for s in os.listdir(exp_path) if 'Timeline.mat' in s]))
39 |         
40 |         self.frame_times = get_neural_frame_times(timeline_path, self.ops["nframes"])
41 |         
42 |         #Now  we somehow need to get this into a continuous time series.
43 |         (self.trialtime, self.iscorrect, self.side, self.isgo, self.contrast,
44 |          self.trial_id, self.trial_num, self.peritrialtime, 
45 |          self.trial_component) = self.get_timeseries()
46 |         
47 |         self.licks = get_lick_state_by_frame(timeline_path, self.frame_times)
48 |         #This licks is the bool series, we want the deltaT series
49 |         self.licks = lick_transform(self.licks)
50 |         
51 |         #Need to check if this experiment had a video that was processed
52 |         #by our DeepLabCut Network...
53 |         path, exp_id = os.path.split(self.exp_path)
54 |         for file in os.listdir(DLC_ANALYSED_VIDEOS_DIRECTORY):
55 |             if exp_id in file and '.h5' in file:
56 |                 hdf_path = os.path.join(DLC_ANALYSED_VIDEOS_DIRECTORY,
57 |                                         file)
58 |                 if not ignore_eye_video:
59 |                     self.pupil_diameter = get_eye_diameter_at_timepoints(hdf_path, 
60 |                                                                  timeline_path, 
61 |                                                                  self.frame_times)
62 |                     break
63 |         else:
64 |             if not ignore_eye_video and not tolerate_lack_of_eye_video:
65 |                 raise ValueError(f"No associated eyecam footage found at {DLC_ANALYSED_VIDEOS_DIRECTORY}")
66 |             self.pupil_diameter = [np.nan]*self.ops["nframes"]
67 | 
68 | 
69 |     def get_timeseries(self):
70 |         #When did trials happen in terms of neural frames?
71 |         start_times = np.array([trial.start_stimulus for trial in self.trials]) - 1
72 |         start_idxs  = self.frame_times.searchsorted(start_times)
73 |         end_idxs    = start_idxs + 5*5 #each trial is 5s at 5FPS
74 |         
75 | 
76 |         trial_s     = np.ones(self.ops["nframes"])*(-999)
77 |         peritrial_s = np.ones(self.ops["nframes"])*(-999)
78 |         corre_s     = np.full(self.ops["nframes"],-1)
79 |         side_s      = np.full(self.ops["nframes"],'NA',dtype = object)
80 |         isgo_s      = np.full(self.ops["nframes"],-1)
81 |         con_s       = np.full(self.ops["nframes"],-1,dtype=float)
82 |         id_s        = np.full(self.ops["nframes"],"NA", dtype= object)
83 |         comp_s      = np.full(self.ops["nframes"],"NA", dtype= object)
84 |         trial_num   = np.zeros(self.ops["nframes"])
85 |         
86 |         trial_struct = np.array(["Tone"]*5 + ["Stim"]*10 + ["Resp"]*10)
87 |         
88 |         for idx,(trial, start_idx, end_idx) in enumerate(zip(self.trials, start_idxs, end_idxs)):
89 |             peristart = start_idx - 5*3
90 |             periend = start_idx
91 |             trial_id = self.exp_path.split("\\")[-1] + f" {idx}"
92 |             trial_s[start_idx:end_idx] = np.arange(1,end_idx-start_idx+1)
93 |             comp_s[start_idx:end_idx] = trial_struct
94 |             peritrial_s[peristart:periend] = np.arange(periend-peristart,0,-1)
95 |             corre_s[start_idx:end_idx] = 1 if trial.correct else 0
96 |             side_s[start_idx:end_idx]  = 'Left' if trial.isleft else 'Right'
97 |             isgo_s[start_idx:end_idx]  = 1 if trial.isgo else 0
98 |             con_s[start_idx:end_idx]  = trial.contrast
99 |             id_s[start_idx:end_idx] = trial_id
100|             trial_num[start_idx:] += 1
101|         return (trial_s,corre_s,side_s,isgo_s,con_s,id_s,trial_num,peritrial_s,
102|                 comp_s)
103|     
104|     def to_unrolled_records(self):
105|         results = []
106|         ROI_IDs = self.trials[0].ROI_identifiers
107|         for roi_df, roi_spks, roi_id, roi_log in zip(self.dF_on_F, self.spks, ROI_IDs,self.logged_dF):
108|             for idx,(df, spk, log, trialtime, peritrialtime, comp, correct, side, go,
109|                      contrast, lick, pupil, frametime, trial_id, 
110|                      trial_num) in enumerate(zip(
111|                                                             roi_df,
112|                                                             roi_spks,
113|                                                             roi_log,
114|                                                             self.trialtime,
115|                                                             self.peritrialtime,
116|                                                             self.trial_component,
117|                                                             self.iscorrect,
118|                                                             self.side,
119|                                                             self.isgo,
120|                                                             self.contrast,
121|                                                             self.licks,
122|                                                             self.pupil_diameter,
123|                                                             self.frame_times,
124|                                                             self.trial_id,
125|                                                             self.trial_num)):
126|                         results.append(
127|                             {
128|                             'ROI_ID':self.exp_path.split("\\")[-1] + f" {roi_id.item()}",
129|                             'Trial_ID':trial_id,
130|                             'go': go,
131|                             'side': side,
132|                             'correct': correct,
133|                             'contrast':contrast,
134|                             "time": frametime,
135|                             "trial_factor": trialtime,
136|                             "peritrial_factor":peritrialtime,
137|                             "trial_component":comp,
138|                             "dF_on_F": df,
139|                             "logged_dF":log,
140|                             "spks": spk,
141|                             "lick_factor": lick,
142|                             "pupil_diameter": pupil if not np.isnan(pupil) else 'NA',
143|                             "number_of_trials_seen":trial_num
144|                             }
145|                             )
146|         return results
147|     def to_dataframe(self):
148|         records = self.to_unrolled_records()
149|         return pd.DataFrame(records)
150| 
151| def get_dataframe_from_path(path, ignore_dprime=False):
152|     try:
153|         return pd.DataFrame(RecordingUnroller(path, ignore_dprime=ignore_dprime
154|                                       ).to_unrolled_records())
155|     except ValueError as e:
156|         print("ValueError in get_dataframe_from_path")
157|         raise e
158|         return None
159| 
160| def append_recording_to_csv(filestream,path, ignore_dprime=False):
161|     df = get_dataframe_from_path(path,ignore_dprime=ignore_dprime)
162|     if type(df)==pd.DataFrame:
163|         df.to_csv(filestream,header=(filestream.tell()==0))
164|     del df
165| 
166| def get_whole_dataset(drive, cls=None):
167|     result = []
168|     def func(path):
169|         recorder = RecordingUnroller(path, ignore_dprime = True, 
170|                                      tolerate_lack_of_eye_video = True)
171|         df = recorder.to_dataframe()
172|         del recorder
173|         result.append(df)
174|         del df
175|     if not cls:
176|         apply_to_all_one_plane_recordings(drive, func)
177|     else:
178|         apply_to_all_recordings_of_class(cls,drive,func)
179|     result = pd.concat(result,ignore_index=True)
180|     return result
181| 
182| def construct_csv_for_recording_class(csv_path,cls):
183|     dataset = get_whole_dataset("H:",cls = cls)
184|     csv = open(csv_path, "w")
185|     dataset.to_csv(csv)
186|     
187| if __name__=="__main__":
188|     construct_csv_for_recording_class("C:/Users/viviani/Desktop/full_datasets_for_analysis/low_contrast.csv",
189|                                       "low_contrast")
190|     construct_csv_for_recording_class("C:/Users/viviani/Desktop/full_datasets_for_analysis/left_only_high_contrast.csv",
191|                                       "left_only_high_contrast")
192|     construct_csv_for_recording_class("C:/Users/viviani/Desktop/full_datasets_for_analysis/both_sides_high_contrast.csv",
193|                                       "both_sides_high_contrast")
194|     #grab some misc recordings for testing too:
195|     import pickle as pkl
196|     with open("../DataCleaning/low_contrast.pkl", 'rb') as file:
197|         cleaned_list = pkl.load(file)
198|     with open("../DataCleaning/low_contrast_uncleaned.pkl", 'rb') as file:
199|         uncleaned_list = pkl.load(file)
200|     misc_recordings = [r for r in uncleaned_list if not r in cleaned_list]
201|     for r_path in misc_recordings:
202|         get_dataframe_from_path(r_path).to_csv(os.path.join(
203|             "C:/Users/viviani/Desktop/single_experiments_for_testing",
204|             f'{get_exp_id(r_path)}.csv'))


\ToCSV\with_collapsing.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Jul  2 11:46:41 2020
3  | 
4  | @author: Vivian Imbriotis
5  | 
6  | Converting the suite2p, DeepLabCut, and experiment metadata intermediaries
7  | to a CSV for further analysis with R, collapsing across time or across both
8  | time and ROIS to reduce the number of dimentions in the output data.
9  | Collapsing across ROIs without collapsing across time is not currently implemented.
10 | """
11 | import warnings
12 | 
13 | from accdatatools.ToCSV.without_collapsing import RecordingUnroller
14 | import numpy as np
15 | import pandas as pd
16 | from scipy.stats import kstest, zscore, pearsonr, ttest_ind
17 | import matplotlib.pyplot as plt
18 | from jenkspy import jenks_breaks
19 | # from scipy.stats import pearsonr
20 | from accdatatools.Utils.map_across_dataset import apply_to_all_recordings_of_class
21 | import seaborn as sb
22 | 
23 | from numpy import diff
24 | 
25 | sb.set_style('darkgrid')
26 | 
27 | # def calculate_pvalues(df):
28 | #     df = df.dropna()._get_numeric_data()
29 | #     dfcols = pd.DataFrame(columns=df.columns)
30 | #     pvalues = dfcols.transpose().join(dfcols, how='outer')
31 | #     for r in df.columns:
32 | #         for c in df.columns:
33 | #             pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)
34 | #     return pvalues
35 | 
36 | class CollapsingRecordingUnroller(RecordingUnroller):
37 |     def get_trial_auc(self,collapse_across_ROIs = True):
38 |         roi_aucs = np.array([np.sum(trial.dF_on_F,axis=-1) for trial in self.trials])
39 |         if collapse_across_ROIs:
40 |             return np.mean(roi_aucs,axis=-1)
41 |         else:
42 |             return roi_aucs
43 |         
44 |     def get_tone_auc(self,collapse_across_ROIs = True):
45 |         roi_aucs = np.array(
46 |             [np.sum(trial.dF_on_F[:,:5],axis=-1) for trial in self.trials])
47 |         if collapse_across_ROIs:
48 |             return np.mean(roi_aucs,axis=-1)
49 |         else:
50 |             return roi_aucs
51 |         
52 |     def get_stim_auc(self,collapse_across_ROIs = True):
53 |         roi_aucs = np.array(
54 |             [np.sum(trial.dF_on_F[:,5:15],axis=-1) for trial in self.trials])
55 |         if collapse_across_ROIs:
56 |             return np.mean(roi_aucs,axis=-1)
57 |         else:
58 |             return roi_aucs
59 |     def get_resp_auc(self,collapse_across_ROIs = True):
60 |         roi_aucs = np.array(
61 |             [np.sum(trial.dF_on_F[:,15:],axis=-1) for trial in self.trials])
62 |         if collapse_across_ROIs:
63 |             return np.mean(roi_aucs,axis=-1)
64 |         else:
65 |             return roi_aucs
66 |     def get_pupil_change(self):
67 |         mean_change_pupil_diameters = np.zeros(len(self.trials))
68 |         for idx,trial in enumerate(self.trials):
69 |             pupil_size = np.nanmean(
70 |                 np.diff(
71 |                     self.pupil_diameter[trial.start_idx:trial.start_idx+5]
72 |                 ))
73 |             mean_change_pupil_diameters[idx] = pupil_size
74 |         return mean_change_pupil_diameters
75 |     
76 |     def get_reaction_time(self):
77 |         #Going to use the Fisher-Jenks algorithm to exclude the burst of
78 |         #licking behaviour associated with the tone/stimulus onset.
79 |         reaction_times = np.zeros(len(self.trials))
80 |         for idx,trial in enumerate(self.trials):
81 |             #Mice can only have reaction times if they're correct on a go trial.
82 |             if all((trial.correct, not trial.istest, trial.isgo)):
83 |                 licks_during_trial = self.lick_times[np.logical_and(
84 |                                         self.lick_times>trial.start_stimulus,
85 |                                         self.lick_times<trial.end_response)]
86 |                 try:
87 |                     reflex_licks_start,reflex_licks_end,_ = jenks_breaks(
88 |                         licks_during_trial,
89 |                         nb_class = 2)
90 |                     first_lick_after_stim = np.argmax(
91 |                         self.lick_times>(reflex_licks_end))
92 |                     lick_time = self.lick_times[first_lick_after_stim]
93 |                 except ValueError:
94 |                     #We have a single lick
95 |                     lick_time = licks_during_trial[0]
96 |                 if lick_time>trial.end_response: reaction_time = False
97 |                 else: reaction_time = lick_time - trial.start_stimulus
98 |                 if reaction_time>3.5: reaction_time = False
99 |             else:
100|                 reaction_time = False
101|             reaction_times[idx] = reaction_time
102|         return reaction_times
103|     
104|     def cluster_licking(self):
105|         #Let's build up a jagged array of [([],[])], ie so we have a list
106|         #of pairs of lists - each trial has two clusters of licks, one
107|         #for each cluster of licks as defned by Fisher-Jenks.
108|         trial_licking = [] #The outer list
109|         for idx,trial in enumerate(self.trials):
110|             #Mice can only have reaction times if they're correct on a go trial.
111|             if all((trial.correct, not trial.istest, trial.isgo)):
112|                 #Get all licks during the trial
113|                 licks_during_trial = self.lick_times[np.logical_and(
114|                                         self.lick_times>(trial.start_stimulus-1),
115|                                         self.lick_times<trial.start_stimulus+4)]
116|                 #Now we want to split this list in two using Fisher-Jenks
117|                 try:
118|                     reflex_licks_start,reflex_licks_end,_ = jenks_breaks(
119|                         licks_during_trial,
120|                         nb_class = 2)
121|                     fst_cluster = licks_during_trial[licks_during_trial<=reflex_licks_end]
122|                     snd_cluster = licks_during_trial[licks_during_trial>reflex_licks_end]
123|                     fst_cluster -= trial.start_stimulus
124|                     snd_cluster -= trial.start_stimulus
125|                 except ValueError:
126|                     #We have a single lick
127|                     fst_cluster = licks_during_trial
128|                     snd_cluster = np.array([])
129|                 split_ls = (fst_cluster,snd_cluster)
130|                 trial_licking.append(split_ls)
131|         return trial_licking
132|     
133|     def plot_licking_clustering_approach(self):
134|         trial_licking = self.cluster_licking()
135|         sb.set_style('darkgrid')
136|         fig,ax = plt.subplots()
137|         for idx,(fst_cluster,snd_cluster) in enumerate(trial_licking):
138|             x1 = np.full(fst_cluster.shape,idx)
139|             ax.plot(fst_cluster, x1,'o',color='darksalmon',markersize=2.5)
140|             x2 = np.full(snd_cluster.shape,idx)
141|             ax.plot(snd_cluster,x2,'o',color='blue',markersize=2.5)
142|         ymax,ymin = ax.get_ylim()
143|         ax.vlines((0,1,3),ymin,ymax,linestyles='dashed',color = 'k')
144|         for name,pos in zip(('Tone','Stimulus','Response'),(0,1,3)):
145|             ax.text(pos+0.1,ymax,name,
146|                     horizontalalignment='left',
147|                     verticalalignment='bottom')
148|         ax.set_ylabel('Trial Number')
149|         ax.set_xlabel(r'$\Delta$t from trial onset (s)')
150|         plt.show()
151|     
152|     def get_mean_pupil_size_over_period(self,period='trial'):
153|         mean_pupil_diameters = np.zeros(len(self.trials))
154|         for idx,trial in enumerate(self.trials):
155|             if period=='trial':
156|                 pupil_size = np.nanmean(
157|                     self.pupil_diameter[trial.start_idx:trial.end_idx]
158|                     )
159|             elif period=='tone':
160|                 pupil_size = np.nanmean(
161|                     self.pupil_diameter[trial.start_idx:trial.start_idx+5]
162|                     )
163|             else:
164|                 raise NotImplementedError("period kwarg must be 'trial' or 'tone'")
165|             mean_pupil_diameters[idx] = pupil_size
166|         return mean_pupil_diameters
167|     
168|     
169|     def to_unrolled_records(self,collapse_across_ROIs=True):
170|         output = []
171|         for ((trial_idx,trial), full_auc, tone_auc, stim_auc, resp_auc, 
172|              reaction_time, pupil_size, pupil_size_tone, pupil_change) in zip(
173|                 enumerate(self.trials),
174|                 self.get_trial_auc(collapse_across_ROIs),
175|                 self.get_tone_auc(collapse_across_ROIs),
176|                 self.get_stim_auc(collapse_across_ROIs),
177|                 self.get_resp_auc(collapse_across_ROIs),
178|                 self.get_reaction_time(),
179|                 self.get_mean_pupil_size_over_period('trial'),
180|                 self.get_mean_pupil_size_over_period('tone'),
181|                 self.get_pupil_change()):
182|             if collapse_across_ROIs:
183|                 output.append({"TrialID": self.exp_path.split("\\")[-1] + f" {trial_idx}",
184|                         "Correct": trial.correct,
185|                         "Go":      trial.isgo,
186|                         "Side":    "Left" if trial.isleft else ("Right" if trial.isright else "Unknown"),
187|                         "contrast": trial.contrast,
188|                         "TrialAUC":full_auc,
189|                         "ToneAUC": tone_auc,
190|                         "StimAUC": stim_auc,
191|                         "RespAUC": resp_auc,
192|                         "Reaction_Time": reaction_time if reaction_time else np.nan,
193|                         "Pupil_size": pupil_size if pupil_size else np.nan,
194|                         "Pupil_size_tone": pupil_size_tone if pupil_size_tone else np.nan,
195|                         "Pupil_change": pupil_change if pupil_change else np.nan
196|                         })
197|             else:
198|                 for roi, (f,t,s,r) in enumerate(zip(full_auc,tone_auc,
199|                                                     stim_auc,resp_auc)):
200|                     output.append({
201|                             "TrialID": self.exp_path.split("\\")[-1] + f" {trial_idx}",
202|                             "roiNum":  roi,
203|                             "Correct": trial.correct,
204|                             "Go":      trial.isgo,
205|                             "Contrast":trial.contrast,
206|                             "Side":    "Left" if trial.isleft else "Right",
207|                             "TrialAUC":f,
208|                             "ToneAUC": t,
209|                             "StimAUC": s,
210|                             "RespAUC": r,
211|                             "Reaction_Time": reaction_time if reaction_time else np.nan,
212|                             "Pupil_size": pupil_size if pupil_size else np.nan,
213|                             "Pupil_size_tone": pupil_size_tone if pupil_size_tone else np.nan,
214|                             "Pupil_change": pupil_change if pupil_change else np.nan
215|                             })
216|         return output
217|     
218|     def to_csv(self, file, collapse_across_ROIs = True):
219|         df = pd.DataFrame(self.to_unrolled_records(collapse_across_ROIs))
220|         if type(file)==str:
221|             df.to_csv(file)
222|         else:
223|             df.to_csv(file,header=(file.tell()==0))
224|             
225|     def to_dataframe(self,collapse_across_ROIs = True):
226|         df = pd.DataFrame(self.to_unrolled_records(collapse_across_ROIs))
227|         return df
228| 
229| def generate_attention_metrics_figure():
230|     df = CollapsingRecordingUnroller("H:/Local_Repository/CFEB013/2016-06-29_02_CFEB013",True,False).to_dataframe()
231|     df2 = df.loc[:,'Reaction_Time':'Pupil_size'][df.Reaction_Time!='NA'].dropna()
232|     df2.Reaction_Time = pd.to_numeric(df.Reaction_Time)
233|     import matplotlib.pyplot as plt
234|     fig,ax = plt.subplots(nrows=2, ncols=2, constrained_layout=True)
235|     ax[0][0].plot(df2.Reaction_Time,df2.Pupil_size,'o')
236|     ax[0][0].set_xlabel("Reaction Time (s)")
237|     ax[0][0].set_ylabel("Pupil Diameter while reacting (pixels)")
238|     ax[0][1].violinplot([df.Pupil_size[df.Correct==True].values,
239|                 df.Pupil_size[df.Correct==False].values]
240|                    )
241|     ax[0][1].set_ylabel("Pupil diameter while reacting (pixels)")
242|     ax[0][1].set_xticks([1,2])
243|     ax[0][1].set_xticklabels(["Correct","Incorrect"])
244|     reactions = df[['contrast','Reaction_Time']].dropna()
245|     ax[1][0].violinplot([reactions.Reaction_Time[reactions.contrast==0.1].values,
246|                    reactions.Reaction_Time[reactions.contrast==0.5].values],
247|                   )
248|     ax[1][0].set_xticks([1,2])
249|     ax[1][0].set_xticklabels(["10%","50%"])
250|     ax[1][0].set_xlabel("Contrast")
251|     ax[1][0].set_ylabel("Reaction Time")
252|     pupils = df[['contrast','Pupil_size']].dropna()
253|     ax[1][1].violinplot([pupils.Pupil_size[pupils.contrast==0.1].values,
254|                    pupils.Pupil_size[pupils.contrast==0.5].values],
255|                   )
256|     ax[1][1].set_xticks([1,2])
257|     ax[1][1].set_xticklabels(["10%","50%"])
258|     ax[1][1].set_xlabel("Contrast")
259|     ax[1][1].set_ylabel("Pupil Diameter while reacting (pixels)")
260|     fig.show()
261|     
262| def perform_attention_metrics_testing_left_only():
263|     df = pd.read_csv("C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_left_only.csv")
264|     df["Recording_ID"] = list(map(lambda s:s.split(" ")[-1],df.TrialID.values))
265|     for ID in df.Recording_ID.unique():
266|         subset = df[df.Recording_ID == ID].Pupil_size_tone.values
267|         subset_zscore = zscore(subset, nan_policy = "omit")
268|         df.loc[df.Recording_ID == ID, "Pupil_size_tone"] = subset_zscore
269|     df2 = df.loc[:,'Reaction_Time':'Pupil_size_tone'][df.Reaction_Time!='NA'].dropna()
270|     df2.Reaction_Time = pd.to_numeric(df.Reaction_Time)
271|     fig,ax = plt.subplots(ncols=2, constrained_layout=True)
272|     ax[0].plot(df2.Reaction_Time,df2.Pupil_size_tone,'o',
273|                   markersize = 1)
274|     ax[0].set_xlabel("Reaction Time (s)")
275|     ax[0].set_ylabel("Pupil Diameter (mean of second prior to stimulus)\n(z-score within recording)")
276|     df3 = df[['Correct','Pupil_size_tone']].dropna()
277|     pupil_size_by_correctness = [df3.Pupil_size_tone[df.Correct==True].values,
278|                                  df3.Pupil_size_tone[df.Correct==False].values]
279|     ax[1].violinplot(pupil_size_by_correctness)
280|     ax[1].set_ylabel("Pupil diameter (mean of second prior to stimulus\n(z-score within recording)")
281|     ax[1].set_xticks([1,2])
282|     ax[1].set_xticklabels(["Correct","Incorrect"])
283|     print("\nPupil Size Vs Reaction Time on Subsequent Trial")
284|     s,p = pearsonr(df2.Reaction_Time,df2.Pupil_size_tone)
285|     print(f"PearsonrResult(statistic={s},\npvalue={p})")
286|     
287|     print("\nPupil Size vs Correctness on subsequent trial")
288|     print(ttest_ind(pupil_size_by_correctness[0],
289|                  pupil_size_by_correctness[1],
290|                  equal_var=False))
291|     print(kstest(pupil_size_by_correctness[0],
292|                  pupil_size_by_correctness[1]))
293|     print(f"\n0.05-Critical value after Bonferroni Correction\nfor 3 comparisons is {0.05/3:.4f}")
294|     fig.show()
295| 
296| def perform_attention_metrics_testing_low_contrast():
297|     df = pd.read_csv("C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_low_contrast.csv")
298|     df["Recording_ID"] = list(map(lambda s:s.split(" ")[-1],df.TrialID.values))
299|     for ID in df.Recording_ID.unique():
300|         subset = df[df.Recording_ID == ID].Pupil_size_tone.values
301|         subset_zscore = zscore(subset, nan_policy = "omit")
302|         df.loc[df.Recording_ID == ID, "Pupil_size_tone"] = subset_zscore
303|     df2 = df.loc[:,'Reaction_Time':'Pupil_size_tone'][df.Reaction_Time!='NA'].dropna()
304|     df2.Reaction_Time = pd.to_numeric(df.Reaction_Time)
305|     fig,ax = plt.subplots(nrows=2, ncols=2, constrained_layout=True)
306|     ax[0][0].plot(df2.Reaction_Time,df2.Pupil_size_tone,'o',
307|                   markersize = 1)
308|     ax[0][0].set_xlabel("Reaction Time (s)")
309|     ax[0][0].set_ylabel("Pupil Diameter (mean of second prior to stimulus)\n(z-score within recording)")
310|     df3 = df[['Correct','Pupil_size_tone']].dropna()
311|     pupil_size_by_correctness = [df3.Pupil_size_tone[df.Correct==True].values,
312|                                  df3.Pupil_size_tone[df.Correct==False].values]
313|     ax[0][1].violinplot(pupil_size_by_correctness)
314|     ax[0][1].set_ylabel("Pupil Diameter (mean of second prior to stimulus)\n(z-score within recording)")
315|     ax[0][1].set_xticks([1,2])
316|     ax[0][1].set_xticklabels(["Correct","Incorrect"])
317|     reactions = df[['contrast','Reaction_Time']].dropna()
318|     rt_by_contrast = [reactions.Reaction_Time[reactions.contrast==0.1].values,
319|                       reactions.Reaction_Time[reactions.contrast==0.5].values]
320|     ax[1][0].violinplot(rt_by_contrast)
321|     ax[1][0].set_xticks([1,2])
322|     ax[1][0].set_xticklabels(["10%","50%"])
323|     ax[1][0].set_xlabel("Contrast")
324|     ax[1][0].set_ylabel("Reaction Time")
325|     pupils = df[['contrast','Pupil_size_tone']].dropna()
326|     pupil_size_by_contrast = [pupils.Pupil_size_tone[pupils.contrast==0.1].values,
327|                               pupils.Pupil_size_tone[pupils.contrast==0.5].values]
328|     ax[1][1].violinplot(pupil_size_by_contrast)
329|     ax[1][1].set_xticks([1,2])
330|     ax[1][1].set_xticklabels(["10%","50%"])
331|     ax[1][1].set_xlabel("Contrast")
332|     ax[1][1].set_ylabel("Pupil Diameter while reacting\n(z-score within recording)")
333|     fig.show()
334|     
335|     
336|     print("\nPupil Size Vs Reaction Time")
337|     s,p = pearsonr(df2.Reaction_Time,df2.Pupil_size)
338|     print(f"PearsonrResult(statistic={s},\npvalue={p})")
339|     
340|     print("\nPupil Size Vs Contrast:")
341|     print(kstest(pupil_size_by_contrast[0],
342|                  pupil_size_by_contrast[1]))
343|     print("\nPupil Size Vs Correctness")
344|     print(kstest(pupil_size_by_correctness[0],
345|                  pupil_size_by_correctness[1]))
346|     print("\nReaction Time Vs Contrast")
347|     print(kstest(rt_by_contrast[0],
348|                  rt_by_contrast[1]))
349|     print(f"\n0.05-Critical value after Bonferroni Correction\nfor 4 comparisons is {0.05/4:.4f}")
350| 
351| def perform_attention_metrics_testing_with_derivative():
352|     df = pd.read_csv("C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_low_contrast.csv")
353|     df["Recording_ID"] = list(map(lambda s:s.split(" ")[-1],df.TrialID.values))
354|     for ID in df.Recording_ID.unique():
355|         subset = df[df.Recording_ID == ID].Pupil_change.values
356|         subset_zscore = zscore(subset, nan_policy = "omit")
357|         df.loc[df.Recording_ID == ID, "Pupil_change"] = subset_zscore
358|     df2 = df.loc[:,'Reaction_Time':'Pupil_change'][df.Reaction_Time!='NA'].dropna()
359|     df2.Reaction_Time = pd.to_numeric(df.Reaction_Time)
360|     fig,ax = plt.subplots(nrows=2, ncols=2, constrained_layout=True)
361|     ax[0][0].plot(df2.Reaction_Time,df2.Pupil_change,'o',
362|                   markersize = 1)
363|     ax[0][0].set_xlabel("Reaction Time (s)")
364|     ax[0][0].set_ylabel("Mean Change in pupil diameter over second prior to stimulus\n(z-score within recording)")
365|     df3 = df[['Correct','Pupil_change']].dropna()
366|     pupil_size_by_correctness = [df3.Pupil_change[df.Correct==True].values,
367|                                  df3.Pupil_change[df.Correct==False].values]
368|     ax[0][1].violinplot(pupil_size_by_correctness)
369|     ax[0][1].set_ylabel("Mean Change in pupil diameter over second prior to stimulus\n(z-score within recording)")
370|     ax[0][1].set_xticks([1,2])
371|     ax[0][1].set_xticklabels(["Correct","Incorrect"])
372|     reactions = df[['contrast','Reaction_Time']].dropna()
373|     rt_by_contrast = [reactions.Reaction_Time[reactions.contrast==0.1].values,
374|                       reactions.Reaction_Time[reactions.contrast==0.5].values]
375|     ax[1][0].violinplot(rt_by_contrast)
376|     ax[1][0].set_xticks([1,2])
377|     ax[1][0].set_xticklabels(["10%","50%"])
378|     ax[1][0].set_xlabel("Contrast")
379|     ax[1][0].set_ylabel("Reaction Time")
380|     pupils = df[['contrast','Pupil_change']].dropna()
381|     pupil_size_by_contrast = [pupils.Pupil_change[pupils.contrast==0.1].values,
382|                               pupils.Pupil_change[pupils.contrast==0.5].values]
383|     ax[1][1].violinplot(pupil_size_by_contrast)
384|     ax[1][1].set_xticks([1,2])
385|     ax[1][1].set_xticklabels(["10%","50%"])
386|     ax[1][1].set_xlabel("Contrast")
387|     ax[1][1].set_ylabel("Mean Change in pupil diameter over second prior to stimulus\n(z-score within recording)")
388|     fig.show()
389|     
390|     
391|     print("\nChange in Pupil Size Vs Reaction Time")
392|     s,p = pearsonr(df2.Reaction_Time,df2.Pupil_size)
393|     print(f"PearsonrResult(statistic={s},\npvalue={p})")
394|     
395|     print("\nChange in Pupil Size Vs Contrast:")
396|     print(kstest(pupil_size_by_contrast[0],
397|                  pupil_size_by_contrast[1]))
398|     print("\nChange in Pupil Size Vs Correctness")
399|     print(kstest(pupil_size_by_correctness[0],
400|                  pupil_size_by_correctness[1]))
401|     print("\nReaction Time Vs Contrast")
402|     print(kstest(rt_by_contrast[0],
403|                  rt_by_contrast[1]))
404|     print(f"\n0.05-Critical value after Bonferroni Correction\nfor 4 comparisons is {0.05/4:.4f}")
405| 
406| 
407| def get_dataframe_of_full_dataset(cls, drive):
408|     result = []
409|     def func(path):
410|         recorder = CollapsingRecordingUnroller(path, True)
411|         df = recorder.to_dataframe(collapse_across_ROIs = True)
412|         del recorder
413|         result.append(df)
414|         del df
415|     apply_to_all_recordings_of_class(cls, drive, func)
416|     result = pd.concat(result,ignore_index=True)
417|     return result
418| 
419| def dump_dataset_as_csv_to(path, recording_class,collapse_across_rois):
420|     '''Dump every recording into a CSV file'''
421|     #First, delete all existing file contents
422|     open(path,'w').close()
423|     #Reopen in append mode and append each experiment
424|     csv = open(path, 'a')
425|     func = lambda path:CollapsingRecordingUnroller(path, True).to_csv(csv,collapse_across_rois)
426|     apply_to_all_recordings_of_class(recording_class,"E:\\", func)
427|     csv.close()
428| 
429| if __name__=="__main__":
430|     # dump_dataset_as_csv_to(
431|     #     "C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_left_only.csv",
432|     #     recording_class='left_only_high_contrast',
433|     #     collapse_across_rois = True)
434|     # dump_dataset_as_csv_to(
435|     #     "C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_low_contrast.csv",
436|     #     recording_class='low_contrast',
437|     #     collapse_across_rois = True)
438|     # plt.close('all')
439|     # with warnings.catch_warnings():
440|     #     warnings.simplefilter('ignore')
441|     #     df = pd.read_csv("C:/Users/viviani/Desktop/reaction_time_vs_pupil_size_low_contrast.csv")
442|     #     print("In high contrast (easy task difficulty) conditions:")
443|     #     perform_attention_metrics_testing_left_only()
444|     #     print("\n\n In low contrast (hard task difficulty) conditions:")
445|     #     perform_attention_metrics_testing_low_contrast()
446|     #     print("\n\n Considering the Derivative of the pupil size")
447|     #     perform_attention_metrics_testing_with_derivative()
448|     # trial_licking = []
449|     # func = lambda p:trial_licking.extend(CollapsingRecordingUnroller(p,
450|     #                                                                  ignore_eye_video=True
451|     #                                                                  ).cluster_licking())
452|     # apply_to_all_recordings_of_class('left_only_high_contrast','H:\\',func,
453|     #                                  verbose=False)
454|     # sb.set_style('darkgrid')
455|     # fig,ax = plt.subplots()
456|     # for idx,(fst_cluster,snd_cluster) in enumerate(trial_licking):
457|     #     try:
458|     #         x1 = np.full(fst_cluster.shape,idx)
459|     #         ax.plot(fst_cluster, x1,'o',color='k',markersize=2.5)
460|     #         x2 = np.full(snd_cluster.shape,idx)
461|     #         ax.plot(snd_cluster,x2,'o',color='k',markersize=2.5)
462|     #     except AttributeError:
463|     #         pass
464|     # ymax,ymin = ax.get_ylim()
465|     # ax.vlines((0,1,3),ymin,ymax,linestyles='dashed',color = 'k')
466|     # for name,pos in zip(('Tone','Stimulus','Response'),(0,1,3)):
467|     #     ax.text(pos+0.1,ymax,name,
468|     #             horizontalalignment='left',
469|     #             verticalalignment='bottom')
470|     # ax.set_ylabel('Trial Number')
471|     # ax.set_xlabel(r'$\Delta$t from trial onset (s)')
472|     # fig.show()
473|     a=CollapsingRecordingUnroller(get_exp_path("2017-02-09_01_CFEB041",'H:\\'))
474|     a.plot_licking_clustering_approach()


\ToCSV\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


\Utils\convienience.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Jul 16 14:59:02 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | 
8  | def item(ls):
9  |     '''As numpys ndarray.item method but takes list, list of list, etc'''
10 |     if type(ls)==list and len(ls)==1:
11 |         return ls[0] if type(ls[0])!=list else item(ls[0])
12 |     else:
13 |         raise ValueError("ls must be a list with a single element")


\Utils\copy_entire_dataset_excluding_binaries.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun Sep  6 12:58:23 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | 
7  | from shutil import copytree
8  | import os.path as path
9  | 
10 | def is_binary(filepath):
11 |     file,extension = path.splitext(filepath)
12 |     if extension in (".bin",".dat",".tif",".tiff"):
13 |         return True
14 |     return False
15 | 
16 | 
17 | def files_to_exclude(root,children,dst = "D:/vivian/DataSetCopy"):
18 |     return [c for c in children if is_binary(c)]
19 | 
20 | def copy_whole_dataset_to_dir(root = "H:\\",
21 |                               new_directory = "D:/vivian/DataSetCopy"
22 |                               ):
23 |     copytree(root,new_directory,ignore=files_to_exclude)
24 |     
25 | 
26 | if __name__=="__main__":
27 |     copy_whole_dataset_to_dir()


\Utils\deeploadmat.py
0  | # -*- coding: utf-8 -*-
1  | 
2  | import scipy.io as io
3  | 
4  | 
5  | def loadmat(matfile_path):
6  |     '''
7  |     Wrapper of scipy.io.loadmat that more deeply interrogates matlab objects.
8  |     '''
9  |     data = io.loadmat(matfile_path, struct_as_record=False, squeeze_me=True)
10 |     return _check_keys(data)
11 | 
12 | def _check_keys(dic):
13 |     '''
14 |     Checks if entries in dictionary are mat-objects. If yes
15 |     todict is called to change them to nested dictionaries
16 |     '''
17 |     for key in dic:
18 |         if isinstance(dic[key], io.matlab.mio5_params.mat_struct):
19 |             dic[key] = _todict(dic[key])
20 |     return dic        
21 | 
22 | def _todict(matobj):
23 |     '''
24 |     Constructs nested dictionaries from matlab objects.
25 |     '''
26 |     dic = {}
27 |     for strg in matobj._fieldnames:
28 |         elem = matobj.__dict__[strg]
29 |         if isinstance(elem, io.matlab.mio5_params.mat_struct):
30 |             dic[strg] = _todict(elem)
31 |         else:
32 |             dic[strg] = elem
33 |     return dic


\Utils\get_files_with_dprime_and_condn.py
0  | # import suite2p
1  | # from deeploadmat import loadmat
2  | from determine_dprime import get_dprimes_from_dirtree as get_dprimes
3  | import acc_path_tools as p
4  | 
5  | 
6  | 
7  | 
8  | def get_files_with_condition_and_dprime(path, d_prime_min, function, **kwargs):
9  |     paths = []
10 |     all_dprimes = get_dprimes(path)
11 |     all_files = function(path, **kwargs)
12 |     all_files_dic = p.as_nested_dict(all_files)
13 |     for mouse, experiment, dprime in all_dprimes:
14 |         mouse_path = p.mouse_path(mouse, path)
15 |         if mouse_path in all_files_dic:
16 |             experiment_path = p.exp_path(experiment, path)
17 |             if experiment_path in all_files_dic[mouse_path] and dprime>d_prime_min:
18 |                 paths.append(all_files_dic[mouse][experiment])
19 |     paths = list(set(paths))
20 |     paths.sort()
21 |     return paths
22 | 
23 | 
24 |         
25 | if __name__ == "__main__":
26 |     PATH = 'D:\\Local_Repository'
27 |     main = get_files_with_condition_and_dprime
28 |     res = main(PATH, d_prime_min = 1, function = p.get_all_files_with_name, 
29 |                name='spks.npy')
30 |     print(f"Identified {len(res)} satisfactory spks.npy files, at:")
31 |     for _,file in res:
32 |         print(file)
33 |     res = main(PATH, d_prime_min = 1, function = p.get_all_files_with_ext, 
34 |                ext ='.tif')
35 |     print(f"Identified {len(res)} satisfactory tif files, at:")
36 |     for _,file in res:
37 |         print(file) 


\Utils\map_across_dataset.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Sun Jul  5 15:00:38 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | import os
7  | import pickle as pkl
8  | 
9  | 
10 | def no_of_planes(experiment_path):
11 |     '''
12 |     Returns the number of planes in a preprocessed suite2p folder.
13 | 
14 |     Parameters
15 |     ----------
16 |     experiment_path : string
17 |         The path to a recording directory.
18 | 
19 |     Returns
20 |     -------
21 |     count : int
22 |         Returns the number of directories named plane* contained in 
23 |         experiment_path. If experiment_path does not exist, returns 0.
24 | 
25 |     '''
26 |     #I'm sure there's an elegant way to do this with regex
27 |     #or something
28 |     count = 0
29 |     try:
30 |         for file in os.listdir(os.path.join(experiment_path,"suite2p")):
31 |             if file[:-1] == "plane":
32 |                 count +=1
33 |         return count
34 |     except (NotADirectoryError, FileNotFoundError):
35 |         return 0
36 | 
37 | 
38 | def apply_to_all_one_plane_recordings(drive,func,verbose=False):
39 |     '''
40 |     Map a function func across every experiment with 1 plane in a dataset in 
41 |     drive.
42 | 
43 |     Parameters
44 |     ----------
45 |     drive : str
46 |         The drive containing the dataset, eg "E://". The dataset root directory
47 |         must be at top level of the drive
48 |     func : callable
49 |         The function to map. Is passed a single experiment path as an argument.
50 |     verbose : bool, optional
51 |         Whether to print information to console. The default is False.
52 | 
53 |     Returns
54 |     -------
55 |     None.
56 | 
57 |     '''
58 |     root = os.path.join(drive, 'Local_Repository')
59 |     for animal in os.listdir(root):
60 |         animal_path = os.path.join(root,animal)
61 |         if os.path.isdir(animal_path):
62 |             if verbose: print(f"Processing {animal} experiments")
63 |             for recording in os.listdir(animal_path):
64 |                 try:
65 |                     rec_path = os.path.join(animal_path,recording)
66 |                     if os.path.isdir(rec_path) and no_of_planes(rec_path) == 1:
67 |                         func(rec_path)
68 |                 except (FileNotFoundError,ValueError) as e:
69 |                     if verbose:
70 |                         print(
71 |                             f"LOG: {func.__name__} on {recording} resulted in {e}"
72 |                             )
73 | 
74 | 
75 | def apply_to_all_recordings(drive,func,verbose=False):
76 |     '''
77 |     Map a function func across every single experiment in a dataset in drive.
78 | 
79 |     Parameters
80 |     ----------
81 |     drive : str
82 |         The drive containing the dataset, eg "E://". The dataset root directory
83 |         must be at top level of the drive
84 |     func : callable
85 |         The function to map. Is passed a single experiment path as an argument.
86 |     verbose : bool, optional
87 |         Whether to print information to console. The default is False.
88 | 
89 |     Returns
90 |     -------
91 |     None.
92 | 
93 |     '''
94 |     root = os.path.join(drive, 'Local_Repository')
95 |     for animal in os.listdir(root):
96 |         animal_path = os.path.join(root,animal)
97 |         if os.path.isdir(animal_path):
98 |             if verbose: print(f"Processing {animal} experiments")
99 |             for recording in os.listdir(animal_path):
100|                 try:
101|                     rec_path = os.path.join(animal_path,recording)
102|                     func(rec_path)
103|                 except (FileNotFoundError,ValueError) as e:
104|                     if verbose: print(f"LOG: {func.__name__} on {recording} resulted in {e}")
105| 
106| def iterate_across_recordings(drive):
107|     root = os.path.join(drive, 'Local_Repository')
108|     for animal in os.listdir(root):
109|         animal_path = os.path.join(root,animal)
110|         if os.path.isdir(animal_path):
111|             for recording in os.listdir(animal_path): 
112|                 yield os.path.join(animal_path,recording)
113|     return
114| 
115| def apply_to_all_recordings_of_class(cls, drive, func, verbose=True):
116|     '''
117|     Map a function across each experiment in Drive with features determined
118|     by cls.
119| 
120|     Parameters
121|     ----------
122|     cls : string
123|         The class of recording to which to apply func. One of
124|         {"both_sides_high_contrast", "left_only_high_contrast",
125|          "low_contrast", "both_sides_high_contrast_uncleaned", 
126|          "left_only_high_contrast_uncleaned", "low_contrast_uncleaned"}.
127|     drive : str
128|         The drive containing the dataset, eg "E://". The dataset root directory
129|         ("Local_Repository") must be at top level of the drive
130|     func : callable
131|         The function to map. Is passed a single experiment path as an argument.
132| 
133|     Returns
134|     -------
135|     None.
136| 
137|     '''
138|     file_path = f"../DataCleaning/{cls}.pkl"
139|     print(file_path)
140|     try:
141|         with open(file_path,'rb') as file:
142|             paths = pkl.load(file)
143|     except FileNotFoundError:
144|         raise FileNotFoundError(
145|             """
146|             Hey, looks like that recording class doesn't exist on disk yet.
147|             Before calling this function, you need to run 
148|             get_recordings_for_analysis.py to generate the recording classes!
149|             Alternatively, you might have attempted to reference a recording
150|             class that doesn't exist, or might have mistyped the class name.""")
151|     for path in paths:
152|         try:
153|             func(path)
154|         except (FileNotFoundError,ValueError) as e:
155|             if verbose: print(f"LOG: {func.__name__} on {path} resulted in {e}")
156| 


\Utils\path.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Mar  3 14:16:10 2020
3  | 
4  | @author: Vivian Imbriotis
5  | """
6  | from os.path import join, split, splitext, splitdrive, sep
7  | from os import walk, listdir
8  | from collections import defaultdict
9  | 
10 | from accdatatools.Utils.convienience import item
11 | 
12 | 
13 | 
14 | def as_nested_dict(three_tuple):
15 |     '''
16 |     Parameters
17 |     ----------
18 |     obj : list of 3-tuples
19 | 
20 |     Returns
21 |     -------
22 |     result : dictionary.
23 |         1-elements of tuples become keys, with dictionaries as values.
24 |         2-elemements of the tuples become keys of those dictionaries,
25 |         with lists of 3-elements as values.
26 |         
27 |         So [(A,a,x),(A,a,y),(A,b,x),(B,a,x)] => 
28 |             {
29 |                 A: {
30 |                         a:[x,y],
31 |                         b:[x]                        
32 |                     },
33 |                 B: {
34 |                         a:[x]
35 |                         }
36 |             }                                           
37 | 
38 |     '''
39 |     result = defaultdict(lambda:defaultdict(list))
40 |     for layer1, layer2, layer3 in three_tuple:
41 |         result[layer1][layer2].append(layer3)
42 |     for key,value in result.items():
43 |         value.default_factory = None
44 |     result.default_factory = None
45 |     return result
46 | 
47 | 
48 | def prune_path_to_experiment(path):
49 |     head, tail = split(path)
50 |     if tail[:2] == '20':
51 |         return path
52 |     elif head == splitdrive(head)[0]:
53 |         raise AttributeError("Path could not be pruned to experiment")
54 |     else:
55 |         return prune_path_to_experiment(head)
56 | 
57 | def prune_path_to_mouse(path):
58 |     head, tail = split(path)
59 |     if tail[:4] == 'CFEB':
60 |         return path
61 |     elif head == splitdrive(head)[0]:
62 |         raise AttributeError("Path could not be pruned to mouse")
63 |     else:
64 |         return prune_path_to_mouse(head)
65 | 
66 | def get_mouse_id(path):
67 |     '''
68 |     Convert a path including a mouse ID to just the mouse ID.
69 |     '''
70 |     path = prune_path_to_mouse(path)
71 |     path = path.split(sep)[-1]
72 |     return path
73 |     
74 | 
75 | def get_exp_id(path):
76 |     '''
77 |     Convert a path including an experiment ID to just the experiment ID.
78 |     '''
79 |     path = prune_path_to_experiment(path)
80 |     path = path.split(sep)[-1]
81 |     return path
82 | 
83 | def get_mouse_path(id_str, root):
84 |     '''
85 |     Searches the directory tree rooted at root for a path corresponding to
86 |     the Mouse ID id_str.
87 |     eg 'CFEB015', 'D:\\' => 'D:\\Local_Directory\\CFEB015'
88 |     '''
89 |     for root,dirs,files in walk(root):
90 |         for directory in dirs:
91 |             try:
92 |                 if get_mouse_id(directory) == id_str:
93 |                     return join(root,directory)
94 |             except AttributeError: pass
95 |     raise FileNotFoundError(
96 |         "No directory corresponding to that mouse ID found.")
97 |     
98 |     
99 | 
100| def get_exp_path(id_str, root):
101|     '''
102|     Searches the directory tree rooted at root for a path corresponding to
103|     the Experiment ID id_str. 
104|     '''
105|     if "Local_Repository" not in root:
106|         root = join(root,"Local_Repository")
107|     for root,dirs,files in walk(root):
108|         for directory in dirs:
109|             try:
110|                 if get_exp_id(directory) == id_str:
111|                     return join(root,directory)
112|             except AttributeError: pass
113|     raise FileNotFoundError(
114|         f"No directory corresponding to Experiment ID {id_str} found at {root}.")
115| 
116| def get_timeline_path(exp_path):
117|     file = item([file for file in listdir(exp_path) if "Timeline" in file])
118|     return join(exp_path,file)
119| 
120| def get_psychstim_path(exp_path):
121|     file = item([file for file in listdir(exp_path) if "psychstim" in file])
122|     return join(exp_path,file)
123| 
124| 
125| DLC_ANALYSED_VIDEOS_DIRECTORY = "C:/Users/viviani/Desktop/allvideos"
126| 
127| def get_pupil_hdf_path(exp_path,
128|                        directory = DLC_ANALYSED_VIDEOS_DIRECTORY):
129|     path, exp_id = split(exp_path)
130|     for file in listdir(directory):
131|         if exp_id in file and '.h5' in file:
132|             hdf_path = join(DLC_ANALYSED_VIDEOS_DIRECTORY,
133|                                     file)
134|             return hdf_path
135|     raise ValueError(
136|         f"No file in {DLC_ANALYSED_VIDEOS_DIRECTORY} matching {exp_id}"
137|         )
138| 
139| 
140| def get_all_files_with_condition(path, condition, verbose=False):
141|     '''
142|     Searches all files in directories rooted at path, including files
143|     if they satisfy condition. Typical usage idiom similar to os.walk:
144|         
145|         > For mouse, exp, file in get_all_files_with_condition(PATH,cond):
146|             #do things
147|         
148|     If only the files are needed, discard the mouse and experiment information
149|     
150|         >for _,_,file in get_all_files_with_condition(PATH, cond)
151|             #do things
152| 
153|     Parameters
154|     ----------
155|     path : str
156|         The root path to search recursively.
157|     condition : callable object
158|         A function that accepts a string filename and returns a bool.
159|         eg > lambda file: ext==".tif" for _,ext in os.splitext(file)
160| 
161|     Returns
162|     -------
163|     result : list
164|         A list of 3-tuples of 
165|         (str mouse_path, str experiment_path, str file_path)
166| 
167|     '''
168|     all_files = set()
169|     experiments = set()
170|     mice = set()
171|     result= []
172|     for root, dirs, files in walk(path):
173|         for file in files:
174|             filename, file_extension = splitext(file)
175|             if condition(file):
176|                 if verbose:
177|                     all_files.add(join(root,file))
178|                     experiments.add(prune_path_to_experiment(root))
179|                     mice.add(prune_path_to_mouse(root))
180|                 result.append((prune_path_to_mouse(root),
181|                                prune_path_to_experiment(root),
182|                                join(root,file)))
183|     if verbose:
184|         print(f'\n-----{len(all_files)} Files-----')
185|         for i in all_files:
186|             print(i)
187|             print(f'\n-----{len(experiments)} Experiments-----')
188|         for i in experiments:
189|             print(i)
190|             print(f'\n-----{len(mice)} Mice-----')
191|         for i in mice:
192|             print(i)
193|     return result
194| 
195| def get_all_files_with_name(path, name):
196|     '''
197|     Gets all files from a directory tree rooted at path with filename name.
198| 
199|     Parameters
200|     ----------
201|     path : str
202|         The root path to search recursively.
203|     name : str
204|         The filename for which to search.
205| 
206|     Returns
207|     -------
208|     result : list
209|         A list of 3-tuples of 
210|         (str mouse_path, str experiment_path, str file_path)
211|     '''
212|     condition = lambda file:file==name
213|     return get_all_files_with_condition(path, condition)
214| 
215| def get_all_files_with_ext(path,ext):
216|     '''
217|     Gets all files from a directory tree rooted at path with file extention
218|     ext.
219| 
220|     Parameters
221|     ----------
222|     path : str
223|         The root path to search recursively.
224|     ext : str
225|         The file extention for which to search, including period, eg '.tif'
226| 
227|     Returns
228|     -------
229|     result : list
230|         A list of 3-tuples of 
231|         (str mouse_path, str experiment_path, str file_path)
232|     '''
233|     condition = lambda file:splitext(file)[1]==ext
234|     return get_all_files_with_condition(path,condition)
235| 
236| if __name__=="__main__":
237|     # res = get_all_files_with_ext('D:\\Local_Repository', ext='data.bin')
238|     # mice = set()
239|     # exps = set()
240|     # files = set()
241|     # for mouse, exp, file in res:
242|     #         mice.add(mouse)
243|     #         exps.add(exp)
244|     #         files.add(file)
245|     # mice = list(mice); mice.sort()
246|     # exps = list(exps); exps.sort()
247|     # files = list(files); files.sort()
248|     # for mouse in mice:
249|     #     print(mouse)
250|     # print('\n\n----------\n\n')
251|     # for exp in exps:
252|     #     print(exp)
253|     pass
254| 
255| 


\Utils\quine.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Tue Sep 22 10:59:17 2020
3  | 
4  | @author: Vivian Imbriotis
5  | 
6  | A small utility to create a text version of this entire repository to dump
7  | to a txt file, so all the code can be put into an appendix.
8  | """
9  | 
10 | from os import listdir, walk 
11 | from os.path import isdir, join, split, splitext
12 | from contextlib import redirect_stdout
13 | 
14 | import accdatatools
15 | root = accdatatools.__path__[0]
16 | 
17 | 
18 | 
19 | def tree(path, indent = 0):
20 |     name = split(path)[1]
21 |     print(f"{''.join([' |']*indent)}{name}")
22 |     if isdir(path) and "git" not in path:
23 |         [tree(join(path,f),
24 |               indent=indent+1) for f in listdir(path)]
25 | 
26 | def dump_formatted_file_content(path):
27 |     '''
28 |     Cat the contents of a file to stdout, prepending each line with
29 |     a nicely formatted line number.
30 | 
31 |     Parameters
32 |     ----------
33 |     path : str
34 |         Path to the code file.
35 | 
36 |     '''
37 |     newline = '\n'
38 |     with open(path,'r') as file:
39 |         code = file.readlines()
40 |     for n, line in enumerate(code):
41 |         print(f"{n:<3}| {line.replace(newline,'')}")
42 | 
43 | def is_text(file):
44 |     '''Check if a file is a text file or a binary file'''
45 |     name, ext = splitext(file)
46 |     if ext in ("",".py",".R",".yaml",".txt"): 
47 |         return True
48 |     return False
49 | 
50 | def print_all_code(root):
51 |     '''
52 |     Cat formatted representations of the directory structure and the content
53 |     of all text files in a projected rooted at root to stdout.
54 | 
55 |     Parameters
56 |     ----------
57 |     root : str
58 |         The project's root directory.
59 | 
60 |     '''
61 |     for root,dirs,files in walk(root):
62 |         rname = root.split("accdatatools")[-1]
63 |         if 'git' not in root:
64 |             for file in files:
65 |                 if is_text(file) and file != "quine.txt":
66 |                     print(join(rname,file))
67 |                     dump_formatted_file_content(join(root,file))
68 |                     print("\n")
69 | 
70 | def main():
71 |     try:
72 |         outfile = open("quine.txt",'w')
73 |         #This is roughly similar to a unix forward pipe operator
74 |         with redirect_stdout(outfile):
75 |             print("Directory Structure\n")
76 |             tree(root)
77 |             print_all_code(root)
78 |     finally:
79 |         outfile.close()
80 | 
81 | if __name__=="__main__":
82 |     main()


\Utils\signal_processing.py
0  | # -*- coding: utf-8 -*-
1  | """
2  | Created on Thu Jul 16 14:10:30 2020
3  | 
4  | @author: viviani
5  | """
6  | 
7  | import numpy as np
8  | 
9  | def rising_edges(array, cutoff = 2.5):
10 |     '''
11 |     Detect rising edges.
12 | 
13 |     Parameters
14 |     ----------
15 |     array : arraylike of float
16 |         Signal array of voltages.
17 |     cutoff : int, optional
18 |         The cutoff voltage to distinguish digital True from False. 
19 |         The default is 2.5.
20 | 
21 |     Returns
22 |     -------
23 |     Arraylike of Bool
24 |         True on rising edge indicies of input array, else false
25 |     '''
26 |     digital_array = np.greater(array,cutoff)
27 |     shifted_array = np.logical_not(np.append(digital_array[0],digital_array[:-1]))
28 |     return np.logical_and(digital_array,shifted_array)
29 | 
30 | def falling_edges(array, cutoff = 2.5):
31 |     '''
32 |     Detect falling edges.
33 | 
34 |     Parameters
35 |     ----------
36 |     array : arraylike of float
37 |         Signal array of voltages.
38 |     cutoff : int, optional
39 |         The cutoff voltage to distinguish digital True from False. 
40 |         The default is 2.5.
41 | 
42 |     Returns
43 |     -------
44 |     Arraylike of Bool
45 |         True on falling edge indicies of input array, else False
46 |     '''
47 |     digital_array = np.greater(array,cutoff)
48 |     shifted_array = np.logical_not(np.append(digital_array[1:],digital_array[-1]))
49 |     return np.logical_and(digital_array,shifted_array)
50 | 
51 | def rising_or_falling_edges(array, cuttoff=2.5):
52 |     rising  = rising_edges(array,cuttoff)
53 |     falling = falling_edges(array,cuttoff)
54 |     return np.logical_or(rising,falling)


\Utils\__init__.py
0  | # -*- coding: utf-8 -*-
1  | 


